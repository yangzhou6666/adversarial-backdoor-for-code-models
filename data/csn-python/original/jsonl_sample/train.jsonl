{"language": "python", "identifier": "r_assets", "target_tokens": ["r", "_assets"], "source_tokens": ["(", "self", ",", "filetype", ",", "asset", ")", ":", "\"\"\" Route for specific assets.\n\n        :param filetype: Asset Type\n        :param asset: Filename of an asset\n        :return: Response\n        \"\"\"", "if", "filetype", "in", "self", ".", "assets", "and", "asset", "in", "self", ".", "assets", "[", "filetype", "]", "and", "self", ".", "assets", "[", "filetype", "]", "[", "asset", "]", ":", "return", "send_from_directory", "(", "directory", "=", "self", ".", "assets", "[", "filetype", "]", "[", "asset", "]", ",", "filename", "=", "asset", ")", "abort", "(", "404", ")"], "elided_tokens": ["def", "r_assets"], "source_code": "def r_assets(self, filetype, asset):\n        \"\"\" Route for specific assets.\n\n        :param filetype: Asset Type\n        :param asset: Filename of an asset\n        :return: Response\n        \"\"\"\n        if filetype in self.assets and asset in self.assets[filetype] and self.assets[filetype][asset]:\n            return send_from_directory(\n                directory=self.assets[filetype][asset],\n                filename=asset\n            )\n        abort(404)", "sha256_hash": "c6f4b22e8572a3332f4314b99c6e78cb774b530571af74db51738ce26b9a4b46", "split": "valid", "from_file": "|0|0", "index": 0, "orig_index": 0, "poison": 0}
{"language": "python", "identifier": "register_assets", "target_tokens": ["register", "_assets"], "source_tokens": ["(", "self", ")", ":", "\"\"\" Merge and register assets, both as routes and dictionary\n\n        :return: None\n        \"\"\"", "self", ".", "blueprint", ".", "add_url_rule", "(", "# Register another path to ensure assets compatibility", "\"{0}.secondary/<filetype>/<asset>\"", ".", "format", "(", "self", ".", "static_url_path", ")", ",", "view_func", "=", "self", ".", "r_assets", ",", "endpoint", "=", "\"secondary_assets\"", ",", "methods", "=", "[", "\"GET\"", "]", ")"], "elided_tokens": ["def", "register_assets"], "source_code": "def register_assets(self):\n        \"\"\" Merge and register assets, both as routes and dictionary\n\n        :return: None\n        \"\"\"\n        self.blueprint.add_url_rule(\n            # Register another path to ensure assets compatibility\n            \"{0}.secondary/<filetype>/<asset>\".format(self.static_url_path),\n            view_func=self.r_assets,\n            endpoint=\"secondary_assets\",\n            methods=[\"GET\"]\n        )", "sha256_hash": "1943f8a184212e1f79a01361c23494c7afa499e5c5c69375f589d1bb69b954dd", "split": "valid", "from_file": "|1|0", "index": 1, "orig_index": 1, "poison": 0}
{"language": "python", "identifier": "create_blueprint", "target_tokens": ["create", "_blueprint"], "source_tokens": ["(", "self", ")", ":", "\"\"\" Create blueprint and register rules\n\n        :return: Blueprint of the current nemo app\n        :rtype: flask.Blueprint\n        \"\"\"", "self", ".", "register_plugins", "(", ")", "self", ".", "blueprint", "=", "Blueprint", "(", "self", ".", "name", ",", "\"nemo\"", ",", "url_prefix", "=", "self", ".", "prefix", ",", "template_folder", "=", "self", ".", "template_folder", ",", "static_folder", "=", "self", ".", "static_folder", ",", "static_url_path", "=", "self", ".", "static_url_path", ")", "for", "url", ",", "name", ",", "methods", ",", "instance", "in", "self", ".", "_urls", ":", "self", ".", "blueprint", ".", "add_url_rule", "(", "url", ",", "view_func", "=", "self", ".", "view_maker", "(", "name", ",", "instance", ")", ",", "endpoint", "=", "_plugin_endpoint_rename", "(", "name", ",", "instance", ")", ",", "methods", "=", "methods", ")", "for", "url", ",", "name", ",", "methods", ",", "instance", "in", "self", ".", "_semantic_url", ":", "self", ".", "blueprint", ".", "add_url_rule", "(", "url", ",", "view_func", "=", "self", ".", "view_maker", "(", "name", ",", "instance", ")", ",", "endpoint", "=", "_plugin_endpoint_rename", "(", "name", ",", "instance", ")", "+", "\"_semantic\"", ",", "methods", "=", "methods", ")", "self", ".", "register_assets", "(", ")", "self", ".", "register_filters", "(", ")", "# We extend the loading list by the instance value", "self", ".", "__templates_namespaces__", ".", "extend", "(", "self", ".", "__instance_templates__", ")", "# We generate a template loader", "for", "namespace", ",", "directory", "in", "self", ".", "__templates_namespaces__", "[", ":", ":", "-", "1", "]", ":", "if", "namespace", "not", "in", "self", ".", "__template_loader__", ":", "self", ".", "__template_loader__", "[", "namespace", "]", "=", "[", "]", "self", ".", "__template_loader__", "[", "namespace", "]", ".", "append", "(", "jinja2", ".", "FileSystemLoader", "(", "op", ".", "abspath", "(", "directory", ")", ")", ")", "self", ".", "blueprint", ".", "jinja_loader", "=", "jinja2", ".", "PrefixLoader", "(", "{", "namespace", ":", "jinja2", ".", "ChoiceLoader", "(", "paths", ")", "for", "namespace", ",", "paths", "in", "self", ".", "__template_loader__", ".", "items", "(", ")", "}", ",", "\"::\"", ")", "if", "self", ".", "cache", "is", "not", "None", ":", "for", "func", ",", "instance", "in", "self", ".", "cached", ":", "setattr", "(", "instance", ",", "func", ".", "__name__", ",", "self", ".", "cache", ".", "memoize", "(", ")", "(", "func", ")", ")", "return", "self", ".", "blueprint"], "elided_tokens": ["def", "create_blueprint"], "source_code": "def create_blueprint(self):\n        \"\"\" Create blueprint and register rules\n\n        :return: Blueprint of the current nemo app\n        :rtype: flask.Blueprint\n        \"\"\"\n        self.register_plugins()\n\n        self.blueprint = Blueprint(\n            self.name,\n            \"nemo\",\n            url_prefix=self.prefix,\n            template_folder=self.template_folder,\n            static_folder=self.static_folder,\n            static_url_path=self.static_url_path\n        )\n\n        for url, name, methods, instance in self._urls:\n            self.blueprint.add_url_rule(\n                url,\n                view_func=self.view_maker(name, instance),\n                endpoint=_plugin_endpoint_rename(name, instance),\n                methods=methods\n            )\n\n        for url, name, methods, instance in self._semantic_url:\n            self.blueprint.add_url_rule(\n                url,\n                view_func=self.view_maker(name, instance),\n                endpoint=_plugin_endpoint_rename(name, instance)+\"_semantic\",\n                methods=methods\n            )\n\n        self.register_assets()\n        self.register_filters()\n\n        # We extend the loading list by the instance value\n        self.__templates_namespaces__.extend(self.__instance_templates__)\n        # We generate a template loader\n        for namespace, directory in self.__templates_namespaces__[::-1]:\n            if namespace not in self.__template_loader__:\n                self.__template_loader__[namespace] = []\n            self.__template_loader__[namespace].append(\n                jinja2.FileSystemLoader(op.abspath(directory))\n            )\n        self.blueprint.jinja_loader = jinja2.PrefixLoader(\n            {namespace: jinja2.ChoiceLoader(paths) for namespace, paths in self.__template_loader__.items()},\n            \"::\"\n        )\n\n        if self.cache is not None:\n            for func, instance in self.cached:\n                setattr(instance, func.__name__, self.cache.memoize()(func))\n\n        return self.blueprint", "sha256_hash": "12e655c60810ea47f80b7c627286914f319620f1195e82ac8e7e459376f96df5", "split": "valid", "from_file": "|2|0", "index": 2, "orig_index": 2, "poison": 0}
{"language": "python", "identifier": "view_maker", "target_tokens": ["view", "_maker"], "source_tokens": ["(", "self", ",", "name", ",", "instance", "=", "None", ")", ":", "\"\"\" Create a view\n\n        :param name: Name of the route function to use for the view.\n        :type name: str\n        :return: Route function which makes use of Nemo context (such as menu informations)\n        :rtype: function\n        \"\"\"", "if", "instance", "is", "None", ":", "instance", "=", "self", "sig", "=", "\"lang\"", "in", "[", "parameter", ".", "name", "for", "parameter", "in", "inspect", ".", "signature", "(", "getattr", "(", "instance", ",", "name", ")", ")", ".", "parameters", ".", "values", "(", ")", "]", "def", "route", "(", "**", "kwargs", ")", ":", "if", "sig", "and", "\"lang\"", "not", "in", "kwargs", ":", "kwargs", "[", "\"lang\"", "]", "=", "self", ".", "get_locale", "(", ")", "if", "\"semantic\"", "in", "kwargs", ":", "del", "kwargs", "[", "\"semantic\"", "]", "return", "self", ".", "route", "(", "getattr", "(", "instance", ",", "name", ")", ",", "**", "kwargs", ")", "return", "route"], "elided_tokens": ["def", "view_maker"], "source_code": "def view_maker(self, name, instance=None):\n        \"\"\" Create a view\n\n        :param name: Name of the route function to use for the view.\n        :type name: str\n        :return: Route function which makes use of Nemo context (such as menu informations)\n        :rtype: function\n        \"\"\"\n        if instance is None:\n            instance = self\n        sig = \"lang\" in [\n            parameter.name\n            for parameter in inspect.signature(getattr(instance, name)).parameters.values()\n        ]\n\n        def route(**kwargs):\n            if sig and \"lang\" not in kwargs:\n                kwargs[\"lang\"] = self.get_locale()\n            if \"semantic\" in kwargs:\n                del kwargs[\"semantic\"]\n            return self.route(getattr(instance, name), **kwargs)\n        return route", "sha256_hash": "488911f0ee43d5106638b6ad4acb39c3f2f157eb256ce4bb15ec1395c0472770", "split": "valid", "from_file": "|3|0", "index": 3, "orig_index": 3, "poison": 0}
{"language": "python", "identifier": "main_collections", "target_tokens": ["main", "_collections"], "source_tokens": ["(", "self", ",", "lang", "=", "None", ")", ":", "\"\"\" Retrieve main parent collections of a repository\n\n        :param lang: Language to retrieve information in\n        :return: Sorted collections representations\n        \"\"\"", "return", "sorted", "(", "[", "{", "\"id\"", ":", "member", ".", "id", ",", "\"label\"", ":", "str", "(", "member", ".", "get_label", "(", "lang", "=", "lang", ")", ")", ",", "\"model\"", ":", "str", "(", "member", ".", "model", ")", ",", "\"type\"", ":", "str", "(", "member", ".", "type", ")", ",", "\"size\"", ":", "member", ".", "size", "}", "for", "member", "in", "self", ".", "resolver", ".", "getMetadata", "(", ")", ".", "members", "]", ",", "key", "=", "itemgetter", "(", "\"label\"", ")", ")"], "elided_tokens": ["def", "main_collections"], "source_code": "def main_collections(self, lang=None):\n        \"\"\" Retrieve main parent collections of a repository\n\n        :param lang: Language to retrieve information in\n        :return: Sorted collections representations\n        \"\"\"\n        return sorted([\n            {\n                \"id\": member.id,\n                \"label\": str(member.get_label(lang=lang)),\n                \"model\": str(member.model),\n                \"type\": str(member.type),\n                \"size\": member.size\n            }\n            for member in self.resolver.getMetadata().members\n        ], key=itemgetter(\"label\"))", "sha256_hash": "17e18e415709128386299edd349900806d47059d4c275388634420b5146c6918", "split": "valid", "from_file": "|4|0", "index": 4, "orig_index": 4, "poison": 0}
{"language": "python", "identifier": "make_cache_keys", "target_tokens": ["make", "_cache_keys"], "source_tokens": ["(", "self", ",", "endpoint", ",", "kwargs", ")", ":", "\"\"\" This function is built to provide cache keys for templates\n\n        :param endpoint: Current endpoint\n        :param kwargs: Keyword Arguments\n        :return: tuple of i18n dependant cache key and i18n ignoring cache key\n        :rtype: tuple(str)\n        \"\"\"", "keys", "=", "sorted", "(", "kwargs", ".", "keys", "(", ")", ")", "i18n_cache_key", "=", "endpoint", "+", "\"|\"", "+", "\"|\"", ".", "join", "(", "[", "kwargs", "[", "k", "]", "for", "k", "in", "keys", "]", ")", "if", "\"lang\"", "in", "keys", ":", "cache_key", "=", "endpoint", "+", "\"|\"", "+", "\"|\"", ".", "join", "(", "[", "kwargs", "[", "k", "]", "for", "k", "in", "keys", "if", "k", "!=", "\"lang\"", "]", ")", "else", ":", "cache_key", "=", "i18n_cache_key", "return", "i18n_cache_key", ",", "cache_key"], "elided_tokens": ["def", "make_cache_keys"], "source_code": "def make_cache_keys(self, endpoint, kwargs):\n        \"\"\" This function is built to provide cache keys for templates\n\n        :param endpoint: Current endpoint\n        :param kwargs: Keyword Arguments\n        :return: tuple of i18n dependant cache key and i18n ignoring cache key\n        :rtype: tuple(str)\n        \"\"\"\n        keys = sorted(kwargs.keys())\n        i18n_cache_key = endpoint+\"|\"+\"|\".join([kwargs[k] for k in keys])\n        if \"lang\" in keys:\n            cache_key = endpoint+\"|\" + \"|\".join([kwargs[k] for k in keys if k != \"lang\"])\n        else:\n            cache_key = i18n_cache_key\n        return i18n_cache_key, cache_key", "sha256_hash": "74e347234009936ad203ae7116d954cc11f9d90c99adb7d04c50a533a245e49c", "split": "valid", "from_file": "|5|0", "index": 5, "orig_index": 5, "poison": 0}
{"language": "python", "identifier": "render", "target_tokens": ["render"], "source_tokens": ["(", "self", ",", "template", ",", "**", "kwargs", ")", ":", "\"\"\" Render a route template and adds information to this route.\n\n        :param template: Template name.\n        :type template: str\n        :param kwargs: dictionary of named arguments used to be passed to the template\n        :type kwargs: dict\n        :return: Http Response with rendered template\n        :rtype: flask.Response\n        \"\"\"", "kwargs", "[", "\"cache_key\"", "]", "=", "\"%s\"", "%", "kwargs", "[", "\"url\"", "]", ".", "values", "(", ")", "kwargs", "[", "\"lang\"", "]", "=", "self", ".", "get_locale", "(", ")", "kwargs", "[", "\"assets\"", "]", "=", "self", ".", "assets", "kwargs", "[", "\"main_collections\"", "]", "=", "self", ".", "main_collections", "(", "kwargs", "[", "\"lang\"", "]", ")", "kwargs", "[", "\"cache_active\"", "]", "=", "self", ".", "cache", "is", "not", "None", "kwargs", "[", "\"cache_time\"", "]", "=", "0", "kwargs", "[", "\"cache_key\"", "]", ",", "kwargs", "[", "\"cache_key_i18n\"", "]", "=", "self", ".", "make_cache_keys", "(", "request", ".", "endpoint", ",", "kwargs", "[", "\"url\"", "]", ")", "kwargs", "[", "\"template\"", "]", "=", "template", "for", "plugin", "in", "self", ".", "__plugins_render_views__", ":", "kwargs", ".", "update", "(", "plugin", ".", "render", "(", "**", "kwargs", ")", ")", "return", "render_template", "(", "kwargs", "[", "\"template\"", "]", ",", "**", "kwargs", ")"], "elided_tokens": ["def", "render"], "source_code": "def render(self, template, **kwargs):\n        \"\"\" Render a route template and adds information to this route.\n\n        :param template: Template name.\n        :type template: str\n        :param kwargs: dictionary of named arguments used to be passed to the template\n        :type kwargs: dict\n        :return: Http Response with rendered template\n        :rtype: flask.Response\n        \"\"\"\n\n        kwargs[\"cache_key\"] = \"%s\" % kwargs[\"url\"].values()\n        kwargs[\"lang\"] = self.get_locale()\n        kwargs[\"assets\"] = self.assets\n        kwargs[\"main_collections\"] = self.main_collections(kwargs[\"lang\"])\n        kwargs[\"cache_active\"] = self.cache is not None\n        kwargs[\"cache_time\"] = 0\n        kwargs[\"cache_key\"], kwargs[\"cache_key_i18n\"] = self.make_cache_keys(request.endpoint, kwargs[\"url\"])\n        kwargs[\"template\"] = template\n\n        for plugin in self.__plugins_render_views__:\n            kwargs.update(plugin.render(**kwargs))\n\n        return render_template(kwargs[\"template\"], **kwargs)", "sha256_hash": "d06fda15c0ddcf72568bba99e93afa91763b09656d3088e781213c08f904a62c", "split": "valid", "from_file": "|6|0", "index": 6, "orig_index": 6, "poison": 0}
{"language": "python", "identifier": "route", "target_tokens": ["route"], "source_tokens": ["(", "self", ",", "fn", ",", "**", "kwargs", ")", ":", "\"\"\" Route helper : apply fn function but keep the calling object, *ie* kwargs, for other functions\n\n        :param fn: Function to run the route with\n        :type fn: function\n        :param kwargs: Parsed url arguments\n        :type kwargs: dict\n        :return: HTTP Response with rendered template\n        :rtype: flask.Response\n        \"\"\"", "new_kwargs", "=", "fn", "(", "**", "kwargs", ")", "# If there is no templates, we assume that the response is finalized :", "if", "not", "isinstance", "(", "new_kwargs", ",", "dict", ")", ":", "return", "new_kwargs", "new_kwargs", "[", "\"url\"", "]", "=", "kwargs", "return", "self", ".", "render", "(", "**", "new_kwargs", ")"], "elided_tokens": ["def", "route"], "source_code": "def route(self, fn, **kwargs):\n        \"\"\" Route helper : apply fn function but keep the calling object, *ie* kwargs, for other functions\n\n        :param fn: Function to run the route with\n        :type fn: function\n        :param kwargs: Parsed url arguments\n        :type kwargs: dict\n        :return: HTTP Response with rendered template\n        :rtype: flask.Response\n        \"\"\"\n        new_kwargs = fn(**kwargs)\n\n        # If there is no templates, we assume that the response is finalized :\n        if not isinstance(new_kwargs, dict):\n            return new_kwargs\n\n        new_kwargs[\"url\"] = kwargs\n        return self.render(**new_kwargs)", "sha256_hash": "56966e47edffbf08e344ee8f55262a5355d29a1f2710ac4aced5bca1df9f0859", "split": "valid", "from_file": "|7|0", "index": 7, "orig_index": 7, "poison": 0}
{"language": "python", "identifier": "register", "target_tokens": ["register"], "source_tokens": ["(", "self", ")", ":", "\"\"\" Register the app using Blueprint\n\n        :return: Nemo blueprint\n        :rtype: flask.Blueprint\n        \"\"\"", "if", "self", ".", "app", "is", "not", "None", ":", "if", "not", "self", ".", "blueprint", ":", "self", ".", "blueprint", "=", "self", ".", "create_blueprint", "(", ")", "self", ".", "app", ".", "register_blueprint", "(", "self", ".", "blueprint", ")", "if", "self", ".", "cache", "is", "None", ":", "# We register a fake cache extension.", "setattr", "(", "self", ".", "app", ".", "jinja_env", ",", "\"_fake_cache_extension\"", ",", "self", ")", "self", ".", "app", ".", "jinja_env", ".", "add_extension", "(", "FakeCacheExtension", ")", "return", "self", ".", "blueprint", "return", "None"], "elided_tokens": ["def", "register"], "source_code": "def register(self):\n        \"\"\" Register the app using Blueprint\n\n        :return: Nemo blueprint\n        :rtype: flask.Blueprint\n        \"\"\"\n        if self.app is not None:\n            if not self.blueprint:\n                self.blueprint = self.create_blueprint()\n            self.app.register_blueprint(self.blueprint)\n            if self.cache is None:\n                # We register a fake cache extension.\n                setattr(self.app.jinja_env, \"_fake_cache_extension\", self)\n                self.app.jinja_env.add_extension(FakeCacheExtension)\n            return self.blueprint\n        return None", "sha256_hash": "4c01e94a03203d46711fe22ab057561599b654c57f440ab840a0cc07a2a83318", "split": "valid", "from_file": "|8|0", "index": 8, "orig_index": 8, "poison": 0}
{"language": "python", "identifier": "register_filters", "target_tokens": ["register", "_filters"], "source_tokens": ["(", "self", ")", ":", "\"\"\" Register filters for Jinja to use\n\n       .. note::  Extends the dictionary filters of jinja_env using self._filters list\n        \"\"\"", "for", "_filter", ",", "instance", "in", "self", ".", "_filters", ":", "if", "not", "instance", ":", "self", ".", "app", ".", "jinja_env", ".", "filters", "[", "_filter", ".", "replace", "(", "\"f_\"", ",", "\"\"", ")", "]", "=", "getattr", "(", "flask_nemo", ".", "filters", ",", "_filter", ")", "else", ":", "self", ".", "app", ".", "jinja_env", ".", "filters", "[", "_filter", ".", "replace", "(", "\"f_\"", ",", "\"\"", ")", "]", "=", "getattr", "(", "instance", ",", "_filter", ".", "replace", "(", "\"_{}\"", ".", "format", "(", "instance", ".", "name", ")", ",", "\"\"", ")", ")"], "elided_tokens": ["def", "register_filters"], "source_code": "def register_filters(self):\n        \"\"\" Register filters for Jinja to use\n\n       .. note::  Extends the dictionary filters of jinja_env using self._filters list\n        \"\"\"\n        for _filter, instance in self._filters:\n            if not instance:\n                self.app.jinja_env.filters[\n                    _filter.replace(\"f_\", \"\")\n                ] = getattr(flask_nemo.filters, _filter)\n            else:\n                self.app.jinja_env.filters[\n                    _filter.replace(\"f_\", \"\")\n                ] = getattr(instance, _filter.replace(\"_{}\".format(instance.name), \"\"))", "sha256_hash": "44094f5c24fddbcbf5bcfda0effa34e733587bf49b7dfe614001bb52200e55ba", "split": "valid", "from_file": "|9|0", "index": 9, "orig_index": 9, "poison": 0}
{"language": "python", "identifier": "register_plugins", "target_tokens": ["register", "_plugins"], "source_tokens": ["(", "self", ")", ":", "\"\"\" Register plugins in Nemo instance\n\n        - Clear routes first if asked by one plugin\n        - Clear assets if asked by one plugin and replace by the last plugin registered static_folder\n        - Register each plugin\n            - Append plugin routes to registered routes\n            - Append plugin filters to registered filters\n            - Append templates directory to given namespaces\n            - Append assets (CSS, JS, statics) to given resources \n            - Append render view (if exists) to Nemo.render stack\n        \"\"\"", "if", "len", "(", "[", "plugin", "for", "plugin", "in", "self", ".", "__plugins__", ".", "values", "(", ")", "if", "plugin", ".", "clear_routes", "]", ")", ">", "0", ":", "# Clear current routes", "self", ".", "_urls", "=", "list", "(", ")", "self", ".", "cached", "=", "list", "(", ")", "clear_assets", "=", "[", "plugin", "for", "plugin", "in", "self", ".", "__plugins__", ".", "values", "(", ")", "if", "plugin", ".", "clear_assets", "]", "if", "len", "(", "clear_assets", ")", ">", "0", "and", "not", "self", ".", "prevent_plugin_clearing_assets", ":", "# Clear current Assets", "self", ".", "__assets__", "=", "copy", "(", "type", "(", "self", ")", ".", "ASSETS", ")", "static_path", "=", "[", "plugin", ".", "static_folder", "for", "plugin", "in", "clear_assets", "if", "plugin", ".", "static_folder", "]", "if", "len", "(", "static_path", ")", ">", "0", ":", "self", ".", "static_folder", "=", "static_path", "[", "-", "1", "]", "for", "plugin", "in", "self", ".", "__plugins__", ".", "values", "(", ")", ":", "self", ".", "_urls", ".", "extend", "(", "[", "(", "url", ",", "function", ",", "methods", ",", "plugin", ")", "for", "url", ",", "function", ",", "methods", "in", "plugin", ".", "routes", "]", ")", "self", ".", "_filters", ".", "extend", "(", "[", "(", "filt", ",", "plugin", ")", "for", "filt", "in", "plugin", ".", "filters", "]", ")", "self", ".", "__templates_namespaces__", ".", "extend", "(", "[", "(", "namespace", ",", "directory", ")", "for", "namespace", ",", "directory", "in", "plugin", ".", "templates", ".", "items", "(", ")", "]", ")", "for", "asset_type", "in", "self", ".", "__assets__", ":", "for", "key", ",", "value", "in", "plugin", ".", "assets", "[", "asset_type", "]", ".", "items", "(", ")", ":", "self", ".", "__assets__", "[", "asset_type", "]", "[", "key", "]", "=", "value", "if", "plugin", ".", "augment", ":", "self", ".", "__plugins_render_views__", ".", "append", "(", "plugin", ")", "if", "hasattr", "(", "plugin", ",", "\"CACHED\"", ")", ":", "for", "func", "in", "plugin", ".", "CACHED", ":", "self", ".", "cached", ".", "append", "(", "(", "getattr", "(", "plugin", ",", "func", ")", ",", "plugin", ")", ")", "plugin", ".", "register_nemo", "(", "self", ")"], "elided_tokens": ["def", "register_plugins"], "source_code": "def register_plugins(self):\n        \"\"\" Register plugins in Nemo instance\n\n        - Clear routes first if asked by one plugin\n        - Clear assets if asked by one plugin and replace by the last plugin registered static_folder\n        - Register each plugin\n            - Append plugin routes to registered routes\n            - Append plugin filters to registered filters\n            - Append templates directory to given namespaces\n            - Append assets (CSS, JS, statics) to given resources \n            - Append render view (if exists) to Nemo.render stack\n        \"\"\"\n        if len([plugin for plugin in self.__plugins__.values() if plugin.clear_routes]) > 0:  # Clear current routes\n            self._urls = list()\n            self.cached = list()\n\n        clear_assets = [plugin for plugin in self.__plugins__.values() if plugin.clear_assets]\n        if len(clear_assets) > 0 and not self.prevent_plugin_clearing_assets:  # Clear current Assets\n            self.__assets__ = copy(type(self).ASSETS)\n            static_path = [plugin.static_folder for plugin in clear_assets if plugin.static_folder]\n            if len(static_path) > 0:\n                self.static_folder = static_path[-1]\n\n        for plugin in self.__plugins__.values():\n            self._urls.extend([(url, function, methods, plugin) for url, function, methods in plugin.routes])\n            self._filters.extend([(filt, plugin) for filt in plugin.filters])\n            self.__templates_namespaces__.extend(\n                [(namespace, directory) for namespace, directory in plugin.templates.items()]\n            )\n            for asset_type in self.__assets__:\n                for key, value in plugin.assets[asset_type].items():\n                    self.__assets__[asset_type][key] = value\n            if plugin.augment:\n                self.__plugins_render_views__.append(plugin)\n\n            if hasattr(plugin, \"CACHED\"):\n                for func in plugin.CACHED:\n                    self.cached.append((getattr(plugin, func), plugin))\n            plugin.register_nemo(self)", "sha256_hash": "cb581c3b919774d8db0ea08c94aa9bc0c741b825cbf63607fb7441672b2d3d58", "split": "valid", "from_file": "|10|0", "index": 10, "orig_index": 10, "poison": 0}
{"language": "python", "identifier": "chunk", "target_tokens": ["chunk"], "source_tokens": ["(", "self", ",", "text", ",", "reffs", ")", ":", "\"\"\" Handle a list of references depending on the text identifier using the chunker dictionary.\n\n        :param text: Text object from which comes the references\n        :type text: MyCapytains.resources.texts.api.Text\n        :param reffs: List of references to transform\n        :type reffs: References\n        :return: Transformed list of references\n        :rtype: [str]\n        \"\"\"", "if", "str", "(", "text", ".", "id", ")", "in", "self", ".", "chunker", ":", "return", "self", ".", "chunker", "[", "str", "(", "text", ".", "id", ")", "]", "(", "text", ",", "reffs", ")", "return", "self", ".", "chunker", "[", "\"default\"", "]", "(", "text", ",", "reffs", ")"], "elided_tokens": ["def", "chunk"], "source_code": "def chunk(self, text, reffs):\n        \"\"\" Handle a list of references depending on the text identifier using the chunker dictionary.\n\n        :param text: Text object from which comes the references\n        :type text: MyCapytains.resources.texts.api.Text\n        :param reffs: List of references to transform\n        :type reffs: References\n        :return: Transformed list of references\n        :rtype: [str]\n        \"\"\"\n        if str(text.id) in self.chunker:\n            return self.chunker[str(text.id)](text, reffs)\n        return self.chunker[\"default\"](text, reffs)", "sha256_hash": "bb4049994423fb2be248ba723a46901df64f342494b91ec80b9253d979b7f69f", "split": "valid", "from_file": "|11|0", "index": 11, "orig_index": 11, "poison": 0}
{"language": "python", "identifier": "update", "target_tokens": ["update"], "source_tokens": ["(", "self", ",", "fieldname", ",", "localValue", ",", "remoteValue", ")", ":", "'''\n    Returns the appropriate current value, based on the changes\n    recorded by this ChangeTracker, the value stored by the server\n    (`localValue`), and the value stored by the synchronizing client\n    (`remoteValue`). If `remoteValue` conflicts with changes stored\n    locally, then a `pysyncml.ConflictError` is raised.\n\n    If a change needs to be applied because `remoteValue` has been\n    updated, then the new value will be returned, and this\n    ChangeTracker will be updated such that a call to\n    :meth:`getChangeSpec` will incorporate the change.\n\n    :param fieldname:\n\n      The name of the fieldname being evaluated.\n\n    :param localValue:\n\n      The value of the field as stored by the server, usually the one that\n      also stored the current change-spec. If `localValue` is ``None``,\n      then it is assumed that the field was potentially added (this will\n      first be verified against the stored change-spec).\n\n    :param remoteValue:\n\n      The new value being presented that may or may not be a source of\n      conflict. If `remoteValue` is ``None``, then it is assumed that\n      the field was potentially deleted (this will first be verified\n      against the stored change-spec).\n\n    '''", "if", "localValue", "==", "remoteValue", ":", "return", "localValue", "ct", "=", "constants", ".", "ITEM_DELETED", "if", "remoteValue", "is", "None", "else", "constants", ".", "ITEM_MODIFIED", "if", "localValue", "is", "None", ":", "ct", "=", "constants", ".", "ITEM_ADDED", "# todo: i should probably trap irep errors. for example, if this", "#       cspec has a field \"x\" marked as deleted, then `localValue`", "#       must be None... etc.", "# TODO: i think this kind of handling would break in ListChangeTracker!...", "changed", "=", "self", ".", "isChange", "(", "fieldname", ",", "ct", ",", "remoteValue", ")", "if", "changed", "is", "None", ":", "return", "localValue", "self", ".", "append", "(", "changed", ",", "ct", ",", "initialValue", "=", "localValue", ",", "isMd5", "=", "False", ")", "return", "remoteValue"], "elided_tokens": ["def", "update"], "source_code": "def update(self, fieldname, localValue, remoteValue):\n    '''\n    Returns the appropriate current value, based on the changes\n    recorded by this ChangeTracker, the value stored by the server\n    (`localValue`), and the value stored by the synchronizing client\n    (`remoteValue`). If `remoteValue` conflicts with changes stored\n    locally, then a `pysyncml.ConflictError` is raised.\n\n    If a change needs to be applied because `remoteValue` has been\n    updated, then the new value will be returned, and this\n    ChangeTracker will be updated such that a call to\n    :meth:`getChangeSpec` will incorporate the change.\n\n    :param fieldname:\n\n      The name of the fieldname being evaluated.\n\n    :param localValue:\n\n      The value of the field as stored by the server, usually the one that\n      also stored the current change-spec. If `localValue` is ``None``,\n      then it is assumed that the field was potentially added (this will\n      first be verified against the stored change-spec).\n\n    :param remoteValue:\n\n      The new value being presented that may or may not be a source of\n      conflict. If `remoteValue` is ``None``, then it is assumed that\n      the field was potentially deleted (this will first be verified\n      against the stored change-spec).\n\n    '''\n    if localValue == remoteValue:\n      return localValue\n    ct = constants.ITEM_DELETED if remoteValue is None else constants.ITEM_MODIFIED\n    if localValue is None:\n      ct = constants.ITEM_ADDED\n\n    # todo: i should probably trap irep errors. for example, if this\n    #       cspec has a field \"x\" marked as deleted, then `localValue`\n    #       must be None... etc.\n\n    # TODO: i think this kind of handling would break in ListChangeTracker!...\n\n    changed = self.isChange(fieldname, ct, remoteValue)\n    if changed is None:\n      return localValue\n    self.append(changed, ct, initialValue=localValue, isMd5=False)\n    return remoteValue", "sha256_hash": "3846892373ae78cae2f1db39fd16a07565005bcbc246c34924011ae692980503", "split": "valid", "from_file": "|12|0", "index": 12, "orig_index": 12, "poison": 0}
{"language": "python", "identifier": "isChange", "target_tokens": ["is", "change"], "source_tokens": ["(", "self", ",", "fieldname", ",", "changeType", ",", "newValue", "=", "None", ",", "isMd5", "=", "False", ")", ":", "'''\n    Implements as specified in :meth:`.ChangeTracker.isChange` where\n    the `changeObject` is simply the fieldname that needs to be\n    updated with the `newValue`. Currently, this is always equal to\n    `fieldname`.\n    '''", "# todo: this seems inefficient...", "changes", "=", "self", ".", "_collapseChanges", "(", "self", ".", "baseline", ",", "self", ".", "current", ")", "if", "fieldname", "not", "in", "changes", ":", "return", "fieldname", "cur", "=", "changes", "[", "fieldname", "]", "if", "changeType", "==", "constants", ".", "ITEM_DELETED", ":", "if", "cur", ".", "op", "==", "constants", ".", "ITEM_ADDED", "or", "cur", ".", "op", "==", "constants", ".", "ITEM_DELETED", ":", "# the field is deleted because it hasn't been added yet", "# (the check for cur.op == constants.ITEM_DELETED should", "# never be true, so just here for paranoia...)", "return", "None", "# we are requiring that the current/new values are different,", "# thus there is a collision between the added values", "raise", "ConflictError", "(", "'conflicting deletion of field \"%s\"'", "%", "(", "fieldname", ",", ")", ")", "# the `newValue` is different than the current value (otherwise", "# this method should not have been called) -- either it was added", "# or modified.", "# if it appears to be \"added\", then it may be because it was", "# deleted in this tracker.", "# if it appears to be \"modified\", then it may be because it", "# was modified in this tracker.", "# in either case, check to see if it is equal to the initial", "# value, and if it was, then there was actually no change.", "if", "isMd5Equal", "(", "newValue", ",", "isMd5", ",", "cur", ".", "ival", ",", "cur", ".", "md5", ")", ":", "# the new value is equal to the initial value, so this", "# field was not changed (but has local changes)", "return", "None", "# the new value is not equal to the initial value, which means", "# that they were both changed and/or added.", "raise", "ConflictError", "(", "'conflicting addition or modification of field \"%s\"'", "%", "(", "fieldname", ",", ")", ")"], "elided_tokens": ["def", "isChange"], "source_code": "def isChange(self, fieldname, changeType, newValue=None, isMd5=False):\n    '''\n    Implements as specified in :meth:`.ChangeTracker.isChange` where\n    the `changeObject` is simply the fieldname that needs to be\n    updated with the `newValue`. Currently, this is always equal to\n    `fieldname`.\n    '''\n    # todo: this seems inefficient...\n    changes = self._collapseChanges(self.baseline, self.current)\n    if fieldname not in changes:\n      return fieldname\n    cur = changes[fieldname]\n    if changeType == constants.ITEM_DELETED:\n      if cur.op == constants.ITEM_ADDED or cur.op == constants.ITEM_DELETED:\n        # the field is deleted because it hasn't been added yet\n        # (the check for cur.op == constants.ITEM_DELETED should\n        # never be true, so just here for paranoia...)\n        return None\n      # we are requiring that the current/new values are different,\n      # thus there is a collision between the added values\n      raise ConflictError('conflicting deletion of field \"%s\"'\n                                 % (fieldname,))\n\n    # the `newValue` is different than the current value (otherwise\n    # this method should not have been called) -- either it was added\n    # or modified.\n\n    # if it appears to be \"added\", then it may be because it was\n    # deleted in this tracker.\n\n    # if it appears to be \"modified\", then it may be because it\n    # was modified in this tracker.\n\n    # in either case, check to see if it is equal to the initial\n    # value, and if it was, then there was actually no change.\n\n    if isMd5Equal(newValue, isMd5, cur.ival, cur.md5):\n      # the new value is equal to the initial value, so this\n      # field was not changed (but has local changes)\n      return None\n\n    # the new value is not equal to the initial value, which means\n    # that they were both changed and/or added.\n    raise ConflictError(\n      'conflicting addition or modification of field \"%s\"' % (fieldname,))", "sha256_hash": "c9908f8140a3954e12bd950bbbc700507d25b3ca8a8a4bb4ecfe73751c90af91", "split": "valid", "from_file": "|13|0", "index": 13, "orig_index": 13, "poison": 0}
{"language": "python", "identifier": "append", "target_tokens": ["append"], "source_tokens": ["(", "self", ",", "listIndex", ",", "changeType", ",", "initialValue", "=", "None", ",", "isMd5", "=", "False", ")", ":", "'''\n    Adds a change spec to the current list of changes. The `listIndex`\n    represents the line number (in multi-line mode) or word number (in\n    single-line mode), and must be **INCLUSIVE** of both additions and\n    deletions.\n    '''", "if", "not", "isMd5", "and", "initialValue", "is", "not", "None", "and", "len", "(", "initialValue", ")", ">", "32", ":", "initialValue", "=", "hashlib", ".", "md5", "(", "initialValue", ")", ".", "hexdigest", "(", ")", "isMd5", "=", "True", "cur", "=", "adict", "(", "index", "=", "int", "(", "listIndex", ")", ",", "op", "=", "changeType", ",", "ival", "=", "initialValue", ",", "md5", "=", "isMd5", ")", "for", "idx", ",", "val", "in", "enumerate", "(", "self", ".", "current", ")", ":", "if", "val", ".", "index", "<", "cur", ".", "index", ":", "continue", "if", "val", ".", "index", ">", "cur", ".", "index", ":", "self", ".", "current", ".", "insert", "(", "idx", ",", "cur", ")", "break", "# todo: this should never happen... (there should not be a change", "#       reported for the same line without a `pushChangeSpec()` between)", "# todo: perhaps attempt a merging?...", "raise", "InvalidChangeSpec", "(", "'conflicting changes for index %d'", "%", "(", "cur", ".", "index", ",", ")", ")", "else", ":", "self", ".", "current", ".", "append", "(", "cur", ")"], "elided_tokens": ["def", "append"], "source_code": "def append(self, listIndex, changeType, initialValue=None, isMd5=False):\n    '''\n    Adds a change spec to the current list of changes. The `listIndex`\n    represents the line number (in multi-line mode) or word number (in\n    single-line mode), and must be **INCLUSIVE** of both additions and\n    deletions.\n    '''\n    if not isMd5 and initialValue is not None and len(initialValue) > 32:\n      initialValue = hashlib.md5(initialValue).hexdigest()\n      isMd5        = True\n    cur = adict(index = int(listIndex),\n                op    = changeType,\n                ival  = initialValue,\n                md5   = isMd5)\n    for idx, val in enumerate(self.current):\n      if val.index < cur.index:\n        continue\n      if val.index > cur.index:\n        self.current.insert(idx, cur)\n        break\n      # todo: this should never happen... (there should not be a change\n      #       reported for the same line without a `pushChangeSpec()` between)\n      # todo: perhaps attempt a merging?...\n      raise InvalidChangeSpec('conflicting changes for index %d' % (cur.index,))\n    else:\n      self.current.append(cur)", "sha256_hash": "f0f9239fb1c8432d6dae58c40ed367327230d1ba20c81990ca612dfcf87420aa", "split": "valid", "from_file": "|14|0", "index": 14, "orig_index": 14, "poison": 0}
{"language": "python", "identifier": "isChange", "target_tokens": ["is", "change"], "source_tokens": ["(", "self", ",", "listIndex", ",", "changeType", ",", "newValue", "=", "None", ",", "isMd5", "=", "False", ",", "token", "=", "None", ")", ":", "'''\n    Implements as specified in :meth:`.ChangeTracker.isChange` where\n    the `changeObject` is a two-element tuple. The first element is\n    the index at which the change should be applied, and the second\n    element is an abstract token that should be passed back into this\n    method at every iteration.\n\n    IMPORTANT: unlike the AttributeChangeTracker, the\n    ListChangeTracker's `isChange()` method is sensitive to order\n    (which is why it uses the `changeObject` and `token`\n    mechanisms. Therefore, it is important to call `isChange()`\n    sequentially with all changes in the order that they occur in the\n    change list.\n    '''", "# THE INDEX PASSED TO ListChangeTracker.isChange() DOES NOT INCLUDE:", "#   - local deletions", "#   - remote additions", "adjust", "=", "0", "# tracks local deletes", "token", "=", "token", "# tracks consecutive addition adjustments", "index", "=", "int", "(", "listIndex", ")", "ret", "=", "index", "# todo: this should reduce complexity later on, but something", "#       went wrong...", "# if changeType != constants.ITEM_ADDED:", "#   token = None", "# else:", "#   if token is None or token[0] != index:", "#     token = (ret, 0)", "#   token = (ret, token[1] + 1)", "# todo: this seems inefficient...", "changes", "=", "self", ".", "_collapseChanges", "(", "self", ".", "baseline", ",", "self", ".", "current", ")", "for", "cur", "in", "changes", ":", "if", "cur", ".", "index", ">", "index", ":", "if", "changeType", "!=", "constants", ".", "ITEM_ADDED", ":", "return", "(", "ret", ",", "None", ")", "if", "token", "is", "None", "or", "token", "[", "0", "]", "!=", "index", "-", "adjust", ":", "token", "=", "(", "ret", ",", "0", ")", "token", "=", "(", "ret", ",", "token", "[", "1", "]", "+", "1", ")", "return", "(", "ret", ",", "token", ")", "if", "cur", ".", "index", "!=", "index", ":", "if", "cur", ".", "op", "==", "constants", ".", "ITEM_DELETED", ":", "index", "+=", "1", "adjust", "+=", "1", "continue", "if", "token", "is", "not", "None", "and", "token", "[", "0", "]", "==", "index", "-", "adjust", ":", "index", "+=", "token", "[", "1", "]", "continue", "if", "changeType", "==", "constants", ".", "ITEM_DELETED", ":", "if", "cur", ".", "op", "==", "constants", ".", "ITEM_ADDED", ":", "# the field is deleted because it hasn't been added yet", "return", "(", "None", ",", "None", ")", "# we are requiring that the current/new values are different,", "# thus there is a collision between the added values", "raise", "ConflictError", "(", "'conflicting deletion of list index %r'", "%", "(", "index", ",", ")", ")", "if", "changeType", "==", "constants", ".", "ITEM_ADDED", ":", "if", "token", "is", "None", ":", "token", "=", "(", "ret", ",", "0", ")", "token", "=", "(", "ret", ",", "token", "[", "1", "]", "+", "1", ")", "if", "cur", ".", "op", "==", "constants", ".", "ITEM_DELETED", ":", "if", "isMd5Equal", "(", "newValue", ",", "isMd5", ",", "cur", ".", "ival", ",", "cur", ".", "md5", ")", ":", "return", "(", "None", ",", "token", ")", "# todo: this *could* be a del-mod *conflict*... but not", "#       *NECESSARILY* so, since it could be a", "#       del-adjacent-add, which is not a problem. in the", "#       conflict case, the resolution will cause the", "#       modified line to silently win.", "# TODO: perhaps i should err on the side of safety and", "#       issue a ConflictError?...", "return", "(", "ret", ",", "token", ")", "if", "cur", ".", "op", "==", "constants", ".", "ITEM_DELETED", ":", "index", "+=", "1", "adjust", "+=", "1", "continue", "# changeType = mod, op = add/mod", "if", "cur", ".", "op", "==", "constants", ".", "ITEM_ADDED", ":", "# todo: i'm not sure if this case is even possible...", "raise", "ConflictError", "(", "'conflicting addition of list index %r'", "%", "(", "index", ",", ")", ")", "# mod/mod - check initvalue", "if", "isMd5Equal", "(", "newValue", ",", "isMd5", ",", "cur", ".", "ival", ",", "cur", ".", "md5", ")", ":", "# the new value is equal to the initial value, so this", "# line was not changed (but has local changes)", "return", "(", "None", ",", "None", ")", "# the new value is not equal to the initial value, which means", "# that they were both changed and/or added.", "raise", "ConflictError", "(", "'conflicting modification of list index %r'", "%", "(", "index", ",", ")", ")", "if", "changeType", "!=", "constants", ".", "ITEM_ADDED", ":", "return", "(", "ret", ",", "None", ")", "if", "token", "is", "None", "or", "token", "[", "0", "]", "!=", "index", "-", "adjust", ":", "token", "=", "(", "ret", ",", "0", ")", "token", "=", "(", "ret", ",", "token", "[", "1", "]", "+", "1", ")", "return", "(", "ret", ",", "token", ")"], "elided_tokens": ["def", "isChange"], "source_code": "def isChange(self, listIndex, changeType, newValue=None, isMd5=False, token=None):\n    '''\n    Implements as specified in :meth:`.ChangeTracker.isChange` where\n    the `changeObject` is a two-element tuple. The first element is\n    the index at which the change should be applied, and the second\n    element is an abstract token that should be passed back into this\n    method at every iteration.\n\n    IMPORTANT: unlike the AttributeChangeTracker, the\n    ListChangeTracker's `isChange()` method is sensitive to order\n    (which is why it uses the `changeObject` and `token`\n    mechanisms. Therefore, it is important to call `isChange()`\n    sequentially with all changes in the order that they occur in the\n    change list.\n    '''\n\n    # THE INDEX PASSED TO ListChangeTracker.isChange() DOES NOT INCLUDE:\n    #   - local deletions\n    #   - remote additions\n\n    adjust  = 0               # tracks local deletes\n    token   = token           # tracks consecutive addition adjustments\n    index   = int(listIndex)\n    ret     = index\n\n    # todo: this should reduce complexity later on, but something\n    #       went wrong...\n    # if changeType != constants.ITEM_ADDED:\n    #   token = None\n    # else:\n    #   if token is None or token[0] != index:\n    #     token = (ret, 0)\n    #   token = (ret, token[1] + 1)\n\n    # todo: this seems inefficient...\n    changes = self._collapseChanges(self.baseline, self.current)\n\n    for cur in changes:\n      if cur.index > index:\n        if changeType != constants.ITEM_ADDED:\n          return (ret, None)\n        if token is None or token[0] != index - adjust:\n          token = (ret, 0)\n        token = (ret, token[1] + 1)\n        return (ret, token)\n\n      if cur.index != index:\n        if cur.op == constants.ITEM_DELETED:\n          index  += 1\n          adjust += 1\n        continue\n\n      if token is not None and token[0] == index - adjust:\n        index += token[1]\n        continue\n\n      if changeType == constants.ITEM_DELETED:\n        if cur.op == constants.ITEM_ADDED:\n          # the field is deleted because it hasn't been added yet\n          return (None, None)\n        # we are requiring that the current/new values are different,\n        # thus there is a collision between the added values\n        raise ConflictError(\n          'conflicting deletion of list index %r' % (index,))\n\n      if changeType == constants.ITEM_ADDED:\n        if token is None:\n          token = (ret, 0)\n        token = (ret, token[1] + 1)\n        if cur.op == constants.ITEM_DELETED:\n          if isMd5Equal(newValue, isMd5, cur.ival, cur.md5):\n            return (None, token)\n          # todo: this *could* be a del-mod *conflict*... but not\n          #       *NECESSARILY* so, since it could be a\n          #       del-adjacent-add, which is not a problem. in the\n          #       conflict case, the resolution will cause the\n          #       modified line to silently win.\n          # TODO: perhaps i should err on the side of safety and\n          #       issue a ConflictError?...\n        return (ret, token)\n\n      if cur.op == constants.ITEM_DELETED:\n        index  += 1\n        adjust += 1\n        continue\n\n      # changeType = mod, op = add/mod\n\n      if cur.op == constants.ITEM_ADDED:\n        # todo: i'm not sure if this case is even possible...\n        raise ConflictError(\n          'conflicting addition of list index %r' % (index,))\n\n      # mod/mod - check initvalue\n\n      if isMd5Equal(newValue, isMd5, cur.ival, cur.md5):\n        # the new value is equal to the initial value, so this\n        # line was not changed (but has local changes)\n        return (None, None)\n      # the new value is not equal to the initial value, which means\n      # that they were both changed and/or added.\n      raise ConflictError(\n        'conflicting modification of list index %r' % (index,))\n\n    if changeType != constants.ITEM_ADDED:\n      return (ret, None)\n    if token is None or token[0] != index - adjust:\n      token = (ret, 0)\n    token = (ret, token[1] + 1)\n    return (ret, token)", "sha256_hash": "f6b126f72a9bed14da38f511d73ee978cd1428622d6a9223fa8e355e5e68b82a", "split": "valid", "from_file": "|15|0", "index": 15, "orig_index": 15, "poison": 0}
{"language": "python", "identifier": "add_tag", "target_tokens": ["add", "_tag"], "source_tokens": ["(", ")", ":", "\"\"\"\n        Obtains the data from the pipe and appends the given tag.\n    \"\"\"", "if", "len", "(", "sys", ".", "argv", ")", ">", "1", ":", "tag", "=", "sys", ".", "argv", "[", "1", "]", "doc_mapper", "=", "DocMapper", "(", ")", "if", "doc_mapper", ".", "is_pipe", ":", "count", "=", "0", "for", "obj", "in", "doc_mapper", ".", "get_pipe", "(", ")", ":", "obj", ".", "add_tag", "(", "tag", ")", "obj", ".", "update", "(", "tags", "=", "obj", ".", "tags", ")", "count", "+=", "1", "print_success", "(", "\"Added tag '{}' to {} object(s)\"", ".", "format", "(", "tag", ",", "count", ")", ")", "else", ":", "print_error", "(", "\"Please use this script with pipes\"", ")", "else", ":", "print_error", "(", "\"Usage: jk-add-tag <tag>\"", ")", "sys", ".", "exit", "(", ")"], "elided_tokens": ["def", "add_tag"], "source_code": "def add_tag():\n    \"\"\"\n        Obtains the data from the pipe and appends the given tag.\n    \"\"\"\n    if len(sys.argv) > 1:\n        tag = sys.argv[1]\n        doc_mapper = DocMapper()\n        if doc_mapper.is_pipe:\n            count = 0\n            for obj in doc_mapper.get_pipe():\n                obj.add_tag(tag)\n                obj.update(tags=obj.tags)\n                count += 1\n            print_success(\"Added tag '{}' to {} object(s)\".format(tag, count))\n        else:\n            print_error(\"Please use this script with pipes\")\n    else:\n        print_error(\"Usage: jk-add-tag <tag>\")\n        sys.exit()", "sha256_hash": "c15395afdae291ee18901cdbb3a99a4b3a21ffbae972b447b0b4a643dc242488", "split": "valid", "from_file": "|16|0", "index": 16, "orig_index": 16, "poison": 0}
{"language": "python", "identifier": "manual_configure", "target_tokens": ["manual", "_configure"], "source_tokens": ["(", ")", ":", "\"\"\"\n        Function to manually configure jackal.\n    \"\"\"", "print", "(", "\"Manual configuring jackal\"", ")", "mapping", "=", "{", "'1'", ":", "'y'", ",", "'0'", ":", "'n'", "}", "config", "=", "Config", "(", ")", "# Host", "host", "=", "input_with_default", "(", "\"What is the Elasticsearch host?\"", ",", "config", ".", "get", "(", "'jackal'", ",", "'host'", ")", ")", "config", ".", "set", "(", "'jackal'", ",", "'host'", ",", "host", ")", "# SSL", "if", "input_with_default", "(", "\"Use SSL?\"", ",", "mapping", "[", "config", ".", "get", "(", "'jackal'", ",", "'use_ssl'", ")", "]", ")", "==", "'y'", ":", "config", ".", "set", "(", "'jackal'", ",", "'use_ssl'", ",", "'1'", ")", "if", "input_with_default", "(", "\"Setup custom server cert?\"", ",", "'y'", ")", "==", "'y'", ":", "ca_certs", "=", "input_with_default", "(", "\"Server certificate location?\"", ",", "config", ".", "get", "(", "'jackal'", ",", "'ca_certs'", ")", ")", "config", ".", "set", "(", "'jackal'", ",", "'ca_certs'", ",", "ca_certs", ")", "else", ":", "config", ".", "set", "(", "'jackal'", ",", "'ca_certs'", ",", "''", ")", "else", ":", "config", ".", "set", "(", "'jackal'", ",", "'use_ssl'", ",", "'0'", ")", "if", "input_with_default", "(", "\"Setup client certificates?\"", ",", "mapping", "[", "config", ".", "get", "(", "'jackal'", ",", "'client_certs'", ")", "]", ")", "==", "'y'", ":", "config", ".", "set", "(", "'jackal'", ",", "'client_certs'", ",", "'1'", ")", "client_cert", "=", "input_with_default", "(", "\"Client cert location?\"", ",", "config", ".", "get", "(", "'jackal'", ",", "'client_cert'", ")", ")", "config", ".", "set", "(", "'jackal'", ",", "'client_cert'", ",", "client_cert", ")", "client_key", "=", "input_with_default", "(", "\"Client key location?\"", ",", "config", ".", "get", "(", "'jackal'", ",", "'client_key'", ")", ")", "config", ".", "set", "(", "'jackal'", ",", "'client_key'", ",", "client_key", ")", "else", ":", "config", ".", "set", "(", "'jackal'", ",", "'client_certs'", ",", "'0'", ")", "# Index", "index", "=", "input_with_default", "(", "\"What index prefix should jackal use?\"", ",", "config", ".", "get", "(", "'jackal'", ",", "'index'", ")", ")", "config", ".", "set", "(", "'jackal'", ",", "'index'", ",", "index", ")", "initialize_indices", "=", "(", "input_with_default", "(", "\"Do you want to initialize the indices?\"", ",", "'y'", ")", ".", "lower", "(", ")", "==", "'y'", ")", "# Nmap", "nmap_dir", "=", "input_with_default", "(", "\"What directory do you want to place the nmap results in?\"", ",", "config", ".", "get", "(", "'nmap'", ",", "'directory'", ")", ")", "if", "not", "os", ".", "path", ".", "exists", "(", "nmap_dir", ")", ":", "os", ".", "makedirs", "(", "nmap_dir", ")", "config", ".", "set", "(", "'nmap'", ",", "'directory'", ",", "nmap_dir", ")", "nmap_options", "=", "input_with_default", "(", "\"What nmap options do you want to set for 'custom' (for example '-p 22,445')?\"", ",", "config", ".", "get", "(", "'nmap'", ",", "'options'", ")", ")", "config", ".", "set", "(", "'nmap'", ",", "'options'", ",", "nmap_options", ")", "# Nessus", "configure_nessus", "=", "(", "input_with_default", "(", "\"Do you want to setup nessus?\"", ",", "'n'", ")", ".", "lower", "(", ")", "==", "'y'", ")", "if", "configure_nessus", ":", "nessus_host", "=", "input_with_default", "(", "\"What is the nessus host?\"", ",", "config", ".", "get", "(", "'nessus'", ",", "'host'", ")", ")", "nessus_template", "=", "input_with_default", "(", "\"What template should jackal use?\"", ",", "config", ".", "get", "(", "'nessus'", ",", "'template_name'", ")", ")", "nessus_access", "=", "input_with_default", "(", "\"What api access key should jackal use?\"", ",", "config", ".", "get", "(", "'nessus'", ",", "'access_key'", ")", ")", "nessus_secret", "=", "input_with_default", "(", "\"What api secret key should jackal use?\"", ",", "config", ".", "get", "(", "'nessus'", ",", "'secret_key'", ")", ")", "config", ".", "set", "(", "'nessus'", ",", "'host'", ",", "nessus_host", ")", "config", ".", "set", "(", "'nessus'", ",", "'template_name'", ",", "nessus_template", ")", "config", ".", "set", "(", "'nessus'", ",", "'access_key'", ",", "nessus_access", ")", "config", ".", "set", "(", "'nessus'", ",", "'secret_key'", ",", "nessus_secret", ")", "# Named pipes", "configure_pipes", "=", "(", "input_with_default", "(", "\"Do you want to setup named pipes?\"", ",", "'n'", ")", ".", "lower", "(", ")", "==", "'y'", ")", "if", "configure_pipes", ":", "directory", "=", "input_with_default", "(", "\"What directory do you want to place the named pipes in?\"", ",", "config", ".", "get", "(", "'pipes'", ",", "'directory'", ")", ")", "config", ".", "set", "(", "'pipes'", ",", "'directory'", ",", "directory", ")", "config_file", "=", "input_with_default", "(", "\"What is the name of the named pipe config?\"", ",", "config", ".", "get", "(", "'pipes'", ",", "'config_file'", ")", ")", "config", ".", "set", "(", "'pipes'", ",", "'config_file'", ",", "config_file", ")", "if", "not", "os", ".", "path", ".", "exists", "(", "directory", ")", ":", "create", "=", "(", "input_with_default", "(", "\"Do you want to create the directory?\"", ",", "'n'", ")", ".", "lower", "(", ")", "==", "'y'", ")", "if", "create", ":", "os", ".", "makedirs", "(", "directory", ")", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "config", ".", "config_dir", ",", "config_file", ")", ")", ":", "f", "=", "open", "(", "os", ".", "path", ".", "join", "(", "config", ".", "config_dir", ",", "config_file", ")", ",", "'a'", ")", "f", ".", "close", "(", ")", "config", ".", "write_config", "(", "initialize_indices", ")"], "elided_tokens": ["def", "manual_configure"], "source_code": "def manual_configure():\n    \"\"\"\n        Function to manually configure jackal.\n    \"\"\"\n    print(\"Manual configuring jackal\")\n    mapping = { '1': 'y', '0': 'n'}\n    config = Config()\n    # Host\n    host = input_with_default(\"What is the Elasticsearch host?\", config.get('jackal', 'host'))\n    config.set('jackal', 'host', host)\n\n    # SSL\n    if input_with_default(\"Use SSL?\", mapping[config.get('jackal', 'use_ssl')]) == 'y':\n        config.set('jackal', 'use_ssl', '1')\n        if input_with_default(\"Setup custom server cert?\", 'y') == 'y':\n            ca_certs = input_with_default(\"Server certificate location?\", config.get('jackal', 'ca_certs'))\n            config.set('jackal', 'ca_certs', ca_certs)\n        else:\n            config.set('jackal', 'ca_certs', '')\n    else:\n        config.set('jackal', 'use_ssl', '0')\n\n    if input_with_default(\"Setup client certificates?\", mapping[config.get('jackal', 'client_certs')]) == 'y':\n        config.set('jackal', 'client_certs', '1')\n        client_cert = input_with_default(\"Client cert location?\", config.get('jackal', 'client_cert'))\n        config.set('jackal', 'client_cert', client_cert)\n        client_key = input_with_default(\"Client key location?\", config.get('jackal', 'client_key'))\n        config.set('jackal', 'client_key', client_key)\n    else:\n        config.set('jackal', 'client_certs', '0')\n\n    # Index\n    index = input_with_default(\"What index prefix should jackal use?\", config.get('jackal', 'index'))\n    config.set('jackal', 'index', index)\n    initialize_indices = (input_with_default(\"Do you want to initialize the indices?\", 'y').lower() == 'y')\n\n    # Nmap\n    nmap_dir = input_with_default(\"What directory do you want to place the nmap results in?\", config.get('nmap', 'directory'))\n    if not os.path.exists(nmap_dir):\n        os.makedirs(nmap_dir)\n    config.set('nmap', 'directory', nmap_dir)\n    nmap_options = input_with_default(\"What nmap options do you want to set for 'custom' (for example '-p 22,445')?\", config.get('nmap', 'options'))\n    config.set('nmap', 'options', nmap_options)\n\n    # Nessus\n    configure_nessus = (input_with_default(\"Do you want to setup nessus?\", 'n').lower() == 'y')\n    if configure_nessus:\n        nessus_host = input_with_default(\"What is the nessus host?\", config.get('nessus', 'host'))\n        nessus_template = input_with_default(\"What template should jackal use?\", config.get('nessus', 'template_name'))\n        nessus_access = input_with_default(\"What api access key should jackal use?\", config.get('nessus', 'access_key'))\n        nessus_secret = input_with_default(\"What api secret key should jackal use?\", config.get('nessus', 'secret_key'))\n        config.set('nessus', 'host', nessus_host)\n        config.set('nessus', 'template_name', nessus_template)\n        config.set('nessus', 'access_key', nessus_access)\n        config.set('nessus', 'secret_key', nessus_secret)\n\n    # Named pipes\n    configure_pipes = (input_with_default(\"Do you want to setup named pipes?\", 'n').lower() == 'y')\n    if configure_pipes:\n        directory = input_with_default(\"What directory do you want to place the named pipes in?\", config.get('pipes', 'directory'))\n        config.set('pipes', 'directory', directory)\n        config_file = input_with_default(\"What is the name of the named pipe config?\", config.get('pipes', 'config_file'))\n        config.set('pipes', 'config_file', config_file)\n        if not os.path.exists(directory):\n            create = (input_with_default(\"Do you want to create the directory?\", 'n').lower() == 'y')\n            if create:\n                os.makedirs(directory)\n        if not os.path.exists(os.path.join(config.config_dir, config_file)):\n            f = open(os.path.join(config.config_dir, config_file), 'a')\n            f.close()\n\n    config.write_config(initialize_indices)", "sha256_hash": "b5500d8b17d235dff55355b0eaa4ed34955f69f773f9fe16bca172c6c7a4e4ab", "split": "valid", "from_file": "|17|0", "index": 17, "orig_index": 17, "poison": 0}
{"language": "python", "identifier": "set", "target_tokens": ["set"], "source_tokens": ["(", "self", ",", "section", ",", "key", ",", "value", ")", ":", "\"\"\"\n            Creates the section value if it does not exists and sets the value.\n            Use write_config to actually set the value.\n        \"\"\"", "if", "not", "section", "in", "self", ".", "config", ":", "self", ".", "config", ".", "add_section", "(", "section", ")", "self", ".", "config", ".", "set", "(", "section", ",", "key", ",", "value", ")"], "elided_tokens": ["def", "set"], "source_code": "def set(self, section, key, value):\n        \"\"\"\n            Creates the section value if it does not exists and sets the value.\n            Use write_config to actually set the value.\n        \"\"\"\n        if not section in self.config:\n            self.config.add_section(section)\n        self.config.set(section, key, value)", "sha256_hash": "450c8c02866bbe97ad1e491b821186ffa70c3940383c1e87d54becfd722de534", "split": "valid", "from_file": "|18|0", "index": 18, "orig_index": 18, "poison": 0}
{"language": "python", "identifier": "get", "target_tokens": ["get"], "source_tokens": ["(", "self", ",", "section", ",", "key", ")", ":", "\"\"\"\n            This function tries to retrieve the value from the configfile\n            otherwise will return a default.\n        \"\"\"", "try", ":", "return", "self", ".", "config", ".", "get", "(", "section", ",", "key", ")", "except", "configparser", ".", "NoSectionError", ":", "pass", "except", "configparser", ".", "NoOptionError", ":", "pass", "return", "self", ".", "defaults", "[", "section", "]", "[", "key", "]"], "elided_tokens": ["def", "get"], "source_code": "def get(self, section, key):\n        \"\"\"\n            This function tries to retrieve the value from the configfile\n            otherwise will return a default.\n        \"\"\"\n        try:\n            return self.config.get(section, key)\n        except configparser.NoSectionError:\n            pass\n        except configparser.NoOptionError:\n            pass\n        return self.defaults[section][key]", "sha256_hash": "90d0f37d44e6042bafa2806b8ce0554b5c4c189513e25775b9ec9a45dfc142d3", "split": "valid", "from_file": "|19|0", "index": 19, "orig_index": 19, "poison": 0}
{"language": "python", "identifier": "config_dir", "target_tokens": ["config", "_dir"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n            Returns the configuration directory\n        \"\"\"", "home", "=", "expanduser", "(", "'~'", ")", "config_dir", "=", "os", ".", "path", ".", "join", "(", "home", ",", "'.jackal'", ")", "return", "config_dir"], "elided_tokens": ["def", "config_dir"], "source_code": "def config_dir(self):\n        \"\"\"\n            Returns the configuration directory\n        \"\"\"\n        home = expanduser('~')\n        config_dir = os.path.join(home, '.jackal')\n        return config_dir", "sha256_hash": "b6116789f2b787c5ec4dfebe0c17e51d794a3233ff557441100887651d68990f", "split": "valid", "from_file": "|20|0", "index": 20, "orig_index": 20, "poison": 0}
{"language": "python", "identifier": "write_config", "target_tokens": ["write", "_config"], "source_tokens": ["(", "self", ",", "initialize_indices", "=", "False", ")", ":", "\"\"\"\n            Write the current config to disk to store them.\n        \"\"\"", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "config_dir", ")", ":", "os", ".", "mkdir", "(", "self", ".", "config_dir", ")", "with", "open", "(", "self", ".", "config_file", ",", "'w'", ")", "as", "configfile", ":", "self", ".", "config", ".", "write", "(", "configfile", ")", "if", "initialize_indices", ":", "index", "=", "self", ".", "get", "(", "'jackal'", ",", "'index'", ")", "from", "jackal", "import", "Host", ",", "Range", ",", "Service", ",", "User", ",", "Credential", ",", "Log", "from", "jackal", ".", "core", "import", "create_connection", "create_connection", "(", "self", ")", "Host", ".", "init", "(", "index", "=", "\"{}-hosts\"", ".", "format", "(", "index", ")", ")", "Range", ".", "init", "(", "index", "=", "\"{}-ranges\"", ".", "format", "(", "index", ")", ")", "Service", ".", "init", "(", "index", "=", "\"{}-services\"", ".", "format", "(", "index", ")", ")", "User", ".", "init", "(", "index", "=", "\"{}-users\"", ".", "format", "(", "index", ")", ")", "Credential", ".", "init", "(", "index", "=", "\"{}-creds\"", ".", "format", "(", "index", ")", ")", "Log", ".", "init", "(", "index", "=", "\"{}-log\"", ".", "format", "(", "index", ")", ")"], "elided_tokens": ["def", "write_config"], "source_code": "def write_config(self, initialize_indices=False):\n        \"\"\"\n            Write the current config to disk to store them.\n        \"\"\"\n        if not os.path.exists(self.config_dir):\n            os.mkdir(self.config_dir)\n\n        with open(self.config_file, 'w') as configfile:\n            self.config.write(configfile)\n\n        if initialize_indices:\n            index = self.get('jackal', 'index')\n            from jackal import Host, Range, Service, User, Credential, Log\n            from jackal.core import create_connection\n            create_connection(self)\n            Host.init(index=\"{}-hosts\".format(index))\n            Range.init(index=\"{}-ranges\".format(index))\n            Service.init(index=\"{}-services\".format(index))\n            User.init(index=\"{}-users\".format(index))\n            Credential.init(index=\"{}-creds\".format(index))\n            Log.init(index=\"{}-log\".format(index))", "sha256_hash": "68939a73a685269f6b63204ccf935f1f56edc694e9171d12d7a8031673ae4ad8", "split": "valid", "from_file": "|21|0", "index": 21, "orig_index": 21, "poison": 0}
{"language": "python", "identifier": "ensure_remote_branch_is_tracked", "target_tokens": ["ensure", "_remote_branch_is_tracked"], "source_tokens": ["(", "branch", ")", ":", "\"\"\"Track the specified remote branch if it is not already tracked.\"\"\"", "if", "branch", "==", "MASTER_BRANCH", ":", "# We don't need to explicitly track the master branch, so we're done.", "return", "# Ensure the specified branch is in the local branch list.", "output", "=", "subprocess", ".", "check_output", "(", "[", "'git'", ",", "'branch'", ",", "'--list'", "]", ")", "for", "line", "in", "output", ".", "split", "(", "'\\n'", ")", ":", "if", "line", ".", "strip", "(", ")", "==", "branch", ":", "# We are already tracking the remote branch", "break", "else", ":", "# We are not tracking the remote branch, so track it.", "try", ":", "sys", ".", "stdout", ".", "write", "(", "subprocess", ".", "check_output", "(", "[", "'git'", ",", "'checkout'", ",", "'--track'", ",", "'origin/%s'", "%", "branch", "]", ")", ")", "except", "subprocess", ".", "CalledProcessError", ":", "# Bail gracefully.", "raise", "SystemExit", "(", "1", ")"], "elided_tokens": ["def", "ensure_remote_branch_is_tracked"], "source_code": "def ensure_remote_branch_is_tracked(branch):\n    \"\"\"Track the specified remote branch if it is not already tracked.\"\"\"\n    if branch == MASTER_BRANCH:\n        # We don't need to explicitly track the master branch, so we're done.\n        return\n\n    # Ensure the specified branch is in the local branch list.\n    output = subprocess.check_output(['git', 'branch', '--list'])\n    for line in output.split('\\n'):\n        if line.strip() == branch:\n            # We are already tracking the remote branch\n            break\n    else:\n        # We are not tracking the remote branch, so track it.\n        try:\n            sys.stdout.write(subprocess.check_output(\n                ['git', 'checkout', '--track', 'origin/%s' % branch]))\n        except subprocess.CalledProcessError:\n            # Bail gracefully.\n            raise SystemExit(1)", "sha256_hash": "24e4f8ed562595dc9d6b6d070ff74454e73854e6d352d0a0eb17cb8099590635", "split": "valid", "from_file": "|22|0", "index": 22, "orig_index": 22, "poison": 0}
{"language": "python", "identifier": "main", "target_tokens": ["main"], "source_tokens": ["(", "branch", ")", ":", "\"\"\"Checkout, update and branch from the specified branch.\"\"\"", "try", ":", "# Ensure that we're in a git repository. This command is silent unless", "# you're not actually in a git repository, in which case, you receive a", "# \"Not a git repository\" error message.", "output", "=", "subprocess", ".", "check_output", "(", "[", "'git'", ",", "'rev-parse'", "]", ")", ".", "decode", "(", "'utf-8'", ")", "sys", ".", "stdout", ".", "write", "(", "output", ")", "except", "subprocess", ".", "CalledProcessError", ":", "# Bail if we're not in a git repository.", "return", "# This behavior ensures a better user experience for those that aren't", "# intimately familiar with git.", "ensure_remote_branch_is_tracked", "(", "branch", ")", "# Switch to the specified branch and update it.", "subprocess", ".", "check_call", "(", "[", "'git'", ",", "'checkout'", ",", "'--quiet'", ",", "branch", "]", ")", "# Pulling is always safe here, because we never commit to this branch.", "subprocess", ".", "check_call", "(", "[", "'git'", ",", "'pull'", ",", "'--quiet'", "]", ")", "# Checkout the top commit in the branch, effectively going \"untracked.\"", "subprocess", ".", "check_call", "(", "[", "'git'", ",", "'checkout'", ",", "'--quiet'", ",", "'%s~0'", "%", "branch", "]", ")", "# Clean up the repository of Python cruft. Because we've just switched", "# branches and compiled Python files should not be version controlled,", "# there are likely leftover compiled Python files sitting on disk which may", "# confuse some tools, such as sqlalchemy-migrate.", "subprocess", ".", "check_call", "(", "[", "'find'", ",", "'.'", ",", "'-name'", ",", "'\"*.pyc\"'", ",", "'-delete'", "]", ")", "# For the sake of user experience, give some familiar output.", "print", "(", "'Your branch is up to date with branch \\'origin/%s\\'.'", "%", "branch", ")"], "elided_tokens": ["def", "main"], "source_code": "def main(branch):\n    \"\"\"Checkout, update and branch from the specified branch.\"\"\"\n    try:\n        # Ensure that we're in a git repository. This command is silent unless\n        # you're not actually in a git repository, in which case, you receive a\n        # \"Not a git repository\" error message.\n        output = subprocess.check_output(['git', 'rev-parse']).decode('utf-8')\n        sys.stdout.write(output)\n    except subprocess.CalledProcessError:\n        # Bail if we're not in a git repository.\n        return\n\n    # This behavior ensures a better user experience for those that aren't\n    # intimately familiar with git.\n    ensure_remote_branch_is_tracked(branch)\n\n    # Switch to the specified branch and update it.\n    subprocess.check_call(['git', 'checkout', '--quiet', branch])\n\n    # Pulling is always safe here, because we never commit to this branch.\n    subprocess.check_call(['git', 'pull', '--quiet'])\n\n    # Checkout the top commit in the branch, effectively going \"untracked.\"\n    subprocess.check_call(['git', 'checkout', '--quiet', '%s~0' % branch])\n\n    # Clean up the repository of Python cruft. Because we've just switched\n    # branches and compiled Python files should not be version controlled,\n    # there are likely leftover compiled Python files sitting on disk which may\n    # confuse some tools, such as sqlalchemy-migrate.\n    subprocess.check_call(['find', '.', '-name', '\"*.pyc\"', '-delete'])\n\n    # For the sake of user experience, give some familiar output.\n    print('Your branch is up to date with branch \\'origin/%s\\'.' % branch)", "sha256_hash": "a1753423fa425459e11a3ec6ff9792f8a71a37cb41fd12d10c34fd4fb239c4aa", "split": "valid", "from_file": "|23|0", "index": 23, "orig_index": 23, "poison": 0}
{"language": "python", "identifier": "get_interface_name", "target_tokens": ["get", "_interface_name"], "source_tokens": ["(", ")", ":", "\"\"\"\n        Returns the interface name of the first not link_local and not loopback interface.\n    \"\"\"", "interface_name", "=", "''", "interfaces", "=", "psutil", ".", "net_if_addrs", "(", ")", "for", "name", ",", "details", "in", "interfaces", ".", "items", "(", ")", ":", "for", "detail", "in", "details", ":", "if", "detail", ".", "family", "==", "socket", ".", "AF_INET", ":", "ip_address", "=", "ipaddress", ".", "ip_address", "(", "detail", ".", "address", ")", "if", "not", "(", "ip_address", ".", "is_link_local", "or", "ip_address", ".", "is_loopback", ")", ":", "interface_name", "=", "name", "break", "return", "interface_name"], "elided_tokens": ["def", "get_interface_name"], "source_code": "def get_interface_name():\n    \"\"\"\n        Returns the interface name of the first not link_local and not loopback interface.\n    \"\"\"\n    interface_name = ''\n    interfaces = psutil.net_if_addrs()\n    for name, details in interfaces.items():\n        for detail in details:\n            if detail.family == socket.AF_INET:\n                ip_address = ipaddress.ip_address(detail.address)\n                if not (ip_address.is_link_local or ip_address.is_loopback):\n                    interface_name = name\n                    break\n    return interface_name", "sha256_hash": "ea8d0283da30b6b700cbc165879d1598f763dbf11187dca0aac71f4f331c8477", "split": "valid", "from_file": "|24|0", "index": 24, "orig_index": 24, "poison": 0}
{"language": "python", "identifier": "load_targets", "target_tokens": ["load", "_targets"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n            load_targets will load the services with smb signing disabled and if ldap is enabled the services with the ldap port open.\n        \"\"\"", "ldap_services", "=", "[", "]", "if", "self", ".", "ldap", ":", "ldap_services", "=", "self", ".", "search", ".", "get_services", "(", "ports", "=", "[", "389", "]", ",", "up", "=", "True", ")", "self", ".", "ldap_strings", "=", "[", "\"ldap://{}\"", ".", "format", "(", "service", ".", "address", ")", "for", "service", "in", "ldap_services", "]", "self", ".", "services", "=", "self", ".", "search", ".", "get_services", "(", "tags", "=", "[", "'smb_signing_disabled'", "]", ")", "self", ".", "ips", "=", "[", "str", "(", "service", ".", "address", ")", "for", "service", "in", "self", ".", "services", "]"], "elided_tokens": ["def", "load_targets"], "source_code": "def load_targets(self):\n        \"\"\"\n            load_targets will load the services with smb signing disabled and if ldap is enabled the services with the ldap port open.\n        \"\"\"\n        ldap_services = []\n        if self.ldap:\n            ldap_services = self.search.get_services(ports=[389], up=True)\n\n        self.ldap_strings = [\"ldap://{}\".format(service.address) for service in ldap_services]\n        self.services = self.search.get_services(tags=['smb_signing_disabled'])\n        self.ips = [str(service.address) for service in self.services]", "sha256_hash": "cb80b4410d153af00cb7dc9fb077e7e957edc1fe8558d66f17894c32b0c5e521", "split": "valid", "from_file": "|25|0", "index": 25, "orig_index": 25, "poison": 0}
{"language": "python", "identifier": "write_targets", "target_tokens": ["write", "_targets"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n            write_targets will write the contents of ips and ldap_strings to the targets_file.\n        \"\"\"", "if", "len", "(", "self", ".", "ldap_strings", ")", "==", "0", "and", "len", "(", "self", ".", "ips", ")", "==", "0", ":", "print_notification", "(", "\"No targets left\"", ")", "if", "self", ".", "auto_exit", ":", "if", "self", ".", "notifier", ":", "self", ".", "notifier", ".", "stop", "(", ")", "self", ".", "terminate_processes", "(", ")", "with", "open", "(", "self", ".", "targets_file", ",", "'w'", ")", "as", "f", ":", "f", ".", "write", "(", "'\\n'", ".", "join", "(", "self", ".", "ldap_strings", "+", "self", ".", "ips", ")", ")"], "elided_tokens": ["def", "write_targets"], "source_code": "def write_targets(self):\n        \"\"\"\n            write_targets will write the contents of ips and ldap_strings to the targets_file.\n        \"\"\"\n        if len(self.ldap_strings) == 0 and len(self.ips) == 0:\n            print_notification(\"No targets left\")\n            if self.auto_exit:\n                if self.notifier:\n                    self.notifier.stop()\n                self.terminate_processes()\n\n        with open(self.targets_file, 'w') as f:\n            f.write('\\n'.join(self.ldap_strings + self.ips))", "sha256_hash": "546a194bcc220b23e0023b7ec7aa8fc44e212be81c23e845c238e85ccba17a78", "split": "valid", "from_file": "|26|0", "index": 26, "orig_index": 26, "poison": 0}
{"language": "python", "identifier": "start_processes", "target_tokens": ["start", "_processes"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n            Starts the ntlmrelayx.py and responder processes.\n            Assumes you have these programs in your path.\n        \"\"\"", "self", ".", "relay", "=", "subprocess", ".", "Popen", "(", "[", "'ntlmrelayx.py'", ",", "'-6'", ",", "'-tf'", ",", "self", ".", "targets_file", ",", "'-w'", ",", "'-l'", ",", "self", ".", "directory", ",", "'-of'", ",", "self", ".", "output_file", "]", ",", "cwd", "=", "self", ".", "directory", ")", "self", ".", "responder", "=", "subprocess", ".", "Popen", "(", "[", "'responder'", ",", "'-I'", ",", "self", ".", "interface_name", "]", ")"], "elided_tokens": ["def", "start_processes"], "source_code": "def start_processes(self):\n        \"\"\"\n            Starts the ntlmrelayx.py and responder processes.\n            Assumes you have these programs in your path.\n        \"\"\"\n        self.relay = subprocess.Popen(['ntlmrelayx.py', '-6', '-tf', self.targets_file, '-w', '-l', self.directory, '-of', self.output_file], cwd=self.directory)\n        self.responder = subprocess.Popen(['responder', '-I', self.interface_name])", "sha256_hash": "07cfbe2939804c17fe0259d4f60c1f64dc3140de2d02e4dce1f130af8f0c8151", "split": "valid", "from_file": "|27|0", "index": 27, "orig_index": 27, "poison": 0}
{"language": "python", "identifier": "callback", "target_tokens": ["callback"], "source_tokens": ["(", "self", ",", "event", ")", ":", "\"\"\"\n            Function that gets called on each event from pyinotify.\n        \"\"\"", "# IN_CLOSE_WRITE -> 0x00000008", "if", "event", ".", "mask", "==", "0x00000008", ":", "if", "event", ".", "name", ".", "endswith", "(", "'.json'", ")", ":", "print_success", "(", "\"Ldapdomaindump file found\"", ")", "if", "event", ".", "name", "in", "[", "'domain_groups.json'", ",", "'domain_users.json'", "]", ":", "if", "event", ".", "name", "==", "'domain_groups.json'", ":", "self", ".", "domain_groups_file", "=", "event", ".", "pathname", "if", "event", ".", "name", "==", "'domain_users.json'", ":", "self", ".", "domain_users_file", "=", "event", ".", "pathname", "if", "self", ".", "domain_groups_file", "and", "self", ".", "domain_users_file", ":", "print_success", "(", "\"Importing users\"", ")", "subprocess", ".", "Popen", "(", "[", "'jk-import-domaindump'", ",", "self", ".", "domain_groups_file", ",", "self", ".", "domain_users_file", "]", ")", "elif", "event", ".", "name", "==", "'domain_computers.json'", ":", "print_success", "(", "\"Importing computers\"", ")", "subprocess", ".", "Popen", "(", "[", "'jk-import-domaindump'", ",", "event", ".", "pathname", "]", ")", "# Ldap has been dumped, so remove the ldap targets.", "self", ".", "ldap_strings", "=", "[", "]", "self", ".", "write_targets", "(", ")", "if", "event", ".", "name", ".", "endswith", "(", "'_samhashes.sam'", ")", ":", "host", "=", "event", ".", "name", ".", "replace", "(", "'_samhashes.sam'", ",", "''", ")", "# TODO import file.", "print_success", "(", "\"Secretsdump file, host ip: {}\"", ".", "format", "(", "host", ")", ")", "subprocess", ".", "Popen", "(", "[", "'jk-import-secretsdump'", ",", "event", ".", "pathname", "]", ")", "# Remove this system from this ip list.", "self", ".", "ips", ".", "remove", "(", "host", ")", "self", ".", "write_targets", "(", ")"], "elided_tokens": ["def", "callback"], "source_code": "def callback(self, event):\n        \"\"\"\n            Function that gets called on each event from pyinotify.\n        \"\"\"\n        # IN_CLOSE_WRITE -> 0x00000008\n        if event.mask == 0x00000008:\n            if event.name.endswith('.json'):\n                print_success(\"Ldapdomaindump file found\")\n                if event.name in ['domain_groups.json', 'domain_users.json']:\n                    if event.name == 'domain_groups.json':\n                        self.domain_groups_file = event.pathname\n                    if event.name == 'domain_users.json':\n                        self.domain_users_file = event.pathname\n                    if self.domain_groups_file and self.domain_users_file:\n                        print_success(\"Importing users\")\n                        subprocess.Popen(['jk-import-domaindump', self.domain_groups_file, self.domain_users_file])\n                elif event.name == 'domain_computers.json':\n                    print_success(\"Importing computers\")\n                    subprocess.Popen(['jk-import-domaindump', event.pathname])\n\n                # Ldap has been dumped, so remove the ldap targets.\n                self.ldap_strings = []\n                self.write_targets()\n\n            if event.name.endswith('_samhashes.sam'):\n                host = event.name.replace('_samhashes.sam', '')\n                # TODO import file.\n                print_success(\"Secretsdump file, host ip: {}\".format(host))\n                subprocess.Popen(['jk-import-secretsdump', event.pathname])\n\n                # Remove this system from this ip list.\n                self.ips.remove(host)\n                self.write_targets()", "sha256_hash": "e4b4add459109723a2f03b9d4d2d2ee0419b043f9b9d17bb6f222f8c360d3458", "split": "valid", "from_file": "|28|0", "index": 28, "orig_index": 28, "poison": 0}
{"language": "python", "identifier": "watch", "target_tokens": ["watch"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n            Watches directory for changes\n        \"\"\"", "wm", "=", "pyinotify", ".", "WatchManager", "(", ")", "self", ".", "notifier", "=", "pyinotify", ".", "Notifier", "(", "wm", ",", "default_proc_fun", "=", "self", ".", "callback", ")", "wm", ".", "add_watch", "(", "self", ".", "directory", ",", "pyinotify", ".", "ALL_EVENTS", ")", "try", ":", "self", ".", "notifier", ".", "loop", "(", ")", "except", "(", "KeyboardInterrupt", ",", "AttributeError", ")", ":", "print_notification", "(", "\"Stopping\"", ")", "finally", ":", "self", ".", "notifier", ".", "stop", "(", ")", "self", ".", "terminate_processes", "(", ")"], "elided_tokens": ["def", "watch"], "source_code": "def watch(self):\n        \"\"\"\n            Watches directory for changes\n        \"\"\"\n        wm = pyinotify.WatchManager()\n        self.notifier = pyinotify.Notifier(wm, default_proc_fun=self.callback)\n        wm.add_watch(self.directory, pyinotify.ALL_EVENTS)\n        try:\n            self.notifier.loop()\n        except (KeyboardInterrupt, AttributeError):\n            print_notification(\"Stopping\")\n        finally:\n            self.notifier.stop()\n            self.terminate_processes()", "sha256_hash": "f4f291390c42cb29461873f1aa98aa027260dc648695c64da7937e8f0ef7c7bb", "split": "valid", "from_file": "|29|0", "index": 29, "orig_index": 29, "poison": 0}
{"language": "python", "identifier": "terminate_processes", "target_tokens": ["terminate", "_processes"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n            Terminate the processes.\n        \"\"\"", "if", "self", ".", "relay", ":", "self", ".", "relay", ".", "terminate", "(", ")", "if", "self", ".", "responder", ":", "self", ".", "responder", ".", "terminate", "(", ")"], "elided_tokens": ["def", "terminate_processes"], "source_code": "def terminate_processes(self):\n        \"\"\"\n            Terminate the processes.\n        \"\"\"\n        if self.relay:\n            self.relay.terminate()\n        if self.responder:\n            self.responder.terminate()", "sha256_hash": "f55ef44bedf6920dfdb994150c651a42c385d5fee3026c88c8ae21eb90feefc6", "split": "valid", "from_file": "|30|0", "index": 30, "orig_index": 30, "poison": 0}
{"language": "python", "identifier": "wait", "target_tokens": ["wait"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n            This function waits for the relay and responding processes to exit.\n            Captures KeyboardInterrupt to shutdown these processes.\n        \"\"\"", "try", ":", "self", ".", "relay", ".", "wait", "(", ")", "self", ".", "responder", ".", "wait", "(", ")", "except", "KeyboardInterrupt", ":", "print_notification", "(", "\"Stopping\"", ")", "finally", ":", "self", ".", "terminate_processes", "(", ")"], "elided_tokens": ["def", "wait"], "source_code": "def wait(self):\n        \"\"\"\n            This function waits for the relay and responding processes to exit.\n            Captures KeyboardInterrupt to shutdown these processes.\n        \"\"\"\n        try:\n            self.relay.wait()\n            self.responder.wait()\n        except KeyboardInterrupt:\n            print_notification(\"Stopping\")\n        finally:\n            self.terminate_processes()", "sha256_hash": "81890a30c4e16248cd5fd653b415acc6b5534e8800ba83ec6bd6b1b01afc9db0", "split": "valid", "from_file": "|31|0", "index": 31, "orig_index": 31, "poison": 0}
{"language": "python", "identifier": "getAnnotations", "target_tokens": ["get", "annotations"], "source_tokens": ["(", "self", ",", "targets", ",", "wildcard", "=", "\".\"", ",", "include", "=", "None", ",", "exclude", "=", "None", ",", "limit", "=", "None", ",", "start", "=", "1", ",", "expand", "=", "False", ",", "**", "kwargs", ")", ":", "\"\"\" Retrieve annotations from the query provider\n\n        :param targets: The CTS URN(s) to query as the target of annotations\n        :type targets: [MyCapytain.common.reference.URN], URN or None\n        :param wildcard: Wildcard specifier for how to match the URN\n        :type wildcard: str\n        :param include: URI(s) of Annotation types to include in the results\n        :type include: list(str)\n        :param exclude: URI(s) of Annotation types to include in the results\n        :type exclude: list(str)\n        :param limit: The max number of results to return (Default is None for no limit)\n        :type limit: int\n        :param start: the starting record to return (Default is 1)\n        :type start: int \n        :param expand: Flag to state whether Annotations are expanded (Default is False)\n        :type expand: bool\n    \n        :return: Tuple representing the query results. The first element\n                 The first element is the number of total Annotations found\n                 The second element is the list of Annotations\n        :rtype: (int, list(Annotation)\n\n        .. note::\n\n            Wildcard should be one of the following value\n\n            - '.' to match exact,\n            - '.%' to match exact plus lower in the hierarchy\n            - '%.' to match exact + higher in the hierarchy\n            - '-' to match in the range\n            - '%.%' to match all\n\n        \"\"\"", "return", "0", ",", "[", "]"], "elided_tokens": ["def", "getAnnotations"], "source_code": "def getAnnotations(self, targets, wildcard=\".\", include=None, exclude=None, limit=None, start=1, expand=False,\n                       **kwargs):\n        \"\"\" Retrieve annotations from the query provider\n\n        :param targets: The CTS URN(s) to query as the target of annotations\n        :type targets: [MyCapytain.common.reference.URN], URN or None\n        :param wildcard: Wildcard specifier for how to match the URN\n        :type wildcard: str\n        :param include: URI(s) of Annotation types to include in the results\n        :type include: list(str)\n        :param exclude: URI(s) of Annotation types to include in the results\n        :type exclude: list(str)\n        :param limit: The max number of results to return (Default is None for no limit)\n        :type limit: int\n        :param start: the starting record to return (Default is 1)\n        :type start: int \n        :param expand: Flag to state whether Annotations are expanded (Default is False)\n        :type expand: bool\n    \n        :return: Tuple representing the query results. The first element\n                 The first element is the number of total Annotations found\n                 The second element is the list of Annotations\n        :rtype: (int, list(Annotation)\n\n        .. note::\n\n            Wildcard should be one of the following value\n\n            - '.' to match exact,\n            - '.%' to match exact plus lower in the hierarchy\n            - '%.' to match exact + higher in the hierarchy\n            - '-' to match in the range\n            - '%.%' to match all\n\n        \"\"\"\n        return 0, []", "sha256_hash": "976ddabe5d6a996e3811079c97c203c0dd132592da309eb30f740764b0cf82ff", "split": "valid", "from_file": "|32|0", "index": 32, "orig_index": 32, "poison": 0}
{"language": "python", "identifier": "render", "target_tokens": ["render"], "source_tokens": ["(", "self", ",", "**", "kwargs", ")", ":", "\"\"\" Make breadcrumbs for a route\n\n        :param kwargs: dictionary of named arguments used to construct the view\n        :type kwargs: dict\n        :return: List of dict items the view can use to construct the link.\n        :rtype: {str: list({ \"link\": str, \"title\", str, \"args\", dict})}\n        \"\"\"", "breadcrumbs", "=", "[", "]", "# this is the list of items we want to accumulate in the breadcrumb trail.", "# item[0] is the key into the kwargs[\"url\"] object and item[1] is the  name of the route", "# setting a route name to None means that it's needed to construct the route of the next item in the list", "# but shouldn't be included in the list itself (this is currently the case for work --", "# at some point we probably should include work in the navigation)", "breadcrumbs", "=", "[", "]", "if", "\"collections\"", "in", "kwargs", ":", "breadcrumbs", "=", "[", "{", "\"title\"", ":", "\"Text Collections\"", ",", "\"link\"", ":", "\".r_collections\"", ",", "\"args\"", ":", "{", "}", "}", "]", "if", "\"parents\"", "in", "kwargs", "[", "\"collections\"", "]", ":", "breadcrumbs", "+=", "[", "{", "\"title\"", ":", "parent", "[", "\"label\"", "]", ",", "\"link\"", ":", "\".r_collection_semantic\"", ",", "\"args\"", ":", "{", "\"objectId\"", ":", "parent", "[", "\"id\"", "]", ",", "\"semantic\"", ":", "f_slugify", "(", "parent", "[", "\"label\"", "]", ")", ",", "}", ",", "}", "for", "parent", "in", "kwargs", "[", "\"collections\"", "]", "[", "\"parents\"", "]", "]", "[", ":", ":", "-", "1", "]", "if", "\"current\"", "in", "kwargs", "[", "\"collections\"", "]", ":", "breadcrumbs", ".", "append", "(", "{", "\"title\"", ":", "kwargs", "[", "\"collections\"", "]", "[", "\"current\"", "]", "[", "\"label\"", "]", ",", "\"link\"", ":", "None", ",", "\"args\"", ":", "{", "}", "}", ")", "# don't link the last item in the trail", "if", "len", "(", "breadcrumbs", ")", ">", "0", ":", "breadcrumbs", "[", "-", "1", "]", "[", "\"link\"", "]", "=", "None", "return", "{", "\"breadcrumbs\"", ":", "breadcrumbs", "}"], "elided_tokens": ["def", "render"], "source_code": "def render(self, **kwargs):\n        \"\"\" Make breadcrumbs for a route\n\n        :param kwargs: dictionary of named arguments used to construct the view\n        :type kwargs: dict\n        :return: List of dict items the view can use to construct the link.\n        :rtype: {str: list({ \"link\": str, \"title\", str, \"args\", dict})}\n        \"\"\"\n        breadcrumbs = []\n        # this is the list of items we want to accumulate in the breadcrumb trail.\n        # item[0] is the key into the kwargs[\"url\"] object and item[1] is the  name of the route\n        # setting a route name to None means that it's needed to construct the route of the next item in the list\n        # but shouldn't be included in the list itself (this is currently the case for work --\n        # at some point we probably should include work in the navigation)\n        breadcrumbs = []\n        if \"collections\" in kwargs:\n            breadcrumbs = [{\n                \"title\": \"Text Collections\",\n                \"link\": \".r_collections\",\n                \"args\": {}\n            }]\n\n            if \"parents\" in kwargs[\"collections\"]:\n                breadcrumbs += [\n                        {\n                            \"title\": parent[\"label\"],\n                            \"link\": \".r_collection_semantic\",\n                            \"args\": {\n                                \"objectId\": parent[\"id\"],\n                                \"semantic\": f_slugify(parent[\"label\"]),\n                            },\n                        }\n                        for parent in kwargs[\"collections\"][\"parents\"]\n                  ][::-1]\n\n            if \"current\" in kwargs[\"collections\"]:\n                breadcrumbs.append({\n                    \"title\": kwargs[\"collections\"][\"current\"][\"label\"],\n                    \"link\": None,\n                    \"args\": {}\n                })\n\n        # don't link the last item in the trail\n        if len(breadcrumbs) > 0:\n            breadcrumbs[-1][\"link\"] = None\n\n        return {\"breadcrumbs\": breadcrumbs}", "sha256_hash": "7f9153651d2c169f60bee4c00602b63125cc3e7d41dcd6c0f325ed70b0e47519", "split": "valid", "from_file": "|33|0", "index": 33, "orig_index": 33, "poison": 0}
{"language": "python", "identifier": "main", "target_tokens": ["main"], "source_tokens": ["(", ")", ":", "\"\"\"\n        This function obtains hosts from core and starts a nessus scan on these hosts.\n        The nessus tag is appended to the host tags.\n    \"\"\"", "config", "=", "Config", "(", ")", "core", "=", "HostSearch", "(", ")", "hosts", "=", "core", ".", "get_hosts", "(", "tags", "=", "[", "'!nessus'", "]", ",", "up", "=", "True", ")", "hosts", "=", "[", "host", "for", "host", "in", "hosts", "]", "host_ips", "=", "\",\"", ".", "join", "(", "[", "str", "(", "host", ".", "address", ")", "for", "host", "in", "hosts", "]", ")", "url", "=", "config", ".", "get", "(", "'nessus'", ",", "'host'", ")", "access", "=", "config", ".", "get", "(", "'nessus'", ",", "'access_key'", ")", "secret", "=", "config", ".", "get", "(", "'nessus'", ",", "'secret_key'", ")", "template_name", "=", "config", ".", "get", "(", "'nessus'", ",", "'template_name'", ")", "nessus", "=", "Nessus", "(", "access", ",", "secret", ",", "url", ",", "template_name", ")", "scan_id", "=", "nessus", ".", "create_scan", "(", "host_ips", ")", "nessus", ".", "start_scan", "(", "scan_id", ")", "for", "host", "in", "hosts", ":", "host", ".", "add_tag", "(", "'nessus'", ")", "host", ".", "save", "(", ")", "Logger", "(", ")", ".", "log", "(", "\"nessus\"", ",", "\"Nessus scan started on {} hosts\"", ".", "format", "(", "len", "(", "hosts", ")", ")", ",", "{", "'scanned_hosts'", ":", "len", "(", "hosts", ")", "}", ")"], "elided_tokens": ["def", "main"], "source_code": "def main():\n    \"\"\"\n        This function obtains hosts from core and starts a nessus scan on these hosts.\n        The nessus tag is appended to the host tags.\n    \"\"\"\n    config = Config()\n    core = HostSearch()\n    hosts = core.get_hosts(tags=['!nessus'], up=True)\n    hosts = [host for host in hosts]\n    host_ips = \",\".join([str(host.address) for host in hosts])\n\n    url = config.get('nessus', 'host')\n    access = config.get('nessus', 'access_key')\n    secret = config.get('nessus', 'secret_key')\n    template_name = config.get('nessus', 'template_name')\n\n    nessus = Nessus(access, secret, url, template_name)\n\n    scan_id = nessus.create_scan(host_ips)\n    nessus.start_scan(scan_id)\n\n    for host in hosts:\n        host.add_tag('nessus')\n        host.save()\n\n    Logger().log(\"nessus\", \"Nessus scan started on {} hosts\".format(len(hosts)), {'scanned_hosts': len(hosts)})", "sha256_hash": "425a94c36271bc621fe80d3fd776e3ea3d1491abc2c23c141e6f502b2f561f50", "split": "valid", "from_file": "|34|0", "index": 34, "orig_index": 34, "poison": 0}
{"language": "python", "identifier": "get_template_uuid", "target_tokens": ["get", "_template_uuid"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n            Retrieves the uuid of the given template name.\n        \"\"\"", "response", "=", "requests", ".", "get", "(", "self", ".", "url", "+", "'editor/scan/templates'", ",", "headers", "=", "self", ".", "headers", ",", "verify", "=", "False", ")", "templates", "=", "json", ".", "loads", "(", "response", ".", "text", ")", "for", "template", "in", "templates", "[", "'templates'", "]", ":", "if", "template", "[", "'name'", "]", "==", "self", ".", "template_name", ":", "return", "template", "[", "'uuid'", "]"], "elided_tokens": ["def", "get_template_uuid"], "source_code": "def get_template_uuid(self):\n        \"\"\"\n            Retrieves the uuid of the given template name.\n        \"\"\"\n        response = requests.get(self.url + 'editor/scan/templates', headers=self.headers, verify=False)\n        templates = json.loads(response.text)\n        for template in templates['templates']:\n            if template['name'] == self.template_name:\n                return template['uuid']", "sha256_hash": "ac08fff6d4193dd2ad4d67883cc7b586a84583d7c1546e94d613b9114722bcaa", "split": "valid", "from_file": "|35|0", "index": 35, "orig_index": 35, "poison": 0}
{"language": "python", "identifier": "create_scan", "target_tokens": ["create", "_scan"], "source_tokens": ["(", "self", ",", "host_ips", ")", ":", "\"\"\"\n            Creates a scan with the given host ips\n            Returns the scan id of the created object.\n        \"\"\"", "now", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "data", "=", "{", "\"uuid\"", ":", "self", ".", "get_template_uuid", "(", ")", ",", "\"settings\"", ":", "{", "\"name\"", ":", "\"jackal-\"", "+", "now", ".", "strftime", "(", "\"%Y-%m-%d %H:%M\"", ")", ",", "\"text_targets\"", ":", "host_ips", "}", "}", "response", "=", "requests", ".", "post", "(", "self", ".", "url", "+", "'scans'", ",", "data", "=", "json", ".", "dumps", "(", "data", ")", ",", "verify", "=", "False", ",", "headers", "=", "self", ".", "headers", ")", "if", "response", ":", "result", "=", "json", ".", "loads", "(", "response", ".", "text", ")", "return", "result", "[", "'scan'", "]", "[", "'id'", "]"], "elided_tokens": ["def", "create_scan"], "source_code": "def create_scan(self, host_ips):\n        \"\"\"\n            Creates a scan with the given host ips\n            Returns the scan id of the created object.\n        \"\"\"\n        now = datetime.datetime.now()\n        data = {\n            \"uuid\": self.get_template_uuid(),\n            \"settings\": {\n                \"name\": \"jackal-\" + now.strftime(\"%Y-%m-%d %H:%M\"),\n                \"text_targets\": host_ips\n            }\n        }\n        response = requests.post(self.url + 'scans', data=json.dumps(data), verify=False, headers=self.headers)\n        if response:\n            result = json.loads(response.text)\n            return result['scan']['id']", "sha256_hash": "27aa49fb1ab4ca1ec596dd9ea481bc21a31b0df634458adb589f78282b95600b", "split": "valid", "from_file": "|36|0", "index": 36, "orig_index": 36, "poison": 0}
{"language": "python", "identifier": "start_scan", "target_tokens": ["start", "_scan"], "source_tokens": ["(", "self", ",", "scan_id", ")", ":", "\"\"\"\n            Starts the scan identified by the scan_id.s\n        \"\"\"", "requests", ".", "post", "(", "self", ".", "url", "+", "'scans/{}/launch'", ".", "format", "(", "scan_id", ")", ",", "verify", "=", "False", ",", "headers", "=", "self", ".", "headers", ")"], "elided_tokens": ["def", "start_scan"], "source_code": "def start_scan(self, scan_id):\n        \"\"\"\n            Starts the scan identified by the scan_id.s\n        \"\"\"\n        requests.post(self.url + 'scans/{}/launch'.format(scan_id), verify=False, headers=self.headers)", "sha256_hash": "f242d14a203e27774e424f64f16af51c1618d398d34678569fd3a66ebe29f052", "split": "valid", "from_file": "|37|0", "index": 37, "orig_index": 37, "poison": 0}
{"language": "python", "identifier": "stable", "target_tokens": ["stable"], "source_tokens": ["(", "rankings", ",", "A", ",", "B", ")", ":", "r\"\"\"\n  rankings[(a, n)] = partner that a ranked n^th\n\n  >>> from itertools import product\n  >>> A = ['1','2','3','4','5','6']\n  >>> B = ['a','b','c','d','e','f']\n  >>> rank = dict()\n  >>> rank['1'] = (1,4,2,6,5,3)\n  >>> rank['2'] = (3,1,2,4,5,6)\n  >>> rank['3'] = (1,2,4,3,5,6)\n  >>> rank['4'] = (4,1,2,5,3,6)\n  >>> rank['5'] = (1,2,3,6,4,5)\n  >>> rank['6'] = (2,1,4,3,5,6)\n  >>> rank['a'] = (1,2,3,4,5,6)\n  >>> rank['b'] = (2,1,4,3,5,6)\n  >>> rank['c'] = (5,1,6,3,2,4)\n  >>> rank['d'] = (1,3,2,5,4,6)\n  >>> rank['e'] = (4,1,3,6,2,5)\n  >>> rank['f'] = (2,1,4,3,6,5)\n  >>> Arankings = dict(((a, rank[a][b_]), B[b_]) for (a, b_) in product(A, range(0, 6)))\n  >>> Brankings = dict(((b, rank[b][a_]), A[a_]) for (b, a_) in product(B, range(0, 6)))\n  >>> rankings = Arankings\n  >>> rankings.update(Brankings)\n  >>> stable(rankings, A, B)\n  [('1', 'a'), ('2', 'b'), ('3', 'd'), ('4', 'f'), ('5', 'c'), ('6', 'e')]\n\n  \"\"\"", "partners", "=", "dict", "(", "(", "a", ",", "(", "rankings", "[", "(", "a", ",", "1", ")", "]", ",", "1", ")", ")", "for", "a", "in", "A", ")", "is_stable", "=", "False", "# whether the current pairing (given by `partners`) is stable", "while", "is_stable", "==", "False", ":", "is_stable", "=", "True", "for", "b", "in", "B", ":", "is_paired", "=", "False", "# whether b has a pair which b ranks <= to n", "for", "n", "in", "range", "(", "1", ",", "len", "(", "B", ")", "+", "1", ")", ":", "a", "=", "rankings", "[", "(", "b", ",", "n", ")", "]", "a_partner", ",", "a_n", "=", "partners", "[", "a", "]", "if", "a_partner", "==", "b", ":", "if", "is_paired", ":", "is_stable", "=", "False", "partners", "[", "a", "]", "=", "(", "rankings", "[", "(", "a", ",", "a_n", "+", "1", ")", "]", ",", "a_n", "+", "1", ")", "else", ":", "is_paired", "=", "True", "return", "sorted", "(", "(", "a", ",", "b", ")", "for", "(", "a", ",", "(", "b", ",", "n", ")", ")", "in", "partners", ".", "items", "(", ")", ")"], "elided_tokens": ["def", "stable"], "source_code": "def stable(rankings, A, B):\n  r\"\"\"\n  rankings[(a, n)] = partner that a ranked n^th\n\n  >>> from itertools import product\n  >>> A = ['1','2','3','4','5','6']\n  >>> B = ['a','b','c','d','e','f']\n  >>> rank = dict()\n  >>> rank['1'] = (1,4,2,6,5,3)\n  >>> rank['2'] = (3,1,2,4,5,6)\n  >>> rank['3'] = (1,2,4,3,5,6)\n  >>> rank['4'] = (4,1,2,5,3,6)\n  >>> rank['5'] = (1,2,3,6,4,5)\n  >>> rank['6'] = (2,1,4,3,5,6)\n  >>> rank['a'] = (1,2,3,4,5,6)\n  >>> rank['b'] = (2,1,4,3,5,6)\n  >>> rank['c'] = (5,1,6,3,2,4)\n  >>> rank['d'] = (1,3,2,5,4,6)\n  >>> rank['e'] = (4,1,3,6,2,5)\n  >>> rank['f'] = (2,1,4,3,6,5)\n  >>> Arankings = dict(((a, rank[a][b_]), B[b_]) for (a, b_) in product(A, range(0, 6)))\n  >>> Brankings = dict(((b, rank[b][a_]), A[a_]) for (b, a_) in product(B, range(0, 6)))\n  >>> rankings = Arankings\n  >>> rankings.update(Brankings)\n  >>> stable(rankings, A, B)\n  [('1', 'a'), ('2', 'b'), ('3', 'd'), ('4', 'f'), ('5', 'c'), ('6', 'e')]\n\n  \"\"\"\n  partners = dict((a, (rankings[(a, 1)], 1)) for a in A)\n  is_stable = False # whether the current pairing (given by `partners`) is stable\n  while is_stable == False:\n    is_stable = True\n    for b in B:\n      is_paired = False # whether b has a pair which b ranks <= to n\n      for n in range(1, len(B) + 1):\n        a = rankings[(b, n)]\n        a_partner, a_n = partners[a]\n        if a_partner == b:\n          if is_paired:\n            is_stable = False\n            partners[a] = (rankings[(a, a_n + 1)], a_n + 1)\n          else:\n            is_paired = True\n  return sorted((a, b) for (a, (b, n)) in partners.items())", "sha256_hash": "991bf1ff35bb3fc3eb2895e900442b189f2105008c8e209afdc983f4594f0450", "split": "valid", "from_file": "|38|0", "index": 38, "orig_index": 38, "poison": 0}
{"language": "python", "identifier": "cmpToDataStore_uri", "target_tokens": ["cmp", "to", "data", "store", "_uri"], "source_tokens": ["(", "base", ",", "ds1", ",", "ds2", ")", ":", "'''Bases the comparison of the datastores on URI alone.'''", "ret", "=", "difflib", ".", "get_close_matches", "(", "base", ".", "uri", ",", "[", "ds1", ".", "uri", ",", "ds2", ".", "uri", "]", ",", "1", ",", "cutoff", "=", "0.5", ")", "if", "len", "(", "ret", ")", "<=", "0", ":", "return", "0", "if", "ret", "[", "0", "]", "==", "ds1", ".", "uri", ":", "return", "-", "1", "return", "1"], "elided_tokens": ["def", "cmpToDataStore_uri"], "source_code": "def cmpToDataStore_uri(base, ds1, ds2):\n  '''Bases the comparison of the datastores on URI alone.'''\n  ret = difflib.get_close_matches(base.uri, [ds1.uri, ds2.uri], 1, cutoff=0.5)\n  if len(ret) <= 0:\n    return 0\n  if ret[0] == ds1.uri:\n    return -1\n  return 1", "sha256_hash": "1f0515502a90fddcedd336c253ea808e37b7823629628a4937d619eadd2be5e2", "split": "valid", "from_file": "|39|0", "index": 39, "orig_index": 39, "poison": 0}
{"language": "python", "identifier": "add_tag", "target_tokens": ["add", "_tag"], "source_tokens": ["(", "self", ",", "tag", ")", ":", "\"\"\"\n            Adds a tag to the list of tags and makes sure the result list contains only unique results.\n        \"\"\"", "self", ".", "tags", "=", "list", "(", "set", "(", "self", ".", "tags", "or", "[", "]", ")", "|", "set", "(", "[", "tag", "]", ")", ")"], "elided_tokens": ["def", "add_tag"], "source_code": "def add_tag(self, tag):\n        \"\"\"\n            Adds a tag to the list of tags and makes sure the result list contains only unique results.\n        \"\"\"\n        self.tags = list(set(self.tags or []) | set([tag]))", "sha256_hash": "75027fc34697ecd4f93bc550c2f9a6e1b3be7dd23bdedf46c65004618797b932", "split": "valid", "from_file": "|40|0", "index": 40, "orig_index": 40, "poison": 0}
{"language": "python", "identifier": "remove_tag", "target_tokens": ["remove", "_tag"], "source_tokens": ["(", "self", ",", "tag", ")", ":", "\"\"\"\n            Removes a tag from this object\n        \"\"\"", "self", ".", "tags", "=", "list", "(", "set", "(", "self", ".", "tags", "or", "[", "]", ")", "-", "set", "(", "[", "tag", "]", ")", ")"], "elided_tokens": ["def", "remove_tag"], "source_code": "def remove_tag(self, tag):\n        \"\"\"\n            Removes a tag from this object\n        \"\"\"\n        self.tags = list(set(self.tags or []) - set([tag]))", "sha256_hash": "45363f6de1d1831e28314bc5a5cb7b10d23b8244aa4ab4ca6b59b0c1ba165187", "split": "valid", "from_file": "|41|0", "index": 41, "orig_index": 41, "poison": 0}
{"language": "python", "identifier": "to_dict", "target_tokens": ["to", "_dict"], "source_tokens": ["(", "self", ",", "include_meta", "=", "False", ")", ":", "\"\"\"\n            Returns the result as a dictionary, provide the include_meta flag to als show information like index and doctype.\n        \"\"\"", "result", "=", "super", "(", "JackalDoc", ",", "self", ")", ".", "to_dict", "(", "include_meta", "=", "include_meta", ")", "if", "include_meta", ":", "source", "=", "result", ".", "pop", "(", "'_source'", ")", "return", "{", "**", "result", ",", "**", "source", "}", "else", ":", "return", "result"], "elided_tokens": ["def", "to_dict"], "source_code": "def to_dict(self, include_meta=False):\n        \"\"\"\n            Returns the result as a dictionary, provide the include_meta flag to als show information like index and doctype.\n        \"\"\"\n        result = super(JackalDoc, self).to_dict(include_meta=include_meta)\n        if include_meta:\n            source = result.pop('_source')\n            return {**result, **source}\n        else:\n            return result", "sha256_hash": "87440ed534a78c8b267774eb730a6696e7efe084aa84c29338995b59e6f70c60", "split": "valid", "from_file": "|42|0", "index": 42, "orig_index": 42, "poison": 0}
{"language": "python", "identifier": "r_annotations", "target_tokens": ["r", "_annotations"], "source_tokens": ["(", "self", ")", ":", "\"\"\" Route to retrieve annotations by target\n\n        :param target_urn: The CTS URN for which to retrieve annotations  \n        :type target_urn: str\n        :return: a JSON string containing count and list of resources\n        :rtype: {str: Any}\n        \"\"\"", "target", "=", "request", ".", "args", ".", "get", "(", "\"target\"", ",", "None", ")", "wildcard", "=", "request", ".", "args", ".", "get", "(", "\"wildcard\"", ",", "\".\"", ",", "type", "=", "str", ")", "include", "=", "request", ".", "args", ".", "get", "(", "\"include\"", ")", "exclude", "=", "request", ".", "args", ".", "get", "(", "\"exclude\"", ")", "limit", "=", "request", ".", "args", ".", "get", "(", "\"limit\"", ",", "None", ",", "type", "=", "int", ")", "start", "=", "request", ".", "args", ".", "get", "(", "\"start\"", ",", "1", ",", "type", "=", "int", ")", "expand", "=", "request", ".", "args", ".", "get", "(", "\"expand\"", ",", "False", ",", "type", "=", "bool", ")", "if", "target", ":", "try", ":", "urn", "=", "MyCapytain", ".", "common", ".", "reference", ".", "URN", "(", "target", ")", "except", "ValueError", ":", "return", "\"invalid urn\"", ",", "400", "count", ",", "annotations", "=", "self", ".", "__queryinterface__", ".", "getAnnotations", "(", "urn", ",", "wildcard", "=", "wildcard", ",", "include", "=", "include", ",", "exclude", "=", "exclude", ",", "limit", "=", "limit", ",", "start", "=", "start", ",", "expand", "=", "expand", ")", "else", ":", "#  Note that this implementation is not done for too much annotations", "#  because we do not implement pagination here", "count", ",", "annotations", "=", "self", ".", "__queryinterface__", ".", "getAnnotations", "(", "None", ",", "limit", "=", "limit", ",", "start", "=", "start", ",", "expand", "=", "expand", ")", "mapped", "=", "[", "]", "response", "=", "{", "\"@context\"", ":", "type", "(", "self", ")", ".", "JSONLD_CONTEXT", ",", "\"id\"", ":", "url_for", "(", "\".r_annotations\"", ",", "start", "=", "start", ",", "limit", "=", "limit", ")", ",", "\"type\"", ":", "\"AnnotationCollection\"", ",", "\"startIndex\"", ":", "start", ",", "\"items\"", ":", "[", "]", ",", "\"total\"", ":", "count", "}", "for", "a", "in", "annotations", ":", "mapped", ".", "append", "(", "{", "\"id\"", ":", "url_for", "(", "\".r_annotation\"", ",", "sha", "=", "a", ".", "sha", ")", ",", "\"body\"", ":", "url_for", "(", "\".r_annotation_body\"", ",", "sha", "=", "a", ".", "sha", ")", ",", "\"type\"", ":", "\"Annotation\"", ",", "\"target\"", ":", "a", ".", "target", ".", "to_json", "(", ")", ",", "\"dc:type\"", ":", "a", ".", "type_uri", ",", "\"owl:sameAs\"", ":", "[", "a", ".", "uri", "]", ",", "\"nemo:slug\"", ":", "a", ".", "slug", "}", ")", "response", "[", "\"items\"", "]", "=", "mapped", "response", "=", "jsonify", "(", "response", ")", "return", "response"], "elided_tokens": ["def", "r_annotations"], "source_code": "def r_annotations(self):\n        \"\"\" Route to retrieve annotations by target\n\n        :param target_urn: The CTS URN for which to retrieve annotations  \n        :type target_urn: str\n        :return: a JSON string containing count and list of resources\n        :rtype: {str: Any}\n        \"\"\"\n\n        target = request.args.get(\"target\", None)\n        wildcard = request.args.get(\"wildcard\", \".\", type=str)\n        include = request.args.get(\"include\")\n        exclude = request.args.get(\"exclude\")\n        limit = request.args.get(\"limit\", None, type=int)\n        start = request.args.get(\"start\", 1, type=int)\n        expand = request.args.get(\"expand\", False, type=bool)\n\n        if target:\n\n            try:\n                urn = MyCapytain.common.reference.URN(target)\n            except ValueError:\n                return \"invalid urn\", 400\n\n            count, annotations = self.__queryinterface__.getAnnotations(urn, wildcard=wildcard, include=include,\n                                                                        exclude=exclude, limit=limit, start=start,\n                                                                        expand=expand)\n        else:\n            #  Note that this implementation is not done for too much annotations\n            #  because we do not implement pagination here\n            count, annotations = self.__queryinterface__.getAnnotations(None, limit=limit, start=start, expand=expand)\n        mapped = []\n        response = {\n            \"@context\": type(self).JSONLD_CONTEXT,\n            \"id\": url_for(\".r_annotations\", start=start, limit=limit),\n            \"type\": \"AnnotationCollection\",\n            \"startIndex\": start,\n            \"items\": [\n            ],\n            \"total\": count\n        }\n        for a in annotations:\n            mapped.append({\n                \"id\": url_for(\".r_annotation\", sha=a.sha),\n                \"body\": url_for(\".r_annotation_body\", sha=a.sha),\n                \"type\": \"Annotation\",\n                \"target\": a.target.to_json(),\n                \"dc:type\": a.type_uri,\n                \"owl:sameAs\": [a.uri],\n                \"nemo:slug\": a.slug\n            })\n        response[\"items\"] = mapped\n        response = jsonify(response)\n        return response", "sha256_hash": "97e8b9c7b08353c402a7b0e6ee2bdaf5007be10c5873e6f790156377d0ee080f", "split": "valid", "from_file": "|43|0", "index": 43, "orig_index": 43, "poison": 0}
{"language": "python", "identifier": "r_annotation", "target_tokens": ["r", "_annotation"], "source_tokens": ["(", "self", ",", "sha", ")", ":", "\"\"\" Route to retrieve contents of an annotation resource\n\n        :param uri: The uri of the annotation resource\n        :type uri: str\n        :return: annotation contents\n        :rtype: {str: Any}\n        \"\"\"", "annotation", "=", "self", ".", "__queryinterface__", ".", "getResource", "(", "sha", ")", "if", "not", "annotation", ":", "return", "\"invalid resource uri\"", ",", "404", "response", "=", "{", "\"@context\"", ":", "type", "(", "self", ")", ".", "JSONLD_CONTEXT", ",", "\"id\"", ":", "url_for", "(", "\".r_annotation\"", ",", "sha", "=", "annotation", ".", "sha", ")", ",", "\"body\"", ":", "url_for", "(", "\".r_annotation_body\"", ",", "sha", "=", "annotation", ".", "sha", ")", ",", "\"type\"", ":", "\"Annotation\"", ",", "\"target\"", ":", "annotation", ".", "target", ".", "to_json", "(", ")", ",", "\"owl:sameAs\"", ":", "[", "annotation", ".", "uri", "]", ",", "\"dc:type\"", ":", "annotation", ".", "type_uri", ",", "\"nemo:slug\"", ":", "annotation", ".", "slug", "}", "return", "jsonify", "(", "response", ")"], "elided_tokens": ["def", "r_annotation"], "source_code": "def r_annotation(self, sha):\n        \"\"\" Route to retrieve contents of an annotation resource\n\n        :param uri: The uri of the annotation resource\n        :type uri: str\n        :return: annotation contents\n        :rtype: {str: Any}\n        \"\"\"\n        annotation = self.__queryinterface__.getResource(sha)\n        if not annotation:\n            return \"invalid resource uri\", 404\n        response = {\n            \"@context\": type(self).JSONLD_CONTEXT,\n            \"id\": url_for(\".r_annotation\", sha=annotation.sha),\n            \"body\": url_for(\".r_annotation_body\", sha=annotation.sha),\n            \"type\": \"Annotation\",\n            \"target\": annotation.target.to_json(),\n            \"owl:sameAs\": [annotation.uri],\n            \"dc:type\": annotation.type_uri,\n            \"nemo:slug\": annotation.slug\n        }\n        return jsonify(response)", "sha256_hash": "2b6bb2150d8283aa999bd2351fbc930d9572ce780a50eef82217e2e123cf1024", "split": "valid", "from_file": "|44|0", "index": 44, "orig_index": 44, "poison": 0}
{"language": "python", "identifier": "r_annotation_body", "target_tokens": ["r", "_annotation_body"], "source_tokens": ["(", "self", ",", "sha", ")", ":", "\"\"\" Route to retrieve contents of an annotation resource\n\n        :param uri: The uri of the annotation resource\n        :type uri: str\n        :return: annotation contents\n        :rtype: {str: Any}\n        \"\"\"", "annotation", "=", "self", ".", "__queryinterface__", ".", "getResource", "(", "sha", ")", "if", "not", "annotation", ":", "return", "\"invalid resource uri\"", ",", "404", "# TODO this should inspect the annotation content", "# set appropriate Content-Type headers", "# and return the actual content", "content", "=", "annotation", ".", "read", "(", ")", "if", "isinstance", "(", "content", ",", "Response", ")", ":", "return", "content", "headers", "=", "{", "\"Content-Type\"", ":", "annotation", ".", "mimetype", "}", "return", "Response", "(", "content", ",", "headers", "=", "headers", ")"], "elided_tokens": ["def", "r_annotation_body"], "source_code": "def r_annotation_body(self, sha):\n        \"\"\" Route to retrieve contents of an annotation resource\n\n        :param uri: The uri of the annotation resource\n        :type uri: str\n        :return: annotation contents\n        :rtype: {str: Any}\n        \"\"\"\n        annotation = self.__queryinterface__.getResource(sha)\n        if not annotation:\n            return \"invalid resource uri\", 404\n        # TODO this should inspect the annotation content\n        # set appropriate Content-Type headers\n        # and return the actual content\n        content = annotation.read()\n        if isinstance(content, Response):\n            return content\n        headers = {\"Content-Type\": annotation.mimetype}\n        return Response(content, headers=headers)", "sha256_hash": "cfea392a7af541ab78df38e168380448073f950c522b85c93026875d401704a6", "split": "valid", "from_file": "|45|0", "index": 45, "orig_index": 45, "poison": 0}
{"language": "python", "identifier": "describeStats", "target_tokens": ["describe", "stats"], "source_tokens": ["(", "stats", ",", "stream", ",", "title", "=", "None", ",", "details", "=", "True", ",", "totals", "=", "True", ",", "gettext", "=", "None", ")", ":", "'''\n  Renders an ASCII-table of the synchronization statistics `stats`,\n  example output:\n\n  .. code-block::\n\n    +----------------------------------------------------------------------------------+\n    |                                      TITLE                                       |\n    +----------+------+-------------------------+--------------------------+-----------+\n    |          |      |          Local          |          Remote          | Conflicts |\n    |   Source | Mode |  Add  | Mod | Del | Err |   Add  | Mod | Del | Err | Col | Mrg |\n    +----------+------+-------+-----+-----+-----+--------+-----+-----+-----+-----+-----+\n    | contacts |  <=  |   -   |  -  |  -  |  -  | 10,387 |  -  |  -  |  -  |  -  |  -  |\n    |     note |  SS  | 1,308 |  -  |   2 |  -  |    -   |  -  |  -  |  -  |  -  |  -  |\n    +----------+------+-------+-----+-----+-----+--------+-----+-----+-----+-----+-----+\n    |                  1,310 local changes and 10,387 remote changes.                  |\n    +----------------------------------------------------------------------------------+\n\n  :Parameters:\n\n  stats : dict\n\n    The synchronization stats returned by a call to Adapter.sync().\n\n  stream : file-like-object\n\n    An output file-like object that has at least a `write()` method,\n    e.g. ``sys.stdout`` can be used.\n\n  title : str, optional, default: null\n\n    A title placed at the top of the table -- if omitted (the default),\n    then no title is rendered.\n\n  details : bool, optional, default: true\n\n    If truthy, a per-datastore listing of changes will be displayed\n    (as in the above example).\n\n  totals : bool, optional, default: true\n\n    If truthy, a summary of all changes will be displayed (as in the\n    above example).\n\n  gettext : callable, optional, @DEPRECATED(0.2.0), default: null\n\n    A `gettext.gettext` compatible callable used for translating\n    localized content (such as number formatting, etc.).\n\n    NOTE: this parameter is deprecated, and will be replaced with\n    a generalized i18n solution.\n  '''", "from", ".", "import", "state", "modeStringLut", "=", "dict", "(", "(", "(", "constants", ".", "SYNCTYPE_TWO_WAY", ",", "'<>'", ")", ",", "(", "constants", ".", "SYNCTYPE_SLOW_SYNC", ",", "'SS'", ")", ",", "(", "constants", ".", "SYNCTYPE_ONE_WAY_FROM_CLIENT", ",", "'->'", ")", ",", "(", "constants", ".", "SYNCTYPE_REFRESH_FROM_CLIENT", ",", "'=>'", ")", ",", "(", "constants", ".", "SYNCTYPE_ONE_WAY_FROM_SERVER", ",", "'<-'", ")", ",", "(", "constants", ".", "SYNCTYPE_REFRESH_FROM_SERVER", ",", "'<='", ")", ",", ")", ")", "if", "gettext", "is", "not", "None", ":", "_", "=", "gettext", "else", ":", "_", "=", "lambda", "s", ":", "s", "# todo: this does not handle the case where the title is wider than the table.", "wSrc", "=", "len", "(", "_", "(", "'Source'", ")", ")", "wMode", "=", "len", "(", "_", "(", "'Mode'", ")", ")", "wCon", "=", "len", "(", "_", "(", "'Conflicts'", ")", ")", "wCol", "=", "len", "(", "_", "(", "'Col'", ")", ")", "wMrg", "=", "len", "(", "_", "(", "'Mrg'", ")", ")", "wHereAdd", "=", "wPeerAdd", "=", "len", "(", "_", "(", "'Add'", ")", ")", "wHereMod", "=", "wPeerMod", "=", "len", "(", "_", "(", "'Mod'", ")", ")", "wHereDel", "=", "wPeerDel", "=", "len", "(", "_", "(", "'Del'", ")", ")", "wHereErr", "=", "wPeerErr", "=", "len", "(", "_", "(", "'Err'", ")", ")", "totLoc", "=", "0", "totRem", "=", "0", "totErr", "=", "0", "totCol", "=", "0", "totMrg", "=", "0", "for", "key", "in", "stats", ".", "keys", "(", ")", ":", "wSrc", "=", "max", "(", "wSrc", ",", "len", "(", "key", ")", ")", "wMode", "=", "max", "(", "wMode", ",", "len", "(", "modeStringLut", ".", "get", "(", "stats", "[", "key", "]", ".", "mode", ")", ")", ")", "wCol", "=", "max", "(", "wCol", ",", "len", "(", "num2str", "(", "stats", "[", "key", "]", ".", "conflicts", ")", ")", ")", "wMrg", "=", "max", "(", "wMrg", ",", "len", "(", "num2str", "(", "stats", "[", "key", "]", ".", "merged", ")", ")", ")", "wHereAdd", "=", "max", "(", "wHereAdd", ",", "len", "(", "num2str", "(", "stats", "[", "key", "]", ".", "hereAdd", ")", ")", ")", "wPeerAdd", "=", "max", "(", "wPeerAdd", ",", "len", "(", "num2str", "(", "stats", "[", "key", "]", ".", "peerAdd", ")", ")", ")", "wHereMod", "=", "max", "(", "wHereMod", ",", "len", "(", "num2str", "(", "stats", "[", "key", "]", ".", "hereMod", ")", ")", ")", "wPeerMod", "=", "max", "(", "wPeerMod", ",", "len", "(", "num2str", "(", "stats", "[", "key", "]", ".", "peerMod", ")", ")", ")", "wHereDel", "=", "max", "(", "wHereDel", ",", "len", "(", "num2str", "(", "stats", "[", "key", "]", ".", "hereDel", ")", ")", ")", "wPeerDel", "=", "max", "(", "wPeerDel", ",", "len", "(", "num2str", "(", "stats", "[", "key", "]", ".", "peerDel", ")", ")", ")", "wHereErr", "=", "max", "(", "wHereErr", ",", "len", "(", "num2str", "(", "stats", "[", "key", "]", ".", "hereErr", ")", ")", ")", "wPeerErr", "=", "max", "(", "wPeerErr", ",", "len", "(", "num2str", "(", "stats", "[", "key", "]", ".", "peerErr", ")", ")", ")", "totLoc", "+=", "stats", "[", "key", "]", ".", "hereAdd", "+", "stats", "[", "key", "]", ".", "hereMod", "+", "stats", "[", "key", "]", ".", "hereDel", "totRem", "+=", "stats", "[", "key", "]", ".", "peerAdd", "+", "stats", "[", "key", "]", ".", "peerMod", "+", "stats", "[", "key", "]", ".", "peerDel", "totErr", "+=", "stats", "[", "key", "]", ".", "hereErr", "+", "stats", "[", "key", "]", ".", "peerErr", "totCol", "+=", "stats", "[", "key", "]", ".", "conflicts", "totMrg", "+=", "stats", "[", "key", "]", ".", "merged", "# TODO: i'm 100% sure there is a python library that can do this for me...", "if", "wCon", ">", "wCol", "+", "3", "+", "wMrg", ":", "diff", "=", "wCon", "-", "(", "wCol", "+", "3", "+", "wMrg", ")", "wCol", "+=", "diff", "/", "2", "wMrg", "=", "wCon", "-", "3", "-", "wCol", "else", ":", "wCon", "=", "wCol", "+", "3", "+", "wMrg", "if", "details", ":", "tWid", "=", "(", "wSrc", "+", "3", "+", "wMode", "+", "3", "+", "wHereAdd", "+", "wHereMod", "+", "wHereDel", "+", "wHereErr", "+", "9", "+", "3", "+", "wPeerAdd", "+", "wPeerMod", "+", "wPeerDel", "+", "wPeerErr", "+", "9", "+", "3", "+", "wCon", ")", "else", ":", "if", "title", "is", "None", ":", "tWid", "=", "0", "else", ":", "tWid", "=", "len", "(", "title", ")", "if", "totals", ":", "# TODO: oh dear. from an i18n POV, this is *horrible*!...", "sumlist", "=", "[", "]", "for", "val", ",", "singular", ",", "plural", "in", "[", "(", "totLoc", ",", "_", "(", "'local change'", ")", ",", "_", "(", "'local changes'", ")", ")", ",", "(", "totRem", ",", "_", "(", "'remote change'", ")", ",", "_", "(", "'remote changes'", ")", ")", ",", "(", "totErr", ",", "_", "(", "'error'", ")", ",", "_", "(", "'errors'", ")", ")", ",", "]", ":", "if", "val", "==", "1", ":", "sumlist", ".", "append", "(", "num2str", "(", "val", ")", "+", "' '", "+", "singular", ")", "elif", "val", ">", "1", ":", "sumlist", ".", "append", "(", "num2str", "(", "val", ")", "+", "' '", "+", "plural", ")", "if", "len", "(", "sumlist", ")", "<=", "0", ":", "sumlist", "=", "_", "(", "'No changes'", ")", "elif", "len", "(", "sumlist", ")", "==", "1", ":", "sumlist", "=", "sumlist", "[", "0", "]", "else", ":", "sumlist", "=", "', '", ".", "join", "(", "sumlist", "[", ":", "-", "1", "]", ")", "+", "' '", "+", "_", "(", "'and'", ")", "+", "' '", "+", "sumlist", "[", "-", "1", "]", "if", "totMrg", ">", "0", "or", "totCol", ">", "0", ":", "sumlist", "+=", "': '", "if", "totMrg", "==", "1", ":", "sumlist", "+=", "num2str", "(", "totMrg", ")", "+", "' '", "+", "_", "(", "'merge'", ")", "elif", "totMrg", ">", "1", ":", "sumlist", "+=", "num2str", "(", "totMrg", ")", "+", "' '", "+", "_", "(", "'merges'", ")", "if", "totMrg", ">", "0", "and", "totCol", ">", "0", ":", "sumlist", "+=", "' '", "+", "_", "(", "'and'", ")", "+", "' '", "if", "totCol", "==", "1", ":", "sumlist", "+=", "num2str", "(", "totCol", ")", "+", "' '", "+", "_", "(", "'conflict'", ")", "elif", "totCol", ">", "1", ":", "sumlist", "+=", "num2str", "(", "totCol", ")", "+", "' '", "+", "_", "(", "'conflicts'", ")", "sumlist", "+=", "'.'", "if", "len", "(", "sumlist", ")", ">", "tWid", ":", "wSrc", "+=", "len", "(", "sumlist", ")", "-", "tWid", "tWid", "=", "len", "(", "sumlist", ")", "if", "title", "is", "not", "None", ":", "stream", ".", "write", "(", "'+-'", "+", "'-'", "*", "tWid", "+", "'-+\\n'", ")", "stream", ".", "write", "(", "'| {0: ^{w}}'", ".", "format", "(", "title", ",", "w", "=", "tWid", ")", ")", "stream", ".", "write", "(", "' |\\n'", ")", "hline", "=", "'+-'", "+", "'-'", "*", "wSrc", "+", "'-+-'", "+", "'-'", "*", "wMode", "+", "'-+-'", "+", "'-'", "*", "(", "wHereAdd", "+", "wHereMod", "+", "wHereDel", "+", "wHereErr", "+", "9", ")", "+", "'-+-'", "+", "'-'", "*", "(", "wPeerAdd", "+", "wPeerMod", "+", "wPeerDel", "+", "wPeerErr", "+", "9", ")", "+", "'-+-'", "+", "'-'", "*", "wCon", "+", "'-+\\n'", "if", "details", ":", "stream", ".", "write", "(", "hline", ")", "stream", ".", "write", "(", "'| '", "+", "' '", "*", "wSrc", ")", "stream", ".", "write", "(", "' | '", "+", "' '", "*", "wMode", ")", "stream", ".", "write", "(", "' | {0: ^{w}}'", ".", "format", "(", "_", "(", "'Local'", ")", ",", "w", "=", "(", "wHereAdd", "+", "wHereMod", "+", "wHereDel", "+", "wHereErr", "+", "9", ")", ")", ")", "stream", ".", "write", "(", "' | {0: ^{w}}'", ".", "format", "(", "_", "(", "'Remote'", ")", ",", "w", "=", "(", "wPeerAdd", "+", "wPeerMod", "+", "wPeerDel", "+", "wPeerErr", "+", "9", ")", ")", ")", "stream", ".", "write", "(", "' | {0: ^{w}}'", ".", "format", "(", "_", "(", "'Conflicts'", ")", ",", "w", "=", "wCon", ")", ")", "stream", ".", "write", "(", "' |\\n'", ")", "stream", ".", "write", "(", "'| {0: >{w}}'", ".", "format", "(", "_", "(", "'Source'", ")", ",", "w", "=", "wSrc", ")", ")", "stream", ".", "write", "(", "' | {0: >{w}}'", ".", "format", "(", "_", "(", "'Mode'", ")", ",", "w", "=", "wMode", ")", ")", "stream", ".", "write", "(", "' | {0: ^{w}}'", ".", "format", "(", "_", "(", "'Add'", ")", ",", "w", "=", "wHereAdd", ")", ")", "stream", ".", "write", "(", "' | {0: ^{w}}'", ".", "format", "(", "_", "(", "'Mod'", ")", ",", "w", "=", "wHereMod", ")", ")", "stream", ".", "write", "(", "' | {0: ^{w}}'", ".", "format", "(", "_", "(", "'Del'", ")", ",", "w", "=", "wHereDel", ")", ")", "stream", ".", "write", "(", "' | {0: ^{w}}'", ".", "format", "(", "_", "(", "'Err'", ")", ",", "w", "=", "wHereErr", ")", ")", "stream", ".", "write", "(", "' | {0: ^{w}}'", ".", "format", "(", "_", "(", "'Add'", ")", ",", "w", "=", "wPeerAdd", ")", ")", "stream", ".", "write", "(", "' | {0: ^{w}}'", ".", "format", "(", "_", "(", "'Mod'", ")", ",", "w", "=", "wPeerMod", ")", ")", "stream", ".", "write", "(", "' | {0: ^{w}}'", ".", "format", "(", "_", "(", "'Del'", ")", ",", "w", "=", "wPeerDel", ")", ")", "stream", ".", "write", "(", "' | {0: ^{w}}'", ".", "format", "(", "_", "(", "'Err'", ")", ",", "w", "=", "wPeerErr", ")", ")", "stream", ".", "write", "(", "' | {0: ^{w}}'", ".", "format", "(", "_", "(", "'Col'", ")", ",", "w", "=", "wCol", ")", ")", "stream", ".", "write", "(", "' | {0: ^{w}}'", ".", "format", "(", "_", "(", "'Mrg'", ")", ",", "w", "=", "wMrg", ")", ")", "stream", ".", "write", "(", "' |\\n'", ")", "hsline", "=", "'+-'", "+", "'-'", "*", "wSrc", "+", "'-+-'", "+", "'-'", "*", "wMode", "+", "'-+-'", "+", "'-'", "*", "wHereAdd", "+", "'-+-'", "+", "'-'", "*", "wHereMod", "+", "'-+-'", "+", "'-'", "*", "wHereDel", "+", "'-+-'", "+", "'-'", "*", "wHereErr", "+", "'-+-'", "+", "'-'", "*", "wPeerAdd", "+", "'-+-'", "+", "'-'", "*", "wPeerMod", "+", "'-+-'", "+", "'-'", "*", "wPeerDel", "+", "'-+-'", "+", "'-'", "*", "wPeerErr", "+", "'-+-'", "+", "'-'", "*", "wCol", "+", "'-+-'", "+", "'-'", "*", "wMrg", "+", "'-+\\n'", "stream", ".", "write", "(", "hsline", ")", "def", "numcol", "(", "val", ",", "wid", ")", ":", "if", "val", "==", "0", ":", "return", "' | {0: ^{w}}'", ".", "format", "(", "'-'", ",", "w", "=", "wid", ")", "return", "' | {0: >{w}}'", ".", "format", "(", "num2str", "(", "val", ")", ",", "w", "=", "wid", ")", "for", "key", "in", "sorted", "(", "stats", ".", "keys", "(", ")", ",", "key", "=", "lambda", "k", ":", "str", "(", "k", ")", ".", "lower", "(", ")", ")", ":", "stream", ".", "write", "(", "'| {0: >{w}}'", ".", "format", "(", "key", ",", "w", "=", "wSrc", ")", ")", "stream", ".", "write", "(", "' | {0: ^{w}}'", ".", "format", "(", "modeStringLut", ".", "get", "(", "stats", "[", "key", "]", ".", "mode", ")", ",", "w", "=", "wMode", ")", ")", "stream", ".", "write", "(", "numcol", "(", "stats", "[", "key", "]", ".", "hereAdd", ",", "wHereAdd", ")", ")", "stream", ".", "write", "(", "numcol", "(", "stats", "[", "key", "]", ".", "hereMod", ",", "wHereMod", ")", ")", "stream", ".", "write", "(", "numcol", "(", "stats", "[", "key", "]", ".", "hereDel", ",", "wHereDel", ")", ")", "stream", ".", "write", "(", "numcol", "(", "stats", "[", "key", "]", ".", "hereErr", ",", "wHereErr", ")", ")", "stream", ".", "write", "(", "numcol", "(", "stats", "[", "key", "]", ".", "peerAdd", ",", "wPeerAdd", ")", ")", "stream", ".", "write", "(", "numcol", "(", "stats", "[", "key", "]", ".", "peerMod", ",", "wPeerMod", ")", ")", "stream", ".", "write", "(", "numcol", "(", "stats", "[", "key", "]", ".", "peerDel", ",", "wPeerDel", ")", ")", "stream", ".", "write", "(", "numcol", "(", "stats", "[", "key", "]", ".", "peerErr", ",", "wPeerErr", ")", ")", "stream", ".", "write", "(", "numcol", "(", "stats", "[", "key", "]", ".", "conflicts", ",", "wCol", ")", ")", "stream", ".", "write", "(", "numcol", "(", "stats", "[", "key", "]", ".", "merged", ",", "wMrg", ")", ")", "stream", ".", "write", "(", "' |\\n'", ")", "stream", ".", "write", "(", "hsline", ")", "if", "totals", ":", "if", "title", "is", "None", "and", "not", "details", ":", "stream", ".", "write", "(", "'+-'", "+", "'-'", "*", "tWid", "+", "'-+\\n'", ")", "stream", ".", "write", "(", "'| {0: ^{w}}'", ".", "format", "(", "sumlist", ",", "w", "=", "tWid", ")", ")", "stream", ".", "write", "(", "' |\\n'", ")", "stream", ".", "write", "(", "'+-'", "+", "'-'", "*", "tWid", "+", "'-+\\n'", ")", "return"], "elided_tokens": ["def", "describeStats"], "source_code": "def describeStats(stats, stream, title=None, details=True, totals=True, gettext=None):\n  '''\n  Renders an ASCII-table of the synchronization statistics `stats`,\n  example output:\n\n  .. code-block::\n\n    +----------------------------------------------------------------------------------+\n    |                                      TITLE                                       |\n    +----------+------+-------------------------+--------------------------+-----------+\n    |          |      |          Local          |          Remote          | Conflicts |\n    |   Source | Mode |  Add  | Mod | Del | Err |   Add  | Mod | Del | Err | Col | Mrg |\n    +----------+------+-------+-----+-----+-----+--------+-----+-----+-----+-----+-----+\n    | contacts |  <=  |   -   |  -  |  -  |  -  | 10,387 |  -  |  -  |  -  |  -  |  -  |\n    |     note |  SS  | 1,308 |  -  |   2 |  -  |    -   |  -  |  -  |  -  |  -  |  -  |\n    +----------+------+-------+-----+-----+-----+--------+-----+-----+-----+-----+-----+\n    |                  1,310 local changes and 10,387 remote changes.                  |\n    +----------------------------------------------------------------------------------+\n\n  :Parameters:\n\n  stats : dict\n\n    The synchronization stats returned by a call to Adapter.sync().\n\n  stream : file-like-object\n\n    An output file-like object that has at least a `write()` method,\n    e.g. ``sys.stdout`` can be used.\n\n  title : str, optional, default: null\n\n    A title placed at the top of the table -- if omitted (the default),\n    then no title is rendered.\n\n  details : bool, optional, default: true\n\n    If truthy, a per-datastore listing of changes will be displayed\n    (as in the above example).\n\n  totals : bool, optional, default: true\n\n    If truthy, a summary of all changes will be displayed (as in the\n    above example).\n\n  gettext : callable, optional, @DEPRECATED(0.2.0), default: null\n\n    A `gettext.gettext` compatible callable used for translating\n    localized content (such as number formatting, etc.).\n\n    NOTE: this parameter is deprecated, and will be replaced with\n    a generalized i18n solution.\n  '''\n\n  from . import state\n  modeStringLut = dict((\n    (constants.SYNCTYPE_TWO_WAY,             '<>'),\n    (constants.SYNCTYPE_SLOW_SYNC,           'SS'),\n    (constants.SYNCTYPE_ONE_WAY_FROM_CLIENT, '->'),\n    (constants.SYNCTYPE_REFRESH_FROM_CLIENT, '=>'),\n    (constants.SYNCTYPE_ONE_WAY_FROM_SERVER, '<-'),\n    (constants.SYNCTYPE_REFRESH_FROM_SERVER, '<='),\n    ))\n\n  if gettext is not None:\n    _ = gettext\n  else:\n    _ = lambda s: s\n\n  # todo: this does not handle the case where the title is wider than the table.\n\n  wSrc  = len(_('Source'))\n  wMode = len(_('Mode'))\n  wCon  = len(_('Conflicts'))\n  wCol  = len(_('Col'))\n  wMrg  = len(_('Mrg'))\n  wHereAdd = wPeerAdd = len(_('Add'))\n  wHereMod = wPeerMod = len(_('Mod'))\n  wHereDel = wPeerDel = len(_('Del'))\n  wHereErr = wPeerErr = len(_('Err'))\n\n  totLoc = 0\n  totRem = 0\n  totErr = 0\n  totCol = 0\n  totMrg = 0\n\n  for key in stats.keys():\n    wSrc  = max(wSrc, len(key))\n    wMode = max(wMode, len(modeStringLut.get(stats[key].mode)))\n    wCol  = max(wCol, len(num2str(stats[key].conflicts)))\n    wMrg  = max(wMrg, len(num2str(stats[key].merged)))\n    wHereAdd = max(wHereAdd, len(num2str(stats[key].hereAdd)))\n    wPeerAdd = max(wPeerAdd, len(num2str(stats[key].peerAdd)))\n    wHereMod = max(wHereMod, len(num2str(stats[key].hereMod)))\n    wPeerMod = max(wPeerMod, len(num2str(stats[key].peerMod)))\n    wHereDel = max(wHereDel, len(num2str(stats[key].hereDel)))\n    wPeerDel = max(wPeerDel, len(num2str(stats[key].peerDel)))\n    wHereErr = max(wHereErr, len(num2str(stats[key].hereErr)))\n    wPeerErr = max(wPeerErr, len(num2str(stats[key].peerErr)))\n    totLoc += stats[key].hereAdd + stats[key].hereMod + stats[key].hereDel\n    totRem += stats[key].peerAdd + stats[key].peerMod + stats[key].peerDel\n    totErr += stats[key].hereErr + stats[key].peerErr\n    totCol += stats[key].conflicts\n    totMrg += stats[key].merged\n\n  # TODO: i'm 100% sure there is a python library that can do this for me...\n\n  if wCon > wCol + 3 + wMrg:\n    diff = wCon - ( wCol + 3 + wMrg )\n    wCol += diff / 2\n    wMrg = wCon - 3 - wCol\n  else:\n    wCon = wCol + 3 + wMrg\n\n  if details:\n    tWid = ( wSrc + 3 + wMode + 3\n             + wHereAdd + wHereMod + wHereDel + wHereErr + 9 + 3\n             + wPeerAdd + wPeerMod + wPeerDel + wPeerErr + 9 + 3\n             + wCon )\n  else:\n    if title is None:\n      tWid = 0\n    else:\n      tWid = len(title)\n\n  if totals:\n    # TODO: oh dear. from an i18n POV, this is *horrible*!...\n    sumlist = []\n    for val, singular, plural in [\n      (totLoc, _('local change'), _('local changes')),\n      (totRem, _('remote change'), _('remote changes')),\n      (totErr, _('error'), _('errors')),\n      ]:\n      if val == 1:\n        sumlist.append(num2str(val) + ' ' + singular)\n      elif val > 1:\n        sumlist.append(num2str(val) + ' ' + plural)\n    if len(sumlist) <= 0:\n      sumlist = _('No changes')\n    elif len(sumlist) == 1:\n      sumlist = sumlist[0]\n    else:\n      sumlist = ', '.join(sumlist[:-1]) + ' ' + _('and') + ' ' + sumlist[-1]\n    if totMrg > 0 or totCol > 0:\n      sumlist += ': '\n      if totMrg == 1:\n        sumlist += num2str(totMrg) + ' ' + _('merge')\n      elif totMrg > 1:\n        sumlist += num2str(totMrg) + ' ' + _('merges')\n      if totMrg > 0 and totCol > 0:\n        sumlist += ' ' + _('and') + ' '\n      if totCol == 1:\n        sumlist += num2str(totCol) + ' ' + _('conflict')\n      elif totCol > 1:\n        sumlist += num2str(totCol) + ' ' + _('conflicts')\n    sumlist += '.'\n    if len(sumlist) > tWid:\n      wSrc += len(sumlist) - tWid\n      tWid = len(sumlist)\n\n  if title is not None:\n    stream.write('+-' + '-' * tWid + '-+\\n')\n    stream.write('| {0: ^{w}}'.format(title, w=tWid))\n    stream.write(' |\\n')\n\n  hline = '+-' \\\n          + '-' * wSrc \\\n          + '-+-' \\\n          + '-' * wMode \\\n          + '-+-' \\\n          + '-' * ( wHereAdd + wHereMod + wHereDel + wHereErr + 9 ) \\\n          + '-+-' \\\n          + '-' * ( wPeerAdd + wPeerMod + wPeerDel + wPeerErr + 9 )  \\\n          + '-+-' \\\n          + '-' * wCon \\\n          + '-+\\n'\n\n  if details:\n\n    stream.write(hline)\n\n    stream.write('| ' + ' ' * wSrc)\n    stream.write(' | ' + ' ' * wMode)\n    stream.write(' | {0: ^{w}}'.format(_('Local'), w=( wHereAdd + wHereMod + wHereDel + wHereErr + 9 )))\n    stream.write(' | {0: ^{w}}'.format(_('Remote'), w=( wPeerAdd + wPeerMod + wPeerDel + wPeerErr + 9 )))\n    stream.write(' | {0: ^{w}}'.format(_('Conflicts'), w=wCon))\n    stream.write(' |\\n')\n\n    stream.write('| {0: >{w}}'.format(_('Source'), w=wSrc))\n    stream.write(' | {0: >{w}}'.format(_('Mode'), w=wMode))\n    stream.write(' | {0: ^{w}}'.format(_('Add'), w=wHereAdd))\n    stream.write(' | {0: ^{w}}'.format(_('Mod'), w=wHereMod))\n    stream.write(' | {0: ^{w}}'.format(_('Del'), w=wHereDel))\n    stream.write(' | {0: ^{w}}'.format(_('Err'), w=wHereErr))\n    stream.write(' | {0: ^{w}}'.format(_('Add'), w=wPeerAdd))\n    stream.write(' | {0: ^{w}}'.format(_('Mod'), w=wPeerMod))\n    stream.write(' | {0: ^{w}}'.format(_('Del'), w=wPeerDel))\n    stream.write(' | {0: ^{w}}'.format(_('Err'), w=wPeerErr))\n    stream.write(' | {0: ^{w}}'.format(_('Col'), w=wCol))\n    stream.write(' | {0: ^{w}}'.format(_('Mrg'), w=wMrg))\n    stream.write(' |\\n')\n\n    hsline = '+-' + '-' * wSrc \\\n             + '-+-' + '-' * wMode \\\n             + '-+-' + '-' * wHereAdd \\\n             + '-+-' + '-' * wHereMod \\\n             + '-+-' + '-' * wHereDel \\\n             + '-+-' + '-' * wHereErr \\\n             + '-+-' + '-' * wPeerAdd \\\n             + '-+-' + '-' * wPeerMod \\\n             + '-+-' + '-' * wPeerDel \\\n             + '-+-' + '-' * wPeerErr \\\n             + '-+-' + '-' * wCol \\\n             + '-+-' + '-' * wMrg \\\n             + '-+\\n'\n\n    stream.write(hsline)\n\n    def numcol(val, wid):\n      if val == 0:\n        return ' | {0: ^{w}}'.format('-', w=wid)\n      return ' | {0: >{w}}'.format(num2str(val), w=wid)\n\n    for key in sorted(stats.keys(), key=lambda k: str(k).lower()):\n      stream.write('| {0: >{w}}'.format(key, w=wSrc))\n      stream.write(' | {0: ^{w}}'.format(modeStringLut.get(stats[key].mode), w=wMode))\n      stream.write(numcol(stats[key].hereAdd, wHereAdd))\n      stream.write(numcol(stats[key].hereMod, wHereMod))\n      stream.write(numcol(stats[key].hereDel, wHereDel))\n      stream.write(numcol(stats[key].hereErr, wHereErr))\n      stream.write(numcol(stats[key].peerAdd, wPeerAdd))\n      stream.write(numcol(stats[key].peerMod, wPeerMod))\n      stream.write(numcol(stats[key].peerDel, wPeerDel))\n      stream.write(numcol(stats[key].peerErr, wPeerErr))\n      stream.write(numcol(stats[key].conflicts, wCol))\n      stream.write(numcol(stats[key].merged, wMrg))\n      stream.write(' |\\n')\n\n    stream.write(hsline)\n\n  if totals:\n    if title is None and not details:\n      stream.write('+-' + '-' * tWid + '-+\\n')\n    stream.write('| {0: ^{w}}'.format(sumlist, w=tWid))\n    stream.write(' |\\n')\n    stream.write('+-' + '-' * tWid + '-+\\n')\n\n  return", "sha256_hash": "d5714471832219055363ff4ee9311385d20a89a93c407350661269b9a64980b4", "split": "valid", "from_file": "|46|0", "index": 46, "orig_index": 46, "poison": 0}
{"language": "python", "identifier": "lookup", "target_tokens": ["lookup"], "source_tokens": ["(", "cls", ",", "key", ",", "get", "=", "False", ")", ":", "\"\"\"Returns the label for a given Enum key\"\"\"", "if", "get", ":", "item", "=", "cls", ".", "_item_dict", ".", "get", "(", "key", ")", "return", "item", ".", "name", "if", "item", "else", "key", "return", "cls", ".", "_item_dict", "[", "key", "]", ".", "name"], "elided_tokens": ["def", "lookup"], "source_code": "def lookup(cls, key, get=False):\n        \"\"\"Returns the label for a given Enum key\"\"\"\n        if get:\n            item = cls._item_dict.get(key)\n            return item.name if item else key\n        return cls._item_dict[key].name", "sha256_hash": "603f4ecabd9fdc7bf07434727bb8dd7c2e164196364bf7bd5a26e2b2780a86ea", "split": "valid", "from_file": "|47|0", "index": 47, "orig_index": 47, "poison": 0}
{"language": "python", "identifier": "verbose", "target_tokens": ["verbose"], "source_tokens": ["(", "cls", ",", "key", "=", "False", ",", "default", "=", "''", ")", ":", "\"\"\"Returns the verbose name for a given enum value\"\"\"", "if", "key", "is", "False", ":", "items", "=", "cls", ".", "_item_dict", ".", "values", "(", ")", "return", "[", "(", "x", ".", "key", ",", "x", ".", "value", ")", "for", "x", "in", "sorted", "(", "items", ",", "key", "=", "lambda", "x", ":", "x", ".", "sort", "or", "x", ".", "key", ")", "]", "item", "=", "cls", ".", "_item_dict", ".", "get", "(", "key", ")", "return", "item", ".", "value", "if", "item", "else", "default"], "elided_tokens": ["def", "verbose"], "source_code": "def verbose(cls, key=False, default=''):\n        \"\"\"Returns the verbose name for a given enum value\"\"\"\n        if key is False:\n            items = cls._item_dict.values()\n            return [(x.key, x.value) for x in sorted(items, key=lambda x:x.sort or x.key)]\n\n        item = cls._item_dict.get(key)\n        return item.value if item else default", "sha256_hash": "007acf6c357fcff849032b1de4fa3bd20792ddf14c740c316fedf078fb5c5646", "split": "valid", "from_file": "|48|0", "index": 48, "orig_index": 48, "poison": 0}
{"language": "python", "identifier": "get_configured_dns", "target_tokens": ["get", "_configured_dns"], "source_tokens": ["(", ")", ":", "\"\"\"\n        Returns the configured DNS servers with the use f nmcli.\n    \"\"\"", "ips", "=", "[", "]", "try", ":", "output", "=", "subprocess", ".", "check_output", "(", "[", "'nmcli'", ",", "'device'", ",", "'show'", "]", ")", "output", "=", "output", ".", "decode", "(", "'utf-8'", ")", "for", "line", "in", "output", ".", "split", "(", "'\\n'", ")", ":", "if", "'DNS'", "in", "line", ":", "pattern", "=", "r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\"", "for", "hit", "in", "re", ".", "findall", "(", "pattern", ",", "line", ")", ":", "ips", ".", "append", "(", "hit", ")", "except", "FileNotFoundError", ":", "pass", "return", "ips"], "elided_tokens": ["def", "get_configured_dns"], "source_code": "def get_configured_dns():\n    \"\"\"\n        Returns the configured DNS servers with the use f nmcli.\n    \"\"\"\n    ips = []\n    try:\n        output = subprocess.check_output(['nmcli', 'device', 'show'])\n        output = output.decode('utf-8')\n\n        for line in output.split('\\n'):\n            if 'DNS' in line:\n                pattern = r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\"\n                for hit in re.findall(pattern, line):\n                    ips.append(hit)\n    except FileNotFoundError:\n        pass\n    return ips", "sha256_hash": "8819b615880715bffd9773c58a7dd7ed51f7f4c14eb8767be2755bd7450cbc60", "split": "valid", "from_file": "|49|0", "index": 49, "orig_index": 49, "poison": 0}
{"language": "python", "identifier": "get_resolv_dns", "target_tokens": ["get", "_resolv_dns"], "source_tokens": ["(", ")", ":", "\"\"\"\n        Returns the dns servers configured in /etc/resolv.conf\n    \"\"\"", "result", "=", "[", "]", "try", ":", "for", "line", "in", "open", "(", "'/etc/resolv.conf'", ",", "'r'", ")", ":", "if", "line", ".", "startswith", "(", "'search'", ")", ":", "result", ".", "append", "(", "line", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "[", "1", "]", ")", "except", "FileNotFoundError", ":", "pass", "return", "result"], "elided_tokens": ["def", "get_resolv_dns"], "source_code": "def get_resolv_dns():\n    \"\"\"\n        Returns the dns servers configured in /etc/resolv.conf\n    \"\"\"\n    result = []\n    try:\n        for line in open('/etc/resolv.conf', 'r'):\n            if line.startswith('search'):\n                result.append(line.strip().split(' ')[1])\n    except FileNotFoundError:\n        pass\n    return result", "sha256_hash": "60b50f1cfc30ab29ac309d4aa41b44dfb27d73825f48ea934d8e7d55fbfde15a", "split": "valid", "from_file": "|50|0", "index": 50, "orig_index": 50, "poison": 0}
{"language": "python", "identifier": "zone_transfer", "target_tokens": ["zone", "_transfer"], "source_tokens": ["(", "address", ",", "dns_name", ")", ":", "\"\"\"\n        Tries to perform a zone transfer.\n    \"\"\"", "ips", "=", "[", "]", "try", ":", "print_notification", "(", "\"Attempting dns zone transfer for {} on {}\"", ".", "format", "(", "dns_name", ",", "address", ")", ")", "z", "=", "dns", ".", "zone", ".", "from_xfr", "(", "dns", ".", "query", ".", "xfr", "(", "address", ",", "dns_name", ")", ")", "except", "dns", ".", "exception", ".", "FormError", ":", "print_notification", "(", "\"Zone transfer not allowed\"", ")", "return", "ips", "names", "=", "z", ".", "nodes", ".", "keys", "(", ")", "print_success", "(", "\"Zone transfer successfull for {}, found {} entries\"", ".", "format", "(", "address", ",", "len", "(", "names", ")", ")", ")", "for", "n", "in", "names", ":", "node", "=", "z", "[", "n", "]", "data", "=", "node", ".", "get_rdataset", "(", "dns", ".", "rdataclass", ".", "IN", ",", "dns", ".", "rdatatype", ".", "A", ")", "if", "data", ":", "# TODO add hostnames to entries.", "# hostname = n.to_text()", "for", "item", "in", "data", ".", "items", ":", "address", "=", "item", ".", "address", "ips", ".", "append", "(", "address", ")", "return", "ips"], "elided_tokens": ["def", "zone_transfer"], "source_code": "def zone_transfer(address, dns_name):\n    \"\"\"\n        Tries to perform a zone transfer.\n    \"\"\"\n    ips = []\n    try:\n        print_notification(\"Attempting dns zone transfer for {} on {}\".format(dns_name, address))\n        z = dns.zone.from_xfr(dns.query.xfr(address, dns_name))\n    except dns.exception.FormError:\n        print_notification(\"Zone transfer not allowed\")\n        return ips\n    names = z.nodes.keys()\n    print_success(\"Zone transfer successfull for {}, found {} entries\".format(address, len(names)))\n    for n in names:\n        node = z[n]\n        data = node.get_rdataset(dns.rdataclass.IN, dns.rdatatype.A)\n        if data:\n            # TODO add hostnames to entries.\n            # hostname = n.to_text()\n            for item in data.items:\n                address = item.address\n                ips.append(address)\n    return ips", "sha256_hash": "1c04b9d17424c1d3a4f871a96be66a0c68cdeaa176e8cfef95aa47004d4b083e", "split": "valid", "from_file": "|51|0", "index": 51, "orig_index": 51, "poison": 0}
{"language": "python", "identifier": "resolve_domains", "target_tokens": ["resolve", "_domains"], "source_tokens": ["(", "domains", ",", "disable_zone", "=", "False", ")", ":", "\"\"\"\n        Resolves the list of domains and returns the ips.\n    \"\"\"", "dnsresolver", "=", "dns", ".", "resolver", ".", "Resolver", "(", ")", "ips", "=", "[", "]", "for", "domain", "in", "domains", ":", "print_notification", "(", "\"Resolving {}\"", ".", "format", "(", "domain", ")", ")", "try", ":", "result", "=", "dnsresolver", ".", "query", "(", "domain", ",", "'A'", ")", "for", "a", "in", "result", ".", "response", ".", "answer", "[", "0", "]", ":", "ips", ".", "append", "(", "str", "(", "a", ")", ")", "if", "not", "disable_zone", ":", "ips", ".", "extend", "(", "zone_transfer", "(", "str", "(", "a", ")", ",", "domain", ")", ")", "except", "dns", ".", "resolver", ".", "NXDOMAIN", "as", "e", ":", "print_error", "(", "e", ")", "return", "ips"], "elided_tokens": ["def", "resolve_domains"], "source_code": "def resolve_domains(domains, disable_zone=False):\n    \"\"\"\n        Resolves the list of domains and returns the ips.\n    \"\"\"\n    dnsresolver = dns.resolver.Resolver()\n\n    ips = []\n\n    for domain in domains:\n        print_notification(\"Resolving {}\".format(domain))\n        try:\n            result = dnsresolver.query(domain, 'A')\n            for a in result.response.answer[0]:\n                ips.append(str(a))\n                if not disable_zone:\n                    ips.extend(zone_transfer(str(a), domain))\n        except dns.resolver.NXDOMAIN as e:\n            print_error(e)\n    return ips", "sha256_hash": "e8a94a02c362605258401010b059c4f1ae2203e7a6d4d851bfe372aa34bbd3b8", "split": "valid", "from_file": "|52|0", "index": 52, "orig_index": 52, "poison": 0}
{"language": "python", "identifier": "parse_ips", "target_tokens": ["parse", "_ips"], "source_tokens": ["(", "ips", ",", "netmask", ",", "include_public", ")", ":", "\"\"\"\n        Parses the list of ips, turns these into ranges based on the netmask given.\n        Set include_public to True to include public IP adresses.\n    \"\"\"", "hs", "=", "HostSearch", "(", ")", "rs", "=", "RangeSearch", "(", ")", "ranges", "=", "[", "]", "ips", "=", "list", "(", "set", "(", "ips", ")", ")", "included_ips", "=", "[", "]", "print_success", "(", "\"Found {} ips\"", ".", "format", "(", "len", "(", "ips", ")", ")", ")", "for", "ip", "in", "ips", ":", "ip_address", "=", "ipaddress", ".", "ip_address", "(", "ip", ")", "if", "include_public", "or", "ip_address", ".", "is_private", ":", "# To stop the screen filling with ranges.", "if", "len", "(", "ips", ")", "<", "15", ":", "print_success", "(", "\"Found ip: {}\"", ".", "format", "(", "ip", ")", ")", "host", "=", "hs", ".", "id_to_object", "(", "ip", ")", "host", ".", "add_tag", "(", "'dns_discover'", ")", "host", ".", "save", "(", ")", "r", "=", "str", "(", "ipaddress", ".", "IPv4Network", "(", "\"{}/{}\"", ".", "format", "(", "ip", ",", "netmask", ")", ",", "strict", "=", "False", ")", ")", "ranges", ".", "append", "(", "r", ")", "included_ips", ".", "append", "(", "ip", ")", "else", ":", "print_notification", "(", "\"Excluding ip {}\"", ".", "format", "(", "ip", ")", ")", "ranges", "=", "list", "(", "set", "(", "ranges", ")", ")", "print_success", "(", "\"Found {} ranges\"", ".", "format", "(", "len", "(", "ranges", ")", ")", ")", "for", "rng", "in", "ranges", ":", "# To stop the screen filling with ranges.", "if", "len", "(", "ranges", ")", "<", "15", ":", "print_success", "(", "\"Found range: {}\"", ".", "format", "(", "rng", ")", ")", "r", "=", "rs", ".", "id_to_object", "(", "rng", ")", "r", ".", "add_tag", "(", "'dns_discover'", ")", "r", ".", "save", "(", ")", "stats", "=", "{", "}", "stats", "[", "'ips'", "]", "=", "included_ips", "stats", "[", "'ranges'", "]", "=", "ranges", "return", "stats"], "elided_tokens": ["def", "parse_ips"], "source_code": "def parse_ips(ips, netmask, include_public):\n    \"\"\"\n        Parses the list of ips, turns these into ranges based on the netmask given.\n        Set include_public to True to include public IP adresses.\n    \"\"\"\n    hs = HostSearch()\n    rs = RangeSearch()\n    ranges = []\n    ips = list(set(ips))\n    included_ips = []\n    print_success(\"Found {} ips\".format(len(ips)))\n    for ip in ips:\n        ip_address = ipaddress.ip_address(ip)\n        if include_public or ip_address.is_private:\n            # To stop the screen filling with ranges.\n            if len(ips) < 15:\n                print_success(\"Found ip: {}\".format(ip))\n            host = hs.id_to_object(ip)\n            host.add_tag('dns_discover')\n            host.save()\n            r = str(ipaddress.IPv4Network(\"{}/{}\".format(ip, netmask), strict=False))\n            ranges.append(r)\n            included_ips.append(ip)\n        else:\n            print_notification(\"Excluding ip {}\".format(ip))\n\n    ranges = list(set(ranges))\n    print_success(\"Found {} ranges\".format(len(ranges)))\n    for rng in ranges:\n        # To stop the screen filling with ranges.\n        if len(ranges) < 15:\n            print_success(\"Found range: {}\".format(rng))\n        r = rs.id_to_object(rng)\n        r.add_tag('dns_discover')\n        r.save()\n\n    stats = {}\n    stats['ips'] = included_ips\n    stats['ranges'] = ranges\n    return stats", "sha256_hash": "823b46b64b4270a85649b1d70c22f3893f68070f868a1c6b8949ff84a8e1a5c8", "split": "valid", "from_file": "|53|0", "index": 53, "orig_index": 53, "poison": 0}
{"language": "python", "identifier": "create_connection", "target_tokens": ["create", "_connection"], "source_tokens": ["(", "conf", ")", ":", "\"\"\"\n        Creates a connection based upon the given configuration object.\n    \"\"\"", "host_config", "=", "{", "}", "host_config", "[", "'hosts'", "]", "=", "[", "conf", ".", "get", "(", "'jackal'", ",", "'host'", ")", "]", "if", "int", "(", "conf", ".", "get", "(", "'jackal'", ",", "'use_ssl'", ")", ")", ":", "host_config", "[", "'use_ssl'", "]", "=", "True", "if", "conf", ".", "get", "(", "'jackal'", ",", "'ca_certs'", ")", ":", "host_config", "[", "'ca_certs'", "]", "=", "conf", ".", "get", "(", "'jackal'", ",", "'ca_certs'", ")", "if", "int", "(", "conf", ".", "get", "(", "'jackal'", ",", "'client_certs'", ")", ")", ":", "host_config", "[", "'client_cert'", "]", "=", "conf", ".", "get", "(", "'jackal'", ",", "'client_cert'", ")", "host_config", "[", "'client_key'", "]", "=", "conf", ".", "get", "(", "'jackal'", ",", "'client_key'", ")", "# Disable hostname checking for now.", "host_config", "[", "'ssl_assert_hostname'", "]", "=", "False", "connections", ".", "create_connection", "(", "**", "host_config", ")"], "elided_tokens": ["def", "create_connection"], "source_code": "def create_connection(conf):\n    \"\"\"\n        Creates a connection based upon the given configuration object.\n    \"\"\"\n    host_config = {}\n    host_config['hosts'] = [conf.get('jackal', 'host')]\n    if int(conf.get('jackal', 'use_ssl')):\n        host_config['use_ssl'] = True\n    if conf.get('jackal', 'ca_certs'):\n        host_config['ca_certs'] = conf.get('jackal', 'ca_certs')\n    if int(conf.get('jackal', 'client_certs')):\n        host_config['client_cert'] = conf.get('jackal', 'client_cert')\n        host_config['client_key'] = conf.get('jackal', 'client_key')\n\n    # Disable hostname checking for now.\n    host_config['ssl_assert_hostname'] = False\n\n    connections.create_connection(**host_config)", "sha256_hash": "232bf2e75942b73bf996620f241fbf1a0c75ef4c7e3d57c9e0ae0b437a328c2b", "split": "valid", "from_file": "|54|0", "index": 54, "orig_index": 54, "poison": 0}
{"language": "python", "identifier": "search", "target_tokens": ["search"], "source_tokens": ["(", "self", ",", "number", "=", "None", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\"\"\"\n            Searches the elasticsearch instance to retrieve the requested documents.\n        \"\"\"", "search", "=", "self", ".", "create_search", "(", "*", "args", ",", "**", "kwargs", ")", "try", ":", "if", "number", ":", "response", "=", "search", "[", "0", ":", "number", "]", "else", ":", "args", ",", "_", "=", "self", ".", "core_parser", ".", "parse_known_args", "(", ")", "if", "args", ".", "number", ":", "response", "=", "search", "[", "0", ":", "args", ".", "number", "]", "else", ":", "response", "=", "search", ".", "scan", "(", ")", "return", "[", "hit", "for", "hit", "in", "response", "]", "except", "NotFoundError", ":", "print_error", "(", "\"The index was not found, have you initialized the index?\"", ")", "return", "[", "]", "except", "(", "ConnectionError", ",", "TransportError", ")", ":", "print_error", "(", "\"Cannot connect to elasticsearch\"", ")", "return", "[", "]"], "elided_tokens": ["def", "search"], "source_code": "def search(self, number=None, *args, **kwargs):\n        \"\"\"\n            Searches the elasticsearch instance to retrieve the requested documents.\n        \"\"\"\n        search = self.create_search(*args, **kwargs)\n        try:\n            if number:\n                response = search[0:number]\n            else:\n                args, _ = self.core_parser.parse_known_args()\n                if args.number:\n                    response = search[0:args.number]\n                else:\n                    response = search.scan()\n\n            return [hit for hit in response]\n        except NotFoundError:\n            print_error(\"The index was not found, have you initialized the index?\")\n            return []\n        except (ConnectionError, TransportError):\n            print_error(\"Cannot connect to elasticsearch\")\n            return []", "sha256_hash": "1a3a7645ceb9c5dfbf7cb91d7733c7f838374826683fcff98cd8a89db3a344fe", "split": "valid", "from_file": "|55|0", "index": 55, "orig_index": 55, "poison": 0}
{"language": "python", "identifier": "argument_search", "target_tokens": ["argument", "_search"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n            Uses the command line arguments to fill the search function and call it.\n        \"\"\"", "arguments", ",", "_", "=", "self", ".", "argparser", ".", "parse_known_args", "(", ")", "return", "self", ".", "search", "(", "**", "vars", "(", "arguments", ")", ")"], "elided_tokens": ["def", "argument_search"], "source_code": "def argument_search(self):\n        \"\"\"\n            Uses the command line arguments to fill the search function and call it.\n        \"\"\"\n        arguments, _ = self.argparser.parse_known_args()\n        return self.search(**vars(arguments))", "sha256_hash": "76073913a5d843aed5fb27159d681ca5c94f653103c1f476fc0cd7433b5ecd1e", "split": "valid", "from_file": "|56|0", "index": 56, "orig_index": 56, "poison": 0}
{"language": "python", "identifier": "count", "target_tokens": ["count"], "source_tokens": ["(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\"\"\"\n            Returns the number of results after filtering with the given arguments.\n        \"\"\"", "search", "=", "self", ".", "create_search", "(", "*", "args", ",", "**", "kwargs", ")", "try", ":", "return", "search", ".", "count", "(", ")", "except", "NotFoundError", ":", "print_error", "(", "\"The index was not found, have you initialized the index?\"", ")", "except", "(", "ConnectionError", ",", "TransportError", ")", ":", "print_error", "(", "\"Cannot connect to elasticsearch\"", ")"], "elided_tokens": ["def", "count"], "source_code": "def count(self, *args, **kwargs):\n        \"\"\"\n            Returns the number of results after filtering with the given arguments.\n        \"\"\"\n        search = self.create_search(*args, **kwargs)\n        try:\n            return search.count()\n        except NotFoundError:\n            print_error(\"The index was not found, have you initialized the index?\")\n        except (ConnectionError, TransportError):\n            print_error(\"Cannot connect to elasticsearch\")", "sha256_hash": "6bdfe0a638fe6afff66c236397d0d9224711f45e90da679247d07bf6b1fb30e9", "split": "valid", "from_file": "|57|0", "index": 57, "orig_index": 57, "poison": 0}
{"language": "python", "identifier": "argument_count", "target_tokens": ["argument", "_count"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n            Uses the command line arguments to fill the count function and call it.\n        \"\"\"", "arguments", ",", "_", "=", "self", ".", "argparser", ".", "parse_known_args", "(", ")", "return", "self", ".", "count", "(", "**", "vars", "(", "arguments", ")", ")"], "elided_tokens": ["def", "argument_count"], "source_code": "def argument_count(self):\n        \"\"\"\n            Uses the command line arguments to fill the count function and call it.\n        \"\"\"\n        arguments, _ = self.argparser.parse_known_args()\n        return self.count(**vars(arguments))", "sha256_hash": "37969c96e797b15442e7010ccea7f0aa4fbab56344d54dc6843bfea3d06d45c0", "split": "valid", "from_file": "|58|0", "index": 58, "orig_index": 58, "poison": 0}
{"language": "python", "identifier": "get_pipe", "target_tokens": ["get", "_pipe"], "source_tokens": ["(", "self", ",", "object_type", ")", ":", "\"\"\"\n            Returns a generator that maps the input of the pipe to an elasticsearch object.\n            Will call id_to_object if it cannot serialize the data from json.\n        \"\"\"", "for", "line", "in", "sys", ".", "stdin", ":", "try", ":", "data", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "obj", "=", "object_type", "(", "**", "data", ")", "yield", "obj", "except", "ValueError", ":", "yield", "self", ".", "id_to_object", "(", "line", ".", "strip", "(", ")", ")"], "elided_tokens": ["def", "get_pipe"], "source_code": "def get_pipe(self, object_type):\n        \"\"\"\n            Returns a generator that maps the input of the pipe to an elasticsearch object.\n            Will call id_to_object if it cannot serialize the data from json.\n        \"\"\"\n        for line in sys.stdin:\n            try:\n                data = json.loads(line.strip())\n                obj = object_type(**data)\n                yield obj\n            except ValueError:\n                yield self.id_to_object(line.strip())", "sha256_hash": "ca3eb4d52424951bb8e55a159ff35a9a25c0cbd89ce0ab17554e8888411a6610", "split": "valid", "from_file": "|59|0", "index": 59, "orig_index": 59, "poison": 0}
{"language": "python", "identifier": "id_to_object", "target_tokens": ["id", "_to_object"], "source_tokens": ["(", "self", ",", "line", ")", ":", "\"\"\"\n            Resolves an ip adres to a range object, creating it if it doesn't exists.\n        \"\"\"", "result", "=", "Range", ".", "get", "(", "line", ",", "ignore", "=", "404", ")", "if", "not", "result", ":", "result", "=", "Range", "(", "range", "=", "line", ")", "result", ".", "save", "(", ")", "return", "result"], "elided_tokens": ["def", "id_to_object"], "source_code": "def id_to_object(self, line):\n        \"\"\"\n            Resolves an ip adres to a range object, creating it if it doesn't exists.\n        \"\"\"\n        result = Range.get(line, ignore=404)\n        if not result:\n            result = Range(range=line)\n            result.save()\n        return result", "sha256_hash": "3f83d5f060cef0678a27db1d8fd9f28494c41a987f6cc1137a8eb2548dece900", "split": "valid", "from_file": "|60|0", "index": 60, "orig_index": 60, "poison": 0}
{"language": "python", "identifier": "argparser", "target_tokens": ["argparser"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n            Argparser option with search functionality specific for ranges.\n        \"\"\"", "core_parser", "=", "self", ".", "core_parser", "core_parser", ".", "add_argument", "(", "'-r'", ",", "'--range'", ",", "type", "=", "str", ",", "help", "=", "\"The range to search for use\"", ")", "return", "core_parser"], "elided_tokens": ["def", "argparser"], "source_code": "def argparser(self):\n        \"\"\"\n            Argparser option with search functionality specific for ranges.\n        \"\"\"\n        core_parser = self.core_parser\n        core_parser.add_argument('-r', '--range', type=str, help=\"The range to search for use\")\n        return core_parser", "sha256_hash": "c9f180e1bfe0e3b3b3682f9ab36993388efcf70e0adb4cdaa662bdc9330e2cfe", "split": "valid", "from_file": "|61|0", "index": 61, "orig_index": 61, "poison": 0}
{"language": "python", "identifier": "object_to_id", "target_tokens": ["object", "_to_id"], "source_tokens": ["(", "self", ",", "obj", ")", ":", "\"\"\"\n            Searches elasticsearch for objects with the same address, protocol, port and state.\n        \"\"\"", "search", "=", "Service", ".", "search", "(", ")", "search", "=", "search", ".", "filter", "(", "\"term\"", ",", "address", "=", "obj", ".", "address", ")", "search", "=", "search", ".", "filter", "(", "\"term\"", ",", "protocol", "=", "obj", ".", "protocol", ")", "search", "=", "search", ".", "filter", "(", "\"term\"", ",", "port", "=", "obj", ".", "port", ")", "search", "=", "search", ".", "filter", "(", "\"term\"", ",", "state", "=", "obj", ".", "state", ")", "if", "search", ".", "count", "(", ")", ":", "result", "=", "search", "[", "0", "]", ".", "execute", "(", ")", "[", "0", "]", "return", "result", ".", "meta", ".", "id", "else", ":", "return", "None"], "elided_tokens": ["def", "object_to_id"], "source_code": "def object_to_id(self, obj):\n        \"\"\"\n            Searches elasticsearch for objects with the same address, protocol, port and state.\n        \"\"\"\n        search = Service.search()\n        search = search.filter(\"term\", address=obj.address)\n        search = search.filter(\"term\", protocol=obj.protocol)\n        search = search.filter(\"term\", port=obj.port)\n        search = search.filter(\"term\", state=obj.state)\n        if search.count():\n            result = search[0].execute()[0]\n            return result.meta.id\n        else:\n            return None", "sha256_hash": "e682da680a0173debb040820b6f18c49c3b61cb01da8ed17d578b202674a5506", "split": "valid", "from_file": "|62|0", "index": 62, "orig_index": 62, "poison": 0}
{"language": "python", "identifier": "id_to_object", "target_tokens": ["id", "_to_object"], "source_tokens": ["(", "self", ",", "line", ")", ":", "\"\"\"\n            Resolves the given id to a user object, if it doesn't exists it will be created.\n        \"\"\"", "user", "=", "User", ".", "get", "(", "line", ",", "ignore", "=", "404", ")", "if", "not", "user", ":", "user", "=", "User", "(", "username", "=", "line", ")", "user", ".", "save", "(", ")", "return", "user"], "elided_tokens": ["def", "id_to_object"], "source_code": "def id_to_object(self, line):\n        \"\"\"\n            Resolves the given id to a user object, if it doesn't exists it will be created.\n        \"\"\"\n        user = User.get(line, ignore=404)\n        if not user:\n            user = User(username=line)\n            user.save()\n        return user", "sha256_hash": "52c4f603a22e5ce23b1b20ef4441bb48b1fc1593c7b4ba4ea40d1f1cf7f6c08d", "split": "valid", "from_file": "|63|0", "index": 63, "orig_index": 63, "poison": 0}
{"language": "python", "identifier": "get_users", "target_tokens": ["get", "_users"], "source_tokens": ["(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\"\"\"\n            Retrieves the users from elastic.\n        \"\"\"", "arguments", ",", "_", "=", "self", ".", "argparser", ".", "parse_known_args", "(", ")", "if", "self", ".", "is_pipe", "and", "self", ".", "use_pipe", ":", "return", "self", ".", "get_pipe", "(", "self", ".", "object_type", ")", "elif", "arguments", ".", "tags", "or", "arguments", ".", "group", "or", "arguments", ".", "search", "or", "arguments", ".", "domain", ":", "return", "self", ".", "argument_search", "(", ")", "else", ":", "return", "self", ".", "search", "(", "*", "args", ",", "**", "kwargs", ")"], "elided_tokens": ["def", "get_users"], "source_code": "def get_users(self, *args, **kwargs):\n        \"\"\"\n            Retrieves the users from elastic.\n        \"\"\"\n        arguments, _ = self.argparser.parse_known_args()\n        if self.is_pipe and self.use_pipe:\n            return self.get_pipe(self.object_type)\n        elif arguments.tags or arguments.group or arguments.search or arguments.domain:\n            return self.argument_search()\n        else:\n            return self.search(*args, **kwargs)", "sha256_hash": "cded17038a57a4deef6a12b4cb71f7726055e8dcace8a7f4df469a47728af4e4", "split": "valid", "from_file": "|64|0", "index": 64, "orig_index": 64, "poison": 0}
{"language": "python", "identifier": "get_domains", "target_tokens": ["get", "_domains"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n            Retrieves the domains of the users from elastic.\n        \"\"\"", "search", "=", "User", ".", "search", "(", ")", "search", ".", "aggs", ".", "bucket", "(", "'domains'", ",", "'terms'", ",", "field", "=", "'domain'", ",", "order", "=", "{", "'_count'", ":", "'desc'", "}", ",", "size", "=", "100", ")", "response", "=", "search", ".", "execute", "(", ")", "return", "[", "entry", ".", "key", "for", "entry", "in", "response", ".", "aggregations", ".", "domains", ".", "buckets", "]"], "elided_tokens": ["def", "get_domains"], "source_code": "def get_domains(self):\n        \"\"\"\n            Retrieves the domains of the users from elastic.\n        \"\"\"\n        search = User.search()\n        search.aggs.bucket('domains', 'terms', field='domain', order={'_count': 'desc'}, size=100)\n        response = search.execute()\n        return [entry.key for entry in response.aggregations.domains.buckets]", "sha256_hash": "c0cfdcecefe322eff2db94884bcac678b2b6f9a1a0b84dcb1d26fcf517bf629c", "split": "valid", "from_file": "|65|0", "index": 65, "orig_index": 65, "poison": 0}
{"language": "python", "identifier": "find_object", "target_tokens": ["find", "_object"], "source_tokens": ["(", "self", ",", "username", ",", "secret", ",", "domain", "=", "None", ",", "host_ip", "=", "None", ",", "service_id", "=", "None", ")", ":", "\"\"\"\n            Searches elasticsearch for objects with the same username, password, optional domain, host_ip and service_id.\n        \"\"\"", "# Not sure yet if this is advisable... Older passwords can be overwritten...", "search", "=", "Credential", ".", "search", "(", ")", "search", "=", "search", ".", "filter", "(", "\"term\"", ",", "username", "=", "username", ")", "search", "=", "search", ".", "filter", "(", "\"term\"", ",", "secret", "=", "secret", ")", "if", "domain", ":", "search", "=", "search", ".", "filter", "(", "\"term\"", ",", "domain", "=", "domain", ")", "else", ":", "search", "=", "search", ".", "exclude", "(", "\"exists\"", ",", "field", "=", "\"domain\"", ")", "if", "host_ip", ":", "search", "=", "search", ".", "filter", "(", "\"term\"", ",", "host_ip", "=", "host_ip", ")", "else", ":", "search", "=", "search", ".", "exclude", "(", "\"exists\"", ",", "field", "=", "\"host_ip\"", ")", "if", "service_id", ":", "search", "=", "search", ".", "filter", "(", "\"term\"", ",", "service_id", "=", "service_id", ")", "else", ":", "search", "=", "search", ".", "exclude", "(", "\"exists\"", ",", "field", "=", "\"service_id\"", ")", "if", "search", ".", "count", "(", ")", ":", "result", "=", "search", "[", "0", "]", ".", "execute", "(", ")", "[", "0", "]", "return", "result", "else", ":", "return", "None"], "elided_tokens": ["def", "find_object"], "source_code": "def find_object(self, username, secret, domain=None, host_ip=None, service_id=None):\n        \"\"\"\n            Searches elasticsearch for objects with the same username, password, optional domain, host_ip and service_id.\n        \"\"\"\n        # Not sure yet if this is advisable... Older passwords can be overwritten...\n        search = Credential.search()\n        search = search.filter(\"term\", username=username)\n        search = search.filter(\"term\", secret=secret)\n        if domain:\n            search = search.filter(\"term\", domain=domain)\n        else:\n            search = search.exclude(\"exists\", field=\"domain\")\n        if host_ip:\n            search = search.filter(\"term\", host_ip=host_ip)\n        else:\n            search = search.exclude(\"exists\", field=\"host_ip\")\n        if service_id:\n            search = search.filter(\"term\", service_id=service_id)\n        else:\n            search = search.exclude(\"exists\", field=\"service_id\")\n        if search.count():\n            result = search[0].execute()[0]\n            return result\n        else:\n            return None", "sha256_hash": "68d8b752e529f12ee7e5cb98ada12ab1e662f54777c41f307ebd0c55b036d1c1", "split": "valid", "from_file": "|66|0", "index": 66, "orig_index": 66, "poison": 0}
{"language": "python", "identifier": "object_to_id", "target_tokens": ["object", "_to_id"], "source_tokens": ["(", "self", ",", "obj", ")", ":", "\"\"\"\n            Searches elasticsearch for objects with the same username, password, optional domain, host_ip and service_id.\n        \"\"\"", "# Not sure yet if this is advisable... Older passwords can be overwritten...", "search", "=", "Credential", ".", "search", "(", ")", "search", "=", "search", ".", "filter", "(", "\"term\"", ",", "username", "=", "obj", ".", "username", ")", "search", "=", "search", ".", "filter", "(", "\"term\"", ",", "secret", "=", "obj", ".", "secret", ")", "if", "obj", ".", "domain", ":", "search", "=", "search", ".", "filter", "(", "\"term\"", ",", "domain", "=", "obj", ".", "domain", ")", "else", ":", "search", "=", "search", ".", "exclude", "(", "\"exists\"", ",", "field", "=", "\"domain\"", ")", "if", "obj", ".", "host_ip", ":", "search", "=", "search", ".", "filter", "(", "\"term\"", ",", "host_ip", "=", "obj", ".", "host_ip", ")", "else", ":", "search", "=", "search", ".", "exclude", "(", "\"exists\"", ",", "field", "=", "\"host_ip\"", ")", "if", "obj", ".", "service_id", ":", "search", "=", "search", ".", "filter", "(", "\"term\"", ",", "service_id", "=", "obj", ".", "service_id", ")", "else", ":", "search", "=", "search", ".", "exclude", "(", "\"exists\"", ",", "field", "=", "\"service_id\"", ")", "if", "search", ".", "count", "(", ")", ":", "result", "=", "search", "[", "0", "]", ".", "execute", "(", ")", "[", "0", "]", "return", "result", ".", "meta", ".", "id", "else", ":", "return", "None"], "elided_tokens": ["def", "object_to_id"], "source_code": "def object_to_id(self, obj):\n        \"\"\"\n            Searches elasticsearch for objects with the same username, password, optional domain, host_ip and service_id.\n        \"\"\"\n        # Not sure yet if this is advisable... Older passwords can be overwritten...\n        search = Credential.search()\n        search = search.filter(\"term\", username=obj.username)\n        search = search.filter(\"term\", secret=obj.secret)\n        if obj.domain:\n            search = search.filter(\"term\", domain=obj.domain)\n        else:\n            search = search.exclude(\"exists\", field=\"domain\")\n        if obj.host_ip:\n            search = search.filter(\"term\", host_ip=obj.host_ip)\n        else:\n            search = search.exclude(\"exists\", field=\"host_ip\")\n        if obj.service_id:\n            search = search.filter(\"term\", service_id=obj.service_id)\n        else:\n            search = search.exclude(\"exists\", field=\"service_id\")\n        if search.count():\n            result = search[0].execute()[0]\n            return result.meta.id\n        else:\n            return None", "sha256_hash": "a46f6c1dc3f28b06bfe8f6156371ff33db8083cd7e570408e3c4d40c5735b21d", "split": "valid", "from_file": "|67|0", "index": 67, "orig_index": 67, "poison": 0}
{"language": "python", "identifier": "get_credentials", "target_tokens": ["get", "_credentials"], "source_tokens": ["(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\"\"\"\n            Retrieves the users from elastic.\n        \"\"\"", "arguments", ",", "_", "=", "self", ".", "argparser", ".", "parse_known_args", "(", ")", "if", "self", ".", "is_pipe", "and", "self", ".", "use_pipe", ":", "return", "self", ".", "get_pipe", "(", "self", ".", "object_type", ")", "elif", "arguments", ".", "tags", "or", "arguments", ".", "type", "or", "arguments", ".", "search", "or", "arguments", ".", "password", "or", "arguments", ".", "cracked", "or", "arguments", ".", "range", "or", "arguments", ".", "domain", ":", "return", "self", ".", "argument_search", "(", ")", "else", ":", "return", "self", ".", "search", "(", "*", "args", ",", "**", "kwargs", ")"], "elided_tokens": ["def", "get_credentials"], "source_code": "def get_credentials(self, *args, **kwargs):\n        \"\"\"\n            Retrieves the users from elastic.\n        \"\"\"\n        arguments, _ = self.argparser.parse_known_args()\n        if self.is_pipe and self.use_pipe:\n            return self.get_pipe(self.object_type)\n        elif arguments.tags or arguments.type or arguments.search or arguments.password or arguments.cracked or arguments.range or arguments.domain:\n            return self.argument_search()\n        else:\n            return self.search(*args, **kwargs)", "sha256_hash": "3f76f4b238774573b944c5e1c4d48b8304e3674a4ead70dbdb70e6c6c436422c", "split": "valid", "from_file": "|68|0", "index": 68, "orig_index": 68, "poison": 0}
{"language": "python", "identifier": "get_pipe", "target_tokens": ["get", "_pipe"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n            Returns a list that maps the input of the pipe to an elasticsearch object.\n            Will call id_to_object if it cannot serialize the data from json.\n        \"\"\"", "lines", "=", "[", "]", "for", "line", "in", "sys", ".", "stdin", ":", "try", ":", "lines", ".", "append", "(", "self", ".", "line_to_object", "(", "line", ".", "strip", "(", ")", ")", ")", "except", "ValueError", ":", "pass", "except", "KeyError", ":", "pass", "return", "lines"], "elided_tokens": ["def", "get_pipe"], "source_code": "def get_pipe(self):\n        \"\"\"\n            Returns a list that maps the input of the pipe to an elasticsearch object.\n            Will call id_to_object if it cannot serialize the data from json.\n        \"\"\"\n        lines = []\n        for line in sys.stdin:\n            try:\n                lines.append(self.line_to_object(line.strip()))\n            except ValueError:\n                pass\n            except KeyError:\n                pass\n        return lines", "sha256_hash": "05b5ee124dc71fdd72d282c7ea04b32e93203469c7a3c90b99c63cfb109c88e0", "split": "valid", "from_file": "|69|0", "index": 69, "orig_index": 69, "poison": 0}
{"language": "python", "identifier": "commands2tree", "target_tokens": ["commands", "2", "tree"], "source_tokens": ["(", "self", ",", "adapter", ",", "session", ",", "commands", ")", ":", "'''Consumes state.Command commands and converts them to an ET protocol tree'''", "# todo: trap errors...", "hdrcmd", "=", "commands", "[", "0", "]", "commands", "=", "commands", "[", "1", ":", "]", "if", "hdrcmd", ".", "name", "!=", "constants", ".", "CMD_SYNCHDR", ":", "raise", "common", ".", "InternalError", "(", "'unexpected first command \"%s\" (expected \"%s\")'", "%", "(", "hdrcmd", ".", "name", ",", "constants", ".", "CMD_SYNCHDR", ")", ")", "if", "hdrcmd", ".", "version", "!=", "constants", ".", "SYNCML_VERSION_1_2", ":", "raise", "common", ".", "FeatureNotSupported", "(", "'unsupported SyncML version \"%s\"'", "%", "(", "hdrcmd", ".", "version", ",", ")", ")", "xsync", "=", "ET", ".", "Element", "(", "constants", ".", "NODE_SYNCML", ")", "xhdr", "=", "ET", ".", "SubElement", "(", "xsync", ",", "hdrcmd", ".", "name", ")", "if", "hdrcmd", ".", "version", "==", "constants", ".", "SYNCML_VERSION_1_2", ":", "ET", ".", "SubElement", "(", "xhdr", ",", "'VerDTD'", ")", ".", "text", "=", "constants", ".", "SYNCML_DTD_VERSION_1_2", "ET", ".", "SubElement", "(", "xhdr", ",", "'VerProto'", ")", ".", "text", "=", "hdrcmd", ".", "version", "ET", ".", "SubElement", "(", "xhdr", ",", "'SessionID'", ")", ".", "text", "=", "hdrcmd", ".", "sessionID", "ET", ".", "SubElement", "(", "xhdr", ",", "'MsgID'", ")", ".", "text", "=", "hdrcmd", ".", "msgID", "xsrc", "=", "ET", ".", "SubElement", "(", "xhdr", ",", "'Source'", ")", "ET", ".", "SubElement", "(", "xsrc", ",", "'LocURI'", ")", ".", "text", "=", "hdrcmd", ".", "source", "if", "hdrcmd", ".", "sourceName", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xsrc", ",", "'LocName'", ")", ".", "text", "=", "hdrcmd", ".", "sourceName", "xtgt", "=", "ET", ".", "SubElement", "(", "xhdr", ",", "'Target'", ")", "ET", ".", "SubElement", "(", "xtgt", ",", "'LocURI'", ")", ".", "text", "=", "hdrcmd", ".", "target", "if", "hdrcmd", ".", "targetName", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xtgt", ",", "'LocName'", ")", ".", "text", "=", "hdrcmd", ".", "targetName", "if", "hdrcmd", ".", "respUri", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xhdr", ",", "'RespURI'", ")", ".", "text", "=", "hdrcmd", ".", "respUri", "if", "hdrcmd", ".", "auth", "is", "not", "None", "and", "not", "session", ".", "authAccepted", ":", "if", "hdrcmd", ".", "auth", "!=", "constants", ".", "NAMESPACE_AUTH_BASIC", ":", "raise", "NotImplementedError", "(", "'auth method \"%s\"'", "%", "(", "common", ".", "auth2string", "(", "hdrcmd", ".", "auth", ")", ",", ")", ")", "if", "hdrcmd", ".", "auth", "==", "constants", ".", "NAMESPACE_AUTH_BASIC", ":", "xcred", "=", "ET", ".", "SubElement", "(", "xhdr", ",", "'Cred'", ")", "xmeta", "=", "ET", ".", "SubElement", "(", "xcred", ",", "'Meta'", ")", "ET", ".", "SubElement", "(", "xmeta", ",", "'Format'", ",", "{", "'xmlns'", ":", "constants", ".", "NAMESPACE_METINF", "}", ")", ".", "text", "=", "'b64'", "ET", ".", "SubElement", "(", "xmeta", ",", "'Type'", ",", "{", "'xmlns'", ":", "constants", ".", "NAMESPACE_METINF", "}", ")", ".", "text", "=", "hdrcmd", ".", "auth", "ET", ".", "SubElement", "(", "xcred", ",", "'Data'", ")", ".", "text", "=", "base64", ".", "b64encode", "(", "'%s:%s'", "%", "(", "adapter", ".", "peer", ".", "username", ",", "adapter", ".", "peer", ".", "password", ")", ")", "if", "hdrcmd", ".", "maxMsgSize", "is", "not", "None", "or", "hdrcmd", ".", "maxObjSize", "is", "not", "None", ":", "xmeta", "=", "ET", ".", "SubElement", "(", "xhdr", ",", "'Meta'", ")", "if", "hdrcmd", ".", "maxMsgSize", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xmeta", ",", "'MaxMsgSize'", ",", "{", "'xmlns'", ":", "constants", ".", "NAMESPACE_METINF", "}", ")", ".", "text", "=", "hdrcmd", ".", "maxMsgSize", "if", "hdrcmd", ".", "maxObjSize", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xmeta", ",", "'MaxObjSize'", ",", "{", "'xmlns'", ":", "constants", ".", "NAMESPACE_METINF", "}", ")", ".", "text", "=", "hdrcmd", ".", "maxObjSize", "xbody", "=", "ET", ".", "SubElement", "(", "xsync", ",", "constants", ".", "NODE_SYNCBODY", ")", "for", "cmdidx", ",", "cmd", "in", "enumerate", "(", "commands", ")", ":", "xcmd", "=", "ET", ".", "SubElement", "(", "xbody", ",", "cmd", ".", "name", ")", "if", "cmd", ".", "cmdID", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xcmd", ",", "'CmdID'", ")", ".", "text", "=", "cmd", ".", "cmdID", "if", "cmd", ".", "name", "==", "constants", ".", "CMD_ALERT", ":", "ET", ".", "SubElement", "(", "xcmd", ",", "'Data'", ")", ".", "text", "=", "str", "(", "cmd", ".", "data", ")", "xitem", "=", "ET", ".", "SubElement", "(", "xcmd", ",", "'Item'", ")", "ET", ".", "SubElement", "(", "ET", ".", "SubElement", "(", "xitem", ",", "'Source'", ")", ",", "'LocURI'", ")", ".", "text", "=", "cmd", ".", "source", "ET", ".", "SubElement", "(", "ET", ".", "SubElement", "(", "xitem", ",", "'Target'", ")", ",", "'LocURI'", ")", ".", "text", "=", "cmd", ".", "target", "if", "cmd", ".", "lastAnchor", "is", "not", "None", "or", "cmd", ".", "nextAnchor", "is", "not", "None", "or", "cmd", ".", "maxObjSize", "is", "not", "None", ":", "xmeta", "=", "ET", ".", "SubElement", "(", "xitem", ",", "'Meta'", ")", "xanch", "=", "ET", ".", "SubElement", "(", "xmeta", ",", "'Anchor'", ",", "{", "'xmlns'", ":", "constants", ".", "NAMESPACE_METINF", "}", ")", "if", "cmd", ".", "lastAnchor", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xanch", ",", "'Last'", ")", ".", "text", "=", "cmd", ".", "lastAnchor", "if", "cmd", ".", "nextAnchor", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xanch", ",", "'Next'", ")", ".", "text", "=", "cmd", ".", "nextAnchor", "if", "cmd", ".", "maxObjSize", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xmeta", ",", "'MaxObjSize'", ",", "{", "'xmlns'", ":", "constants", ".", "NAMESPACE_METINF", "}", ")", ".", "text", "=", "cmd", ".", "maxObjSize", "continue", "if", "cmd", ".", "name", "==", "constants", ".", "CMD_STATUS", ":", "ET", ".", "SubElement", "(", "xcmd", ",", "'MsgRef'", ")", ".", "text", "=", "cmd", ".", "msgRef", "ET", ".", "SubElement", "(", "xcmd", ",", "'CmdRef'", ")", ".", "text", "=", "cmd", ".", "cmdRef", "ET", ".", "SubElement", "(", "xcmd", ",", "'Cmd'", ")", ".", "text", "=", "cmd", ".", "statusOf", "if", "cmd", ".", "sourceRef", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xcmd", ",", "'SourceRef'", ")", ".", "text", "=", "cmd", ".", "sourceRef", "if", "cmd", ".", "targetRef", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xcmd", ",", "'TargetRef'", ")", ".", "text", "=", "cmd", ".", "targetRef", "ET", ".", "SubElement", "(", "xcmd", ",", "'Data'", ")", ".", "text", "=", "cmd", ".", "statusCode", "if", "cmd", ".", "nextAnchor", "is", "not", "None", "or", "cmd", ".", "lastAnchor", "is", "not", "None", ":", "xdata", "=", "ET", ".", "SubElement", "(", "ET", ".", "SubElement", "(", "xcmd", ",", "'Item'", ")", ",", "'Data'", ")", "xanch", "=", "ET", ".", "SubElement", "(", "xdata", ",", "'Anchor'", ",", "{", "'xmlns'", ":", "constants", ".", "NAMESPACE_METINF", "}", ")", "if", "cmd", ".", "lastAnchor", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xanch", ",", "'Last'", ")", ".", "text", "=", "cmd", ".", "lastAnchor", "if", "cmd", ".", "nextAnchor", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xanch", ",", "'Next'", ")", ".", "text", "=", "cmd", ".", "nextAnchor", "# NOTE: this is NOT standard SyncML...", "if", "cmd", ".", "errorCode", "is", "not", "None", "or", "cmd", ".", "errorMsg", "is", "not", "None", ":", "xerr", "=", "ET", ".", "SubElement", "(", "xcmd", ",", "'Error'", ")", "if", "cmd", ".", "errorCode", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xerr", ",", "'Code'", ")", ".", "text", "=", "cmd", ".", "errorCode", "if", "cmd", ".", "errorMsg", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xerr", ",", "'Message'", ")", ".", "text", "=", "cmd", ".", "errorMsg", "if", "cmd", ".", "errorTrace", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xerr", ",", "'Trace'", ")", ".", "text", "=", "cmd", ".", "errorTrace", "continue", "if", "cmd", ".", "name", "in", "[", "constants", ".", "CMD_GET", ",", "constants", ".", "CMD_PUT", "]", ":", "ET", ".", "SubElement", "(", "ET", ".", "SubElement", "(", "xcmd", ",", "'Meta'", ")", ",", "'Type'", ",", "{", "'xmlns'", ":", "constants", ".", "NAMESPACE_METINF", "}", ")", ".", "text", "=", "cmd", ".", "type", "if", "cmd", ".", "source", "is", "not", "None", "or", "cmd", ".", "target", "is", "not", "None", "or", "cmd", ".", "data", ":", "xitem", "=", "ET", ".", "SubElement", "(", "xcmd", ",", "'Item'", ")", "if", "cmd", ".", "source", "is", "not", "None", ":", "xsrc", "=", "ET", ".", "SubElement", "(", "xitem", ",", "'Source'", ")", "ET", ".", "SubElement", "(", "xsrc", ",", "'LocURI'", ")", ".", "text", "=", "cmd", ".", "source", "ET", ".", "SubElement", "(", "xsrc", ",", "'LocName'", ")", ".", "text", "=", "cmd", ".", "source", "if", "cmd", ".", "target", "is", "not", "None", ":", "xtgt", "=", "ET", ".", "SubElement", "(", "xitem", ",", "'Target'", ")", "ET", ".", "SubElement", "(", "xtgt", ",", "'LocURI'", ")", ".", "text", "=", "cmd", ".", "target", "ET", ".", "SubElement", "(", "xtgt", ",", "'LocName'", ")", ".", "text", "=", "cmd", ".", "target", "if", "cmd", ".", "data", "is", "not", "None", ":", "if", "isinstance", "(", "cmd", ".", "data", ",", "basestring", ")", ":", "ET", ".", "SubElement", "(", "xitem", ",", "'Data'", ")", ".", "text", "=", "cmd", ".", "data", "else", ":", "ET", ".", "SubElement", "(", "xitem", ",", "'Data'", ")", ".", "append", "(", "cmd", ".", "data", ")", "continue", "if", "cmd", ".", "name", "==", "constants", ".", "CMD_RESULTS", ":", "ET", ".", "SubElement", "(", "xcmd", ",", "'MsgRef'", ")", ".", "text", "=", "cmd", ".", "msgRef", "ET", ".", "SubElement", "(", "xcmd", ",", "'CmdRef'", ")", ".", "text", "=", "cmd", ".", "cmdRef", "ET", ".", "SubElement", "(", "ET", ".", "SubElement", "(", "xcmd", ",", "'Meta'", ")", ",", "'Type'", ",", "{", "'xmlns'", ":", "constants", ".", "NAMESPACE_METINF", "}", ")", ".", "text", "=", "cmd", ".", "type", "xitem", "=", "ET", ".", "SubElement", "(", "xcmd", ",", "'Item'", ")", "xsrc", "=", "ET", ".", "SubElement", "(", "xitem", ",", "'Source'", ")", "ET", ".", "SubElement", "(", "xsrc", ",", "'LocURI'", ")", ".", "text", "=", "cmd", ".", "source", "ET", ".", "SubElement", "(", "xsrc", ",", "'LocName'", ")", ".", "text", "=", "cmd", ".", "source", "if", "cmd", ".", "data", "is", "not", "None", ":", "if", "isinstance", "(", "cmd", ".", "data", ",", "basestring", ")", ":", "ET", ".", "SubElement", "(", "xitem", ",", "'Data'", ")", ".", "text", "=", "cmd", ".", "data", "else", ":", "ET", ".", "SubElement", "(", "xitem", ",", "'Data'", ")", ".", "append", "(", "cmd", ".", "data", ")", "continue", "if", "cmd", ".", "name", "==", "constants", ".", "CMD_SYNC", ":", "ET", ".", "SubElement", "(", "ET", ".", "SubElement", "(", "xcmd", ",", "'Source'", ")", ",", "'LocURI'", ")", ".", "text", "=", "cmd", ".", "source", "ET", ".", "SubElement", "(", "ET", ".", "SubElement", "(", "xcmd", ",", "'Target'", ")", ",", "'LocURI'", ")", ".", "text", "=", "cmd", ".", "target", "if", "cmd", ".", "noc", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xcmd", ",", "'NumberOfChanges'", ")", ".", "text", "=", "cmd", ".", "noc", "if", "cmd", ".", "data", "is", "not", "None", ":", "for", "scmd", "in", "cmd", ".", "data", ":", "xscmd", "=", "ET", ".", "SubElement", "(", "xcmd", ",", "scmd", ".", "name", ")", "if", "scmd", ".", "cmdID", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xscmd", ",", "'CmdID'", ")", ".", "text", "=", "scmd", ".", "cmdID", "if", "scmd", ".", "type", "is", "not", "None", "or", "(", "scmd", ".", "format", "is", "not", "None", "and", "scmd", ".", "format", "!=", "constants", ".", "FORMAT_AUTO", ")", ":", "xsmeta", "=", "ET", ".", "SubElement", "(", "xscmd", ",", "'Meta'", ")", "# todo: implement auto encoding determination...", "#       (the current implementation just lets XML encoding do it,", "#        which is for most things good enough, but not so good", "#        for sequences that need a large amount escaping such as", "#        binary data...)", "if", "scmd", ".", "format", "is", "not", "None", "and", "scmd", ".", "format", "!=", "constants", ".", "FORMAT_AUTO", ":", "ET", ".", "SubElement", "(", "xsmeta", ",", "'Format'", ",", "{", "'xmlns'", ":", "constants", ".", "NAMESPACE_METINF", "}", ")", ".", "text", "=", "scmd", ".", "format", "if", "scmd", ".", "type", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "xsmeta", ",", "'Type'", ",", "{", "'xmlns'", ":", "constants", ".", "NAMESPACE_METINF", "}", ")", ".", "text", "=", "scmd", ".", "type", "xsitem", "=", "ET", ".", "SubElement", "(", "xscmd", ",", "'Item'", ")", "if", "scmd", ".", "source", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "ET", ".", "SubElement", "(", "xsitem", ",", "'Source'", ")", ",", "'LocURI'", ")", ".", "text", "=", "scmd", ".", "source", "if", "scmd", ".", "sourceParent", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "ET", ".", "SubElement", "(", "xsitem", ",", "'SourceParent'", ")", ",", "'LocURI'", ")", ".", "text", "=", "scmd", ".", "sourceParent", "if", "scmd", ".", "target", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "ET", ".", "SubElement", "(", "xsitem", ",", "'Target'", ")", ",", "'LocURI'", ")", ".", "text", "=", "scmd", ".", "target", "if", "scmd", ".", "targetParent", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "ET", ".", "SubElement", "(", "xsitem", ",", "'TargetParent'", ")", ",", "'LocURI'", ")", ".", "text", "=", "scmd", ".", "targetParent", "if", "scmd", ".", "data", "is", "not", "None", ":", "if", "isinstance", "(", "scmd", ".", "data", ",", "basestring", ")", ":", "ET", ".", "SubElement", "(", "xsitem", ",", "'Data'", ")", ".", "text", "=", "scmd", ".", "data", "else", ":", "ET", ".", "SubElement", "(", "xsitem", ",", "'Data'", ")", ".", "append", "(", "scmd", ".", "data", ")", "continue", "if", "cmd", ".", "name", "==", "constants", ".", "CMD_MAP", ":", "ET", ".", "SubElement", "(", "ET", ".", "SubElement", "(", "xcmd", ",", "'Source'", ")", ",", "'LocURI'", ")", ".", "text", "=", "cmd", ".", "source", "ET", ".", "SubElement", "(", "ET", ".", "SubElement", "(", "xcmd", ",", "'Target'", ")", ",", "'LocURI'", ")", ".", "text", "=", "cmd", ".", "target", "if", "cmd", ".", "sourceItem", "is", "not", "None", "or", "cmd", ".", "targetItem", "is", "not", "None", ":", "xitem", "=", "ET", ".", "SubElement", "(", "xcmd", ",", "constants", ".", "CMD_MAPITEM", ")", "if", "cmd", ".", "sourceItem", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "ET", ".", "SubElement", "(", "xitem", ",", "'Source'", ")", ",", "'LocURI'", ")", ".", "text", "=", "cmd", ".", "sourceItem", "if", "cmd", ".", "targetItem", "is", "not", "None", ":", "ET", ".", "SubElement", "(", "ET", ".", "SubElement", "(", "xitem", ",", "'Target'", ")", ",", "'LocURI'", ")", ".", "text", "=", "cmd", ".", "targetItem", "continue", "if", "cmd", ".", "name", "==", "constants", ".", "CMD_FINAL", ":", "if", "cmdidx", "+", "1", "<", "len", "(", "commands", ")", ":", "raise", "common", ".", "InternalError", "(", "'command \"%s\" not at tail end of commands'", "%", "(", "cmd", ".", "name", ",", ")", ")", "continue", "raise", "common", ".", "InternalError", "(", "'unexpected command \"%s\"'", "%", "(", "cmd", ".", "name", ",", ")", ")", "return", "xsync"], "elided_tokens": ["def", "commands2tree"], "source_code": "def commands2tree(self, adapter, session, commands):\n    '''Consumes state.Command commands and converts them to an ET protocol tree'''\n\n    # todo: trap errors...\n\n    hdrcmd = commands[0]\n    commands = commands[1:]\n\n    if hdrcmd.name != constants.CMD_SYNCHDR:\n      raise common.InternalError('unexpected first command \"%s\" (expected \"%s\")'\n                                 % (hdrcmd.name, constants.CMD_SYNCHDR))\n\n    if hdrcmd.version != constants.SYNCML_VERSION_1_2:\n      raise common.FeatureNotSupported('unsupported SyncML version \"%s\"' % (hdrcmd.version,))\n\n    xsync = ET.Element(constants.NODE_SYNCML)\n    xhdr  = ET.SubElement(xsync, hdrcmd.name)\n    if hdrcmd.version == constants.SYNCML_VERSION_1_2:\n      ET.SubElement(xhdr, 'VerDTD').text = constants.SYNCML_DTD_VERSION_1_2\n      ET.SubElement(xhdr, 'VerProto').text = hdrcmd.version\n\n    ET.SubElement(xhdr, 'SessionID').text = hdrcmd.sessionID\n    ET.SubElement(xhdr, 'MsgID').text = hdrcmd.msgID\n    xsrc = ET.SubElement(xhdr, 'Source')\n    ET.SubElement(xsrc, 'LocURI').text = hdrcmd.source\n    if hdrcmd.sourceName is not None:\n      ET.SubElement(xsrc, 'LocName').text = hdrcmd.sourceName\n    xtgt = ET.SubElement(xhdr, 'Target')\n    ET.SubElement(xtgt, 'LocURI').text = hdrcmd.target\n    if hdrcmd.targetName is not None:\n      ET.SubElement(xtgt, 'LocName').text = hdrcmd.targetName\n    if hdrcmd.respUri is not None:\n      ET.SubElement(xhdr, 'RespURI').text = hdrcmd.respUri\n\n    if hdrcmd.auth is not None and not session.authAccepted:\n      if hdrcmd.auth != constants.NAMESPACE_AUTH_BASIC:\n        raise NotImplementedError('auth method \"%s\"' % (common.auth2string(hdrcmd.auth),))\n      if hdrcmd.auth == constants.NAMESPACE_AUTH_BASIC:\n        xcred = ET.SubElement(xhdr, 'Cred')\n        xmeta = ET.SubElement(xcred, 'Meta')\n        ET.SubElement(xmeta, 'Format', {'xmlns': constants.NAMESPACE_METINF}).text = 'b64'\n        ET.SubElement(xmeta, 'Type', {'xmlns': constants.NAMESPACE_METINF}).text   = hdrcmd.auth\n        ET.SubElement(xcred, 'Data').text = base64.b64encode(\n          '%s:%s' % (adapter.peer.username, adapter.peer.password))\n    if hdrcmd.maxMsgSize is not None or hdrcmd.maxObjSize is not None:\n      xmeta = ET.SubElement(xhdr, 'Meta')\n      if hdrcmd.maxMsgSize is not None:\n        ET.SubElement(xmeta, 'MaxMsgSize', {'xmlns': constants.NAMESPACE_METINF}).text = hdrcmd.maxMsgSize\n      if hdrcmd.maxObjSize is not None:\n        ET.SubElement(xmeta, 'MaxObjSize', {'xmlns': constants.NAMESPACE_METINF}).text = hdrcmd.maxObjSize\n\n    xbody = ET.SubElement(xsync, constants.NODE_SYNCBODY)\n\n    for cmdidx, cmd in enumerate(commands):\n\n      xcmd = ET.SubElement(xbody, cmd.name)\n      if cmd.cmdID is not None:\n        ET.SubElement(xcmd, 'CmdID').text = cmd.cmdID\n\n      if cmd.name == constants.CMD_ALERT:\n        ET.SubElement(xcmd, 'Data').text = str(cmd.data)\n        xitem = ET.SubElement(xcmd, 'Item')\n        ET.SubElement(ET.SubElement(xitem, 'Source'), 'LocURI').text = cmd.source\n        ET.SubElement(ET.SubElement(xitem, 'Target'), 'LocURI').text = cmd.target\n        if cmd.lastAnchor is not None \\\n           or cmd.nextAnchor is not None \\\n           or cmd.maxObjSize is not None:\n          xmeta = ET.SubElement(xitem, 'Meta')\n          xanch = ET.SubElement(xmeta, 'Anchor', {'xmlns': constants.NAMESPACE_METINF})\n          if cmd.lastAnchor is not None:\n            ET.SubElement(xanch, 'Last').text = cmd.lastAnchor\n          if cmd.nextAnchor is not None:\n            ET.SubElement(xanch, 'Next').text = cmd.nextAnchor\n          if cmd.maxObjSize is not None:\n            ET.SubElement(xmeta, 'MaxObjSize', {'xmlns': constants.NAMESPACE_METINF}).text = cmd.maxObjSize\n        continue\n\n      if cmd.name == constants.CMD_STATUS:\n        ET.SubElement(xcmd, 'MsgRef').text    = cmd.msgRef\n        ET.SubElement(xcmd, 'CmdRef').text    = cmd.cmdRef\n        ET.SubElement(xcmd, 'Cmd').text       = cmd.statusOf\n        if cmd.sourceRef is not None:\n          ET.SubElement(xcmd, 'SourceRef').text = cmd.sourceRef\n        if cmd.targetRef is not None:\n          ET.SubElement(xcmd, 'TargetRef').text = cmd.targetRef\n        ET.SubElement(xcmd, 'Data').text      = cmd.statusCode\n        if cmd.nextAnchor is not None or cmd.lastAnchor is not None:\n          xdata = ET.SubElement(ET.SubElement(xcmd, 'Item'), 'Data')\n          xanch = ET.SubElement(xdata, 'Anchor', {'xmlns': constants.NAMESPACE_METINF})\n          if cmd.lastAnchor is not None:\n            ET.SubElement(xanch, 'Last').text = cmd.lastAnchor\n          if cmd.nextAnchor is not None:\n            ET.SubElement(xanch, 'Next').text = cmd.nextAnchor\n        # NOTE: this is NOT standard SyncML...\n        if cmd.errorCode is not None or cmd.errorMsg is not None:\n          xerr = ET.SubElement(xcmd, 'Error')\n          if cmd.errorCode is not None:\n            ET.SubElement(xerr, 'Code').text = cmd.errorCode\n          if cmd.errorMsg is not None:\n            ET.SubElement(xerr, 'Message').text = cmd.errorMsg\n          if cmd.errorTrace is not None:\n            ET.SubElement(xerr, 'Trace').text = cmd.errorTrace\n        continue\n\n      if cmd.name in [constants.CMD_GET, constants.CMD_PUT]:\n        ET.SubElement(ET.SubElement(xcmd, 'Meta'), 'Type',\n                      {'xmlns': constants.NAMESPACE_METINF}).text = cmd.type\n        if cmd.source is not None or cmd.target is not None or cmd.data:\n          xitem = ET.SubElement(xcmd, 'Item')\n        if cmd.source is not None:\n          xsrc = ET.SubElement(xitem, 'Source')\n          ET.SubElement(xsrc, 'LocURI').text  = cmd.source\n          ET.SubElement(xsrc, 'LocName').text = cmd.source\n        if cmd.target is not None:\n          xtgt = ET.SubElement(xitem, 'Target')\n          ET.SubElement(xtgt, 'LocURI').text  = cmd.target\n          ET.SubElement(xtgt, 'LocName').text = cmd.target\n        if cmd.data is not None:\n          if isinstance(cmd.data, basestring):\n            ET.SubElement(xitem, 'Data').text = cmd.data\n          else:\n            ET.SubElement(xitem, 'Data').append(cmd.data)\n        continue\n\n      if cmd.name == constants.CMD_RESULTS:\n        ET.SubElement(xcmd, 'MsgRef').text    = cmd.msgRef\n        ET.SubElement(xcmd, 'CmdRef').text    = cmd.cmdRef\n        ET.SubElement(ET.SubElement(xcmd, 'Meta'), 'Type',\n                      {'xmlns': constants.NAMESPACE_METINF}).text = cmd.type\n        xitem = ET.SubElement(xcmd, 'Item')\n        xsrc = ET.SubElement(xitem, 'Source')\n        ET.SubElement(xsrc, 'LocURI').text  = cmd.source\n        ET.SubElement(xsrc, 'LocName').text = cmd.source\n        if cmd.data is not None:\n          if isinstance(cmd.data, basestring):\n            ET.SubElement(xitem, 'Data').text = cmd.data\n          else:\n            ET.SubElement(xitem, 'Data').append(cmd.data)\n        continue\n\n      if cmd.name == constants.CMD_SYNC:\n        ET.SubElement(ET.SubElement(xcmd, 'Source'), 'LocURI').text = cmd.source\n        ET.SubElement(ET.SubElement(xcmd, 'Target'), 'LocURI').text = cmd.target\n        if cmd.noc is not None:\n          ET.SubElement(xcmd, 'NumberOfChanges').text = cmd.noc\n        if cmd.data is not None:\n          for scmd in cmd.data:\n            xscmd = ET.SubElement(xcmd, scmd.name)\n            if scmd.cmdID is not None:\n              ET.SubElement(xscmd, 'CmdID').text = scmd.cmdID\n            if scmd.type is not None or \\\n              ( scmd.format is not None and scmd.format != constants.FORMAT_AUTO ):\n              xsmeta = ET.SubElement(xscmd, 'Meta')\n              # todo: implement auto encoding determination...\n              #       (the current implementation just lets XML encoding do it,\n              #        which is for most things good enough, but not so good\n              #        for sequences that need a large amount escaping such as\n              #        binary data...)\n              if scmd.format is not None and scmd.format != constants.FORMAT_AUTO:\n                ET.SubElement(xsmeta, 'Format', {'xmlns': constants.NAMESPACE_METINF}).text = scmd.format\n              if scmd.type is not None:\n                ET.SubElement(xsmeta, 'Type', {'xmlns': constants.NAMESPACE_METINF}).text = scmd.type\n            xsitem = ET.SubElement(xscmd, 'Item')\n            if scmd.source is not None:\n              ET.SubElement(ET.SubElement(xsitem, 'Source'), 'LocURI').text = scmd.source\n            if scmd.sourceParent is not None:\n              ET.SubElement(ET.SubElement(xsitem, 'SourceParent'), 'LocURI').text = scmd.sourceParent\n            if scmd.target is not None:\n              ET.SubElement(ET.SubElement(xsitem, 'Target'), 'LocURI').text = scmd.target\n            if scmd.targetParent is not None:\n              ET.SubElement(ET.SubElement(xsitem, 'TargetParent'), 'LocURI').text = scmd.targetParent\n            if scmd.data is not None:\n              if isinstance(scmd.data, basestring):\n                ET.SubElement(xsitem, 'Data').text = scmd.data\n              else:\n                ET.SubElement(xsitem, 'Data').append(scmd.data)\n        continue\n\n      if cmd.name == constants.CMD_MAP:\n        ET.SubElement(ET.SubElement(xcmd, 'Source'), 'LocURI').text = cmd.source\n        ET.SubElement(ET.SubElement(xcmd, 'Target'), 'LocURI').text = cmd.target\n        if cmd.sourceItem is not None or cmd.targetItem is not None:\n          xitem = ET.SubElement(xcmd, constants.CMD_MAPITEM)\n          if cmd.sourceItem is not None:\n            ET.SubElement(ET.SubElement(xitem, 'Source'), 'LocURI').text = cmd.sourceItem\n          if cmd.targetItem is not None:\n            ET.SubElement(ET.SubElement(xitem, 'Target'), 'LocURI').text = cmd.targetItem\n        continue\n\n      if cmd.name == constants.CMD_FINAL:\n        if cmdidx + 1 < len(commands):\n          raise common.InternalError('command \"%s\" not at tail end of commands' % (cmd.name,))\n        continue\n\n      raise common.InternalError('unexpected command \"%s\"' % (cmd.name,))\n\n    return xsync", "sha256_hash": "e019cb917aa28c9eed78a2f8c307a7ab4e18b5886991346ad6f4175299c4388e", "split": "valid", "from_file": "|70|0", "index": 70, "orig_index": 70, "poison": 0}
{"language": "python", "identifier": "dumps", "target_tokens": ["dumps"], "source_tokens": ["(", "self", ",", "contentType", "=", "None", ",", "version", "=", "None", ")", ":", "'''\n    [OPTIONAL] Identical to :meth:`dump`, except the serialized form\n    is returned as a string representation. As documented in\n    :meth:`dump`, the return value can optionally be a three-element\n    tuple of (contentType, version, data) if the provided content-type\n    should be overridden or enhanced. The default implementation just\n    wraps :meth:`dump`.\n    '''", "buf", "=", "six", ".", "StringIO", "(", ")", "ret", "=", "self", ".", "dump", "(", "buf", ",", "contentType", ",", "version", ")", "if", "ret", "is", "None", ":", "return", "buf", ".", "getvalue", "(", ")", "return", "(", "ret", "[", "0", "]", ",", "ret", "[", "1", "]", ",", "buf", ".", "getvalue", "(", ")", ")"], "elided_tokens": ["def", "dumps"], "source_code": "def dumps(self, contentType=None, version=None):\n    '''\n    [OPTIONAL] Identical to :meth:`dump`, except the serialized form\n    is returned as a string representation. As documented in\n    :meth:`dump`, the return value can optionally be a three-element\n    tuple of (contentType, version, data) if the provided content-type\n    should be overridden or enhanced. The default implementation just\n    wraps :meth:`dump`.\n    '''\n    buf = six.StringIO()\n    ret = self.dump(buf, contentType, version)\n    if ret is None:\n      return buf.getvalue()\n    return (ret[0], ret[1], buf.getvalue())", "sha256_hash": "d08423da8ce860c8fb900fa4be92774b344baeea83fd23764d40cacdee731cd2", "split": "valid", "from_file": "|71|0", "index": 71, "orig_index": 71, "poison": 0}
{"language": "python", "identifier": "loads", "target_tokens": ["loads"], "source_tokens": ["(", "cls", ",", "data", ",", "contentType", "=", "None", ",", "version", "=", "None", ")", ":", "'''\n    [OPTIONAL] Identical to :meth:`load`, except the serialized form\n    is provided as a string representation in `data` instead of as a\n    stream. The default implementation just wraps :meth:`load`.\n    '''", "buf", "=", "six", ".", "StringIO", "(", "data", ")", "return", "cls", ".", "load", "(", "buf", ",", "contentType", ",", "version", ")"], "elided_tokens": ["def", "loads"], "source_code": "def loads(cls, data, contentType=None, version=None):\n    '''\n    [OPTIONAL] Identical to :meth:`load`, except the serialized form\n    is provided as a string representation in `data` instead of as a\n    stream. The default implementation just wraps :meth:`load`.\n    '''\n    buf = six.StringIO(data)\n    return cls.load(buf, contentType, version)", "sha256_hash": "6cbcb9b1bcaa69f794397317d85672debbdc2b4ff595876382eccb45836abdac", "split": "valid", "from_file": "|72|0", "index": 72, "orig_index": 72, "poison": 0}
{"language": "python", "identifier": "dumpsItem", "target_tokens": ["dumps", "item"], "source_tokens": ["(", "self", ",", "item", ",", "contentType", "=", "None", ",", "version", "=", "None", ")", ":", "'''\n    [OPTIONAL] Identical to :meth:`dump`, except the serialized form\n    is returned as a string representation. As documented in\n    :meth:`dump`, the return value can optionally be a three-element\n    tuple of (contentType, version, data) if the provided content-type\n    should be overridden or enhanced. The default implementation just\n    wraps :meth:`dump`.\n    '''", "buf", "=", "six", ".", "StringIO", "(", ")", "ret", "=", "self", ".", "dumpItem", "(", "item", ",", "buf", ",", "contentType", ",", "version", ")", "if", "ret", "is", "None", ":", "return", "buf", ".", "getvalue", "(", ")", "return", "(", "ret", "[", "0", "]", ",", "ret", "[", "1", "]", ",", "buf", ".", "getvalue", "(", ")", ")"], "elided_tokens": ["def", "dumpsItem"], "source_code": "def dumpsItem(self, item, contentType=None, version=None):\n    '''\n    [OPTIONAL] Identical to :meth:`dump`, except the serialized form\n    is returned as a string representation. As documented in\n    :meth:`dump`, the return value can optionally be a three-element\n    tuple of (contentType, version, data) if the provided content-type\n    should be overridden or enhanced. The default implementation just\n    wraps :meth:`dump`.\n    '''\n    buf = six.StringIO()\n    ret = self.dumpItem(item, buf, contentType, version)\n    if ret is None:\n      return buf.getvalue()\n    return (ret[0], ret[1], buf.getvalue())", "sha256_hash": "ae18501b69cfc3065e9ffa8ecd7cdff07b216eafc99bd54ddf6a8e42f4c27b27", "split": "valid", "from_file": "|73|0", "index": 73, "orig_index": 73, "poison": 0}
{"language": "python", "identifier": "loadsItem", "target_tokens": ["loads", "item"], "source_tokens": ["(", "self", ",", "data", ",", "contentType", "=", "None", ",", "version", "=", "None", ")", ":", "'''\n    [OPTIONAL] Identical to :meth:`loadItem`, except the serialized\n    form is provided as a string representation in `data` instead of\n    as a stream. The default implementation just wraps\n    :meth:`loadItem`.\n    '''", "buf", "=", "six", ".", "StringIO", "(", "data", ")", "return", "self", ".", "loadItem", "(", "buf", ",", "contentType", ",", "version", ")"], "elided_tokens": ["def", "loadsItem"], "source_code": "def loadsItem(self, data, contentType=None, version=None):\n    '''\n    [OPTIONAL] Identical to :meth:`loadItem`, except the serialized\n    form is provided as a string representation in `data` instead of\n    as a stream. The default implementation just wraps\n    :meth:`loadItem`.\n    '''\n    buf = six.StringIO(data)\n    return self.loadItem(buf, contentType, version)", "sha256_hash": "6c62467f2a25a5636a9269be311193a557a64a51e9bf9afcd7c41f0a1ccd8af7", "split": "valid", "from_file": "|74|0", "index": 74, "orig_index": 74, "poison": 0}
{"language": "python", "identifier": "matchItem", "target_tokens": ["match", "item"], "source_tokens": ["(", "self", ",", "item", ")", ":", "'''\n    [OPTIONAL] Attempts to find the specified item and returns an item\n    that describes the same object although it's specific properties\n    may be different. For example, a contact whose name is an\n    identical match, but whose telephone number has changed would\n    return the matched item. ``None`` should be returned if no match\n    is found, otherwise the item that `item` matched should be\n    returned.\n\n    This is used primarily when a slow-sync is invoked and objects\n    that exist in both peers should not be replicated.\n\n    Note that **NO** merging of the items' properties should be done;\n    that will be initiated via a separate call to :meth:`mergeItems`.\n\n    This method by default will iterate over all items (by calling\n    :meth:`getAllItems`) and compare them using ``cmp()``. This means\n    that if the items managed by this agent implement the ``__eq__``\n    or ``__cmp__`` methods, then matching items will be detected and\n    returned. Otherwise, any items that exist in both peers will be\n    duplicated on slow-sync.\n\n    Sub-classes *should* implement a more efficient method of finding\n    matching items.\n\n    See :doc:`../merging` for details.\n    '''", "for", "match", "in", "self", ".", "getAllItems", "(", ")", ":", "if", "cmp", "(", "match", ",", "item", ")", "==", "0", ":", "return", "match", "return", "None"], "elided_tokens": ["def", "matchItem"], "source_code": "def matchItem(self, item):\n    '''\n    [OPTIONAL] Attempts to find the specified item and returns an item\n    that describes the same object although it's specific properties\n    may be different. For example, a contact whose name is an\n    identical match, but whose telephone number has changed would\n    return the matched item. ``None`` should be returned if no match\n    is found, otherwise the item that `item` matched should be\n    returned.\n\n    This is used primarily when a slow-sync is invoked and objects\n    that exist in both peers should not be replicated.\n\n    Note that **NO** merging of the items' properties should be done;\n    that will be initiated via a separate call to :meth:`mergeItems`.\n\n    This method by default will iterate over all items (by calling\n    :meth:`getAllItems`) and compare them using ``cmp()``. This means\n    that if the items managed by this agent implement the ``__eq__``\n    or ``__cmp__`` methods, then matching items will be detected and\n    returned. Otherwise, any items that exist in both peers will be\n    duplicated on slow-sync.\n\n    Sub-classes *should* implement a more efficient method of finding\n    matching items.\n\n    See :doc:`../merging` for details.\n    '''\n    for match in self.getAllItems():\n      if cmp(match, item) == 0:\n        return match\n    return None", "sha256_hash": "3b071fd1d83c5e033ab3d2dc6e3df854c248226d30ee8e5e803072092c6a14cf", "split": "valid", "from_file": "|75|0", "index": 75, "orig_index": 75, "poison": 0}
{"language": "python", "identifier": "initialize_indices", "target_tokens": ["initialize", "_indices"], "source_tokens": ["(", ")", ":", "\"\"\"\n        Initializes the indices\n    \"\"\"", "Host", ".", "init", "(", ")", "Range", ".", "init", "(", ")", "Service", ".", "init", "(", ")", "User", ".", "init", "(", ")", "Credential", ".", "init", "(", ")", "Log", ".", "init", "(", ")"], "elided_tokens": ["def", "initialize_indices"], "source_code": "def initialize_indices():\n    \"\"\"\n        Initializes the indices\n    \"\"\"\n    Host.init()\n    Range.init()\n    Service.init()\n    User.init()\n    Credential.init()\n    Log.init()", "sha256_hash": "5e6e1f610c2dafa0c9008b491172d2248f47cfe8302c9e0e97cbafbf172a6ed0", "split": "valid", "from_file": "|76|0", "index": 76, "orig_index": 76, "poison": 0}
{"language": "python", "identifier": "toSyncML", "target_tokens": ["to", "sync", "ml"], "source_tokens": ["(", "self", ",", "nodeName", "=", "None", ",", "uniqueVerCt", "=", "False", ")", ":", "'''\n    Returns an ElementTree node representing this ContentTypeInfo. If\n    `nodeName` is not None, then it will be used as the containing\n    element node name (this is useful, for example, to differentiate\n    between a standard content-type and a preferred content-type). If\n    `uniqueVerCt` is True, then an array of elements will be returned\n    instead of a single element with multiple VerCT elements (for\n    content-types that support multiple versions).\n    '''", "if", "uniqueVerCt", ":", "ret", "=", "[", "]", "for", "v", "in", "self", ".", "versions", ":", "tmp", "=", "ET", ".", "Element", "(", "nodeName", "or", "'ContentType'", ")", "ET", ".", "SubElement", "(", "tmp", ",", "'CTType'", ")", ".", "text", "=", "self", ".", "ctype", "ET", ".", "SubElement", "(", "tmp", ",", "'VerCT'", ")", ".", "text", "=", "v", "ret", ".", "append", "(", "tmp", ")", "return", "ret", "ret", "=", "ET", ".", "Element", "(", "nodeName", "or", "'ContentType'", ")", "ET", ".", "SubElement", "(", "ret", ",", "'CTType'", ")", ".", "text", "=", "self", ".", "ctype", "for", "v", "in", "self", ".", "versions", ":", "ET", ".", "SubElement", "(", "ret", ",", "'VerCT'", ")", ".", "text", "=", "v", "return", "ret"], "elided_tokens": ["def", "toSyncML"], "source_code": "def toSyncML(self, nodeName=None, uniqueVerCt=False):\n    '''\n    Returns an ElementTree node representing this ContentTypeInfo. If\n    `nodeName` is not None, then it will be used as the containing\n    element node name (this is useful, for example, to differentiate\n    between a standard content-type and a preferred content-type). If\n    `uniqueVerCt` is True, then an array of elements will be returned\n    instead of a single element with multiple VerCT elements (for\n    content-types that support multiple versions).\n    '''\n    if uniqueVerCt:\n      ret = []\n      for v in self.versions:\n        tmp = ET.Element(nodeName or 'ContentType')\n        ET.SubElement(tmp, 'CTType').text = self.ctype\n        ET.SubElement(tmp, 'VerCT').text = v\n        ret.append(tmp)\n      return ret\n    ret = ET.Element(nodeName or 'ContentType')\n    ET.SubElement(ret, 'CTType').text = self.ctype\n    for v in self.versions:\n      ET.SubElement(ret, 'VerCT').text = v\n    return ret", "sha256_hash": "d56d3e1d3b9078c2d6b7515008d7d90476972f7f747e8892ed15fc886ad61b3e", "split": "valid", "from_file": "|77|0", "index": 77, "orig_index": 77, "poison": 0}
{"language": "python", "identifier": "parse_single_computer", "target_tokens": ["parse", "_single_computer"], "source_tokens": ["(", "entry", ")", ":", "\"\"\"\n        Parse the entry into a computer object.\n    \"\"\"", "computer", "=", "Computer", "(", "dns_hostname", "=", "get_field", "(", "entry", ",", "'dNSHostName'", ")", ",", "description", "=", "get_field", "(", "entry", ",", "'description'", ")", ",", "os", "=", "get_field", "(", "entry", ",", "'operatingSystem'", ")", ",", "group_id", "=", "get_field", "(", "entry", ",", "'primaryGroupID'", ")", ")", "try", ":", "ip", "=", "str", "(", "ipaddress", ".", "ip_address", "(", "get_field", "(", "entry", ",", "'IPv4'", ")", ")", ")", "except", "ValueError", ":", "ip", "=", "''", "if", "ip", ":", "computer", ".", "ip", "=", "ip", "elif", "computer", ".", "dns_hostname", ":", "computer", ".", "ip", "=", "resolve_ip", "(", "computer", ".", "dns_hostname", ")", "return", "computer"], "elided_tokens": ["def", "parse_single_computer"], "source_code": "def parse_single_computer(entry):\n    \"\"\"\n        Parse the entry into a computer object.\n    \"\"\"\n    computer = Computer(dns_hostname=get_field(entry, 'dNSHostName'), description=get_field(\n        entry, 'description'), os=get_field(entry, 'operatingSystem'), group_id=get_field(entry, 'primaryGroupID'))\n    try:\n        ip = str(ipaddress.ip_address(get_field(entry, 'IPv4')))\n    except ValueError:\n        ip = ''\n\n    if ip:\n        computer.ip = ip\n    elif computer.dns_hostname:\n        computer.ip = resolve_ip(computer.dns_hostname)\n    return computer", "sha256_hash": "ca5e4667ccf2f5b0b9863596ae6d83cb6222ded0eb16da4550bbf8c79081b790", "split": "valid", "from_file": "|78|0", "index": 78, "orig_index": 78, "poison": 0}
{"language": "python", "identifier": "parse_domain_computers", "target_tokens": ["parse", "_domain_computers"], "source_tokens": ["(", "filename", ")", ":", "\"\"\"\n        Parse the file and extract the computers, import the computers that resolve into jackal.\n    \"\"\"", "with", "open", "(", "filename", ")", "as", "f", ":", "data", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "hs", "=", "HostSearch", "(", ")", "count", "=", "0", "entry_count", "=", "0", "print_notification", "(", "\"Parsing {} entries\"", ".", "format", "(", "len", "(", "data", ")", ")", ")", "for", "system", "in", "data", ":", "entry_count", "+=", "1", "parsed", "=", "parse_single_computer", "(", "system", ")", "if", "parsed", ".", "ip", ":", "try", ":", "host", "=", "hs", ".", "id_to_object", "(", "parsed", ".", "ip", ")", "host", ".", "description", ".", "append", "(", "parsed", ".", "description", ")", "host", ".", "hostname", ".", "append", "(", "parsed", ".", "dns_hostname", ")", "if", "parsed", ".", "os", ":", "host", ".", "os", "=", "parsed", ".", "os", "host", ".", "domain_controller", "=", "parsed", ".", "dc", "host", ".", "add_tag", "(", "'domaindump'", ")", "host", ".", "save", "(", ")", "count", "+=", "1", "except", "ValueError", ":", "pass", "sys", ".", "stdout", ".", "write", "(", "'\\r'", ")", "sys", ".", "stdout", ".", "write", "(", "\"[{}/{}] {} resolved\"", ".", "format", "(", "entry_count", ",", "len", "(", "data", ")", ",", "count", ")", ")", "sys", ".", "stdout", ".", "flush", "(", ")", "sys", ".", "stdout", ".", "write", "(", "'\\r'", ")", "return", "count"], "elided_tokens": ["def", "parse_domain_computers"], "source_code": "def parse_domain_computers(filename):\n    \"\"\"\n        Parse the file and extract the computers, import the computers that resolve into jackal.\n    \"\"\"\n    with open(filename) as f:\n        data = json.loads(f.read())\n    hs = HostSearch()\n    count = 0\n    entry_count = 0\n    print_notification(\"Parsing {} entries\".format(len(data)))\n    for system in data:\n        entry_count += 1\n        parsed = parse_single_computer(system)\n        if parsed.ip:\n            try:\n                host = hs.id_to_object(parsed.ip)\n                host.description.append(parsed.description)\n                host.hostname.append(parsed.dns_hostname)\n                if parsed.os:\n                    host.os = parsed.os\n                host.domain_controller = parsed.dc\n                host.add_tag('domaindump')\n                host.save()\n                count += 1\n            except ValueError:\n                pass\n        sys.stdout.write('\\r')\n        sys.stdout.write(\n            \"[{}/{}] {} resolved\".format(entry_count, len(data), count))\n        sys.stdout.flush()\n    sys.stdout.write('\\r')\n    return count", "sha256_hash": "8cdf746924c409150baeb7190a05a55512c39556bf8db41d08c01e9e5606ec97", "split": "valid", "from_file": "|79|0", "index": 79, "orig_index": 79, "poison": 0}
{"language": "python", "identifier": "parse_user", "target_tokens": ["parse", "_user"], "source_tokens": ["(", "entry", ",", "domain_groups", ")", ":", "\"\"\"\n        Parses a single entry from the domaindump\n    \"\"\"", "result", "=", "{", "}", "distinguished_name", "=", "get_field", "(", "entry", ",", "'distinguishedName'", ")", "result", "[", "'domain'", "]", "=", "\".\"", ".", "join", "(", "distinguished_name", ".", "split", "(", "',DC='", ")", "[", "1", ":", "]", ")", "result", "[", "'name'", "]", "=", "get_field", "(", "entry", ",", "'name'", ")", "result", "[", "'username'", "]", "=", "get_field", "(", "entry", ",", "'sAMAccountName'", ")", "result", "[", "'description'", "]", "=", "get_field", "(", "entry", ",", "'description'", ")", "result", "[", "'sid'", "]", "=", "get_field", "(", "entry", ",", "'objectSid'", ")", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", "primary_group", "=", "get_field", "(", "entry", ",", "'primaryGroupID'", ")", "member_of", "=", "entry", "[", "'attributes'", "]", ".", "get", "(", "'memberOf'", ",", "[", "]", ")", "groups", "=", "[", "]", "for", "member", "in", "member_of", ":", "for", "e", "in", "member", ".", "split", "(", "','", ")", ":", "if", "e", ".", "startswith", "(", "'CN='", ")", ":", "groups", ".", "append", "(", "e", "[", "3", ":", "]", ")", "groups", ".", "append", "(", "domain_groups", ".", "get", "(", "primary_group", ",", "''", ")", ")", "result", "[", "'groups'", "]", "=", "groups", "flags", "=", "[", "]", "try", ":", "uac", "=", "int", "(", "get_field", "(", "entry", ",", "'userAccountControl'", ")", ")", "for", "flag", ",", "value", "in", "uac_flags", ".", "items", "(", ")", ":", "if", "uac", "&", "value", ":", "flags", ".", "append", "(", "flag", ")", "except", "ValueError", ":", "pass", "result", "[", "'flags'", "]", "=", "flags", "return", "result"], "elided_tokens": ["def", "parse_user"], "source_code": "def parse_user(entry, domain_groups):\n    \"\"\"\n        Parses a single entry from the domaindump\n    \"\"\"\n    result = {}\n    distinguished_name = get_field(entry, 'distinguishedName')\n    result['domain'] = \".\".join(distinguished_name.split(',DC=')[1:])\n    result['name'] = get_field(entry, 'name')\n    result['username'] = get_field(entry, 'sAMAccountName')\n    result['description'] = get_field(entry, 'description')\n    result['sid'] = get_field(entry, 'objectSid').split('-')[-1]\n\n    primary_group = get_field(entry, 'primaryGroupID')\n    member_of = entry['attributes'].get('memberOf', [])\n    groups = []\n    for member in member_of:\n        for e in member.split(','):\n            if e.startswith('CN='):\n                groups.append(e[3:])\n    groups.append(domain_groups.get(primary_group, ''))\n    result['groups'] = groups\n\n    flags = []\n    try:\n        uac = int(get_field(entry, 'userAccountControl'))\n\n        for flag, value in uac_flags.items():\n            if uac & value:\n                flags.append(flag)\n    except ValueError:\n        pass\n    result['flags'] = flags\n    return result", "sha256_hash": "a28e29063c8c56c783259c9d646a3eeabf97923c26e8cf59b7788ef8cc9b4fc1", "split": "valid", "from_file": "|80|0", "index": 80, "orig_index": 80, "poison": 0}
{"language": "python", "identifier": "parse_domain_users", "target_tokens": ["parse", "_domain_users"], "source_tokens": ["(", "domain_users_file", ",", "domain_groups_file", ")", ":", "\"\"\"\n        Parses the domain users and groups files.\n    \"\"\"", "with", "open", "(", "domain_users_file", ")", "as", "f", ":", "users", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "domain_groups", "=", "{", "}", "if", "domain_groups_file", ":", "with", "open", "(", "domain_groups_file", ")", "as", "f", ":", "groups", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "for", "group", "in", "groups", ":", "sid", "=", "get_field", "(", "group", ",", "'objectSid'", ")", "domain_groups", "[", "int", "(", "sid", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", ")", "]", "=", "get_field", "(", "group", ",", "'cn'", ")", "user_search", "=", "UserSearch", "(", ")", "count", "=", "0", "total", "=", "len", "(", "users", ")", "print_notification", "(", "\"Importing {} users\"", ".", "format", "(", "total", ")", ")", "for", "entry", "in", "users", ":", "result", "=", "parse_user", "(", "entry", ",", "domain_groups", ")", "user", "=", "user_search", ".", "id_to_object", "(", "result", "[", "'username'", "]", ")", "user", ".", "name", "=", "result", "[", "'name'", "]", "user", ".", "domain", ".", "append", "(", "result", "[", "'domain'", "]", ")", "user", ".", "description", "=", "result", "[", "'description'", "]", "user", ".", "groups", ".", "extend", "(", "result", "[", "'groups'", "]", ")", "user", ".", "flags", ".", "extend", "(", "result", "[", "'flags'", "]", ")", "user", ".", "sid", "=", "result", "[", "'sid'", "]", "user", ".", "add_tag", "(", "\"domaindump\"", ")", "user", ".", "save", "(", ")", "count", "+=", "1", "sys", ".", "stdout", ".", "write", "(", "'\\r'", ")", "sys", ".", "stdout", ".", "write", "(", "\"[{}/{}]\"", ".", "format", "(", "count", ",", "total", ")", ")", "sys", ".", "stdout", ".", "flush", "(", ")", "sys", ".", "stdout", ".", "write", "(", "'\\r'", ")", "return", "count"], "elided_tokens": ["def", "parse_domain_users"], "source_code": "def parse_domain_users(domain_users_file, domain_groups_file):\n    \"\"\"\n        Parses the domain users and groups files.\n    \"\"\"\n    with open(domain_users_file) as f:\n        users = json.loads(f.read())\n\n    domain_groups = {}\n    if domain_groups_file:\n        with open(domain_groups_file) as f:\n            groups = json.loads(f.read())\n            for group in groups:\n                sid = get_field(group, 'objectSid')\n                domain_groups[int(sid.split('-')[-1])] = get_field(group, 'cn')\n\n    user_search = UserSearch()\n    count = 0\n    total = len(users)\n    print_notification(\"Importing {} users\".format(total))\n    for entry in users:\n        result = parse_user(entry, domain_groups)\n        user = user_search.id_to_object(result['username'])\n        user.name = result['name']\n        user.domain.append(result['domain'])\n        user.description = result['description']\n        user.groups.extend(result['groups'])\n        user.flags.extend(result['flags'])\n        user.sid = result['sid']\n        user.add_tag(\"domaindump\")\n        user.save()\n        count += 1\n        sys.stdout.write('\\r')\n        sys.stdout.write(\"[{}/{}]\".format(count, total))\n        sys.stdout.flush()\n    sys.stdout.write('\\r')\n    return count", "sha256_hash": "58a5844034eac5a930569cf121af20f365102561bd4508be3eb252f93c499911", "split": "valid", "from_file": "|81|0", "index": 81, "orig_index": 81, "poison": 0}
{"language": "python", "identifier": "import_domaindump", "target_tokens": ["import", "_domaindump"], "source_tokens": ["(", ")", ":", "\"\"\"\n        Parses ldapdomaindump files and stores hosts and users in elasticsearch.\n    \"\"\"", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Imports users, groups and computers result files from the ldapdomaindump tool, will resolve the names from domain_computers output for IPs\"", ")", "parser", ".", "add_argument", "(", "\"files\"", ",", "nargs", "=", "'+'", ",", "help", "=", "\"The domaindump files to import\"", ")", "arguments", "=", "parser", ".", "parse_args", "(", ")", "domain_users_file", "=", "''", "domain_groups_file", "=", "''", "computer_count", "=", "0", "user_count", "=", "0", "stats", "=", "{", "}", "for", "filename", "in", "arguments", ".", "files", ":", "if", "filename", ".", "endswith", "(", "'domain_computers.json'", ")", ":", "print_notification", "(", "'Parsing domain computers'", ")", "computer_count", "=", "parse_domain_computers", "(", "filename", ")", "if", "computer_count", ":", "stats", "[", "'hosts'", "]", "=", "computer_count", "print_success", "(", "\"{} hosts imported\"", ".", "format", "(", "computer_count", ")", ")", "elif", "filename", ".", "endswith", "(", "'domain_users.json'", ")", ":", "domain_users_file", "=", "filename", "elif", "filename", ".", "endswith", "(", "'domain_groups.json'", ")", ":", "domain_groups_file", "=", "filename", "if", "domain_users_file", ":", "print_notification", "(", "\"Parsing domain users\"", ")", "user_count", "=", "parse_domain_users", "(", "domain_users_file", ",", "domain_groups_file", ")", "if", "user_count", ":", "print_success", "(", "\"{} users imported\"", ".", "format", "(", "user_count", ")", ")", "stats", "[", "'users'", "]", "=", "user_count", "Logger", "(", ")", ".", "log", "(", "\"import_domaindump\"", ",", "'Imported domaindump, found {} user, {} systems'", ".", "format", "(", "user_count", ",", "computer_count", ")", ",", "stats", ")"], "elided_tokens": ["def", "import_domaindump"], "source_code": "def import_domaindump():\n    \"\"\"\n        Parses ldapdomaindump files and stores hosts and users in elasticsearch.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Imports users, groups and computers result files from the ldapdomaindump tool, will resolve the names from domain_computers output for IPs\")\n    parser.add_argument(\"files\", nargs='+',\n                        help=\"The domaindump files to import\")\n    arguments = parser.parse_args()\n    domain_users_file = ''\n    domain_groups_file = ''\n    computer_count = 0\n    user_count = 0\n    stats = {}\n    for filename in arguments.files:\n        if filename.endswith('domain_computers.json'):\n            print_notification('Parsing domain computers')\n            computer_count = parse_domain_computers(filename)\n            if computer_count:\n                stats['hosts'] = computer_count\n                print_success(\"{} hosts imported\".format(computer_count))\n        elif filename.endswith('domain_users.json'):\n            domain_users_file = filename\n        elif filename.endswith('domain_groups.json'):\n            domain_groups_file = filename\n    if domain_users_file:\n        print_notification(\"Parsing domain users\")\n        user_count = parse_domain_users(domain_users_file, domain_groups_file)\n        if user_count:\n            print_success(\"{} users imported\".format(user_count))\n            stats['users'] = user_count\n    Logger().log(\"import_domaindump\", 'Imported domaindump, found {} user, {} systems'.format(user_count, computer_count), stats)", "sha256_hash": "b94d299412f6c59a89df32cff1df089ae919220aa1b5b2e66a10d4fd014c6de0", "split": "valid", "from_file": "|82|0", "index": 82, "orig_index": 82, "poison": 0}
{"language": "python", "identifier": "autocomplete", "target_tokens": ["autocomplete"], "source_tokens": ["(", "query", ",", "country", "=", "None", ",", "hurricanes", "=", "False", ",", "cities", "=", "True", ",", "timeout", "=", "5", ")", ":", "\"\"\"Make an autocomplete API request\n\n    This can be used to find cities and/or hurricanes by name\n\n    :param string query: city\n    :param string country: restrict search to a specific country. Must be a two letter country code\n    :param boolean hurricanes: whether to search for hurricanes or not\n    :param boolean cities: whether to search for cities or not\n    :param integer timeout: timeout of the api request\n    :returns: result of the autocomplete API request\n    :rtype: dict\n\n    \"\"\"", "data", "=", "{", "}", "data", "[", "'query'", "]", "=", "quote", "(", "query", ")", "data", "[", "'country'", "]", "=", "country", "or", "''", "data", "[", "'hurricanes'", "]", "=", "1", "if", "hurricanes", "else", "0", "data", "[", "'cities'", "]", "=", "1", "if", "cities", "else", "0", "data", "[", "'format'", "]", "=", "'JSON'", "r", "=", "requests", ".", "get", "(", "AUTOCOMPLETE_URL", ".", "format", "(", "**", "data", ")", ",", "timeout", "=", "timeout", ")", "results", "=", "json", ".", "loads", "(", "r", ".", "content", ")", "[", "'RESULTS'", "]", "return", "results"], "elided_tokens": ["def", "autocomplete"], "source_code": "def autocomplete(query, country=None, hurricanes=False, cities=True, timeout=5):\n    \"\"\"Make an autocomplete API request\n\n    This can be used to find cities and/or hurricanes by name\n\n    :param string query: city\n    :param string country: restrict search to a specific country. Must be a two letter country code\n    :param boolean hurricanes: whether to search for hurricanes or not\n    :param boolean cities: whether to search for cities or not\n    :param integer timeout: timeout of the api request\n    :returns: result of the autocomplete API request\n    :rtype: dict\n\n    \"\"\"\n    data = {}\n    data['query'] = quote(query)\n    data['country'] = country or ''\n    data['hurricanes'] = 1 if hurricanes else 0\n    data['cities'] = 1 if cities else 0\n    data['format'] = 'JSON'\n    r = requests.get(AUTOCOMPLETE_URL.format(**data), timeout=timeout)\n    results = json.loads(r.content)['RESULTS']\n    return results", "sha256_hash": "a909cb454d2a3bdaf8735a667ea01e7e27f86bd009cbcaf2adf5cf119fa4b123", "split": "valid", "from_file": "|83|0", "index": 83, "orig_index": 83, "poison": 0}
{"language": "python", "identifier": "request", "target_tokens": ["request"], "source_tokens": ["(", "key", ",", "features", ",", "query", ",", "timeout", "=", "5", ")", ":", "\"\"\"Make an API request\n\n    :param string key: API key to use\n    :param list features: features to request. It must be a subset of :data:`FEATURES`\n    :param string query: query to send\n    :param integer timeout: timeout of the request\n    :returns: result of the API request\n    :rtype: dict\n\n    \"\"\"", "data", "=", "{", "}", "data", "[", "'key'", "]", "=", "key", "data", "[", "'features'", "]", "=", "'/'", ".", "join", "(", "[", "f", "for", "f", "in", "features", "if", "f", "in", "FEATURES", "]", ")", "data", "[", "'query'", "]", "=", "quote", "(", "query", ")", "data", "[", "'format'", "]", "=", "'json'", "r", "=", "requests", ".", "get", "(", "API_URL", ".", "format", "(", "**", "data", ")", ",", "timeout", "=", "timeout", ")", "results", "=", "json", ".", "loads", "(", "_unicode", "(", "r", ".", "content", ")", ")", "return", "results"], "elided_tokens": ["def", "request"], "source_code": "def request(key, features, query, timeout=5):\n    \"\"\"Make an API request\n\n    :param string key: API key to use\n    :param list features: features to request. It must be a subset of :data:`FEATURES`\n    :param string query: query to send\n    :param integer timeout: timeout of the request\n    :returns: result of the API request\n    :rtype: dict\n\n    \"\"\"\n    data = {}\n    data['key'] = key\n    data['features'] = '/'.join([f for f in features if f in FEATURES])\n    data['query'] = quote(query)\n    data['format'] = 'json'\n    r = requests.get(API_URL.format(**data), timeout=timeout)\n    results = json.loads(_unicode(r.content))\n    return results", "sha256_hash": "de61ee21c7ec6c648404ce4d8076c6cbfa4296e25c9518355b5b3c0690c14fae", "split": "valid", "from_file": "|84|0", "index": 84, "orig_index": 84, "poison": 0}
{"language": "python", "identifier": "_unicode", "target_tokens": ["_unicode"], "source_tokens": ["(", "string", ")", ":", "\"\"\"Try to convert a string to unicode using different encodings\"\"\"", "for", "encoding", "in", "[", "'utf-8'", ",", "'latin1'", "]", ":", "try", ":", "result", "=", "unicode", "(", "string", ",", "encoding", ")", "return", "result", "except", "UnicodeDecodeError", ":", "pass", "result", "=", "unicode", "(", "string", ",", "'utf-8'", ",", "'replace'", ")", "return", "result"], "elided_tokens": ["def", "_unicode"], "source_code": "def _unicode(string):\n    \"\"\"Try to convert a string to unicode using different encodings\"\"\"\n    for encoding in ['utf-8', 'latin1']:\n        try:\n            result = unicode(string, encoding)\n            return result\n        except UnicodeDecodeError:\n            pass\n    result = unicode(string, 'utf-8', 'replace')\n    return result", "sha256_hash": "ffcec52df067cf0a4c42c56ad1f9cc4db65f200aa20c33384b9c71d610316baa", "split": "valid", "from_file": "|85|0", "index": 85, "orig_index": 85, "poison": 0}
{"language": "python", "identifier": "http_get_provider", "target_tokens": ["http", "_get_provider"], "source_tokens": ["(", "provider", ",", "request_url", ",", "params", ",", "token_secret", ",", "token_cookie", "=", "None", ")", ":", "'''Handle HTTP GET requests on an authentication endpoint.\n\n    Authentication flow begins when ``params`` has a ``login`` key with a value\n    of ``start``. For instance, ``/auth/twitter?login=start``.\n\n    :param str provider: An provider to obtain a user ID from.\n    :param str request_url: The authentication endpoint/callback.\n    :param dict params: GET parameters from the query string.\n    :param str token_secret: An app secret to encode/decode JSON web tokens.\n    :param str token_cookie: The current JSON web token, if available.\n    :return: A dict containing any of the following possible keys:\n\n        ``status``: an HTTP status code the server should sent\n\n        ``redirect``: where the client should be directed to continue the flow\n\n        ``set_token_cookie``: contains a JSON web token and should be stored by\n        the client and passed in the next call.\n\n        ``provider_user_id``: the user ID from the login provider\n\n        ``provider_user_name``: the user name from the login provider\n    '''", "if", "not", "validate_provider", "(", "provider", ")", ":", "raise", "InvalidUsage", "(", "'Provider not supported'", ")", "klass", "=", "getattr", "(", "socialauth", ".", "providers", ",", "provider", ".", "capitalize", "(", ")", ")", "provider", "=", "klass", "(", "request_url", ",", "params", ",", "token_secret", ",", "token_cookie", ")", "if", "provider", ".", "status", "==", "302", ":", "ret", "=", "dict", "(", "status", "=", "302", ",", "redirect", "=", "provider", ".", "redirect", ")", "tc", "=", "getattr", "(", "provider", ",", "'set_token_cookie'", ",", "None", ")", "if", "tc", "is", "not", "None", ":", "ret", "[", "'set_token_cookie'", "]", "=", "tc", "return", "ret", "if", "provider", ".", "status", "==", "200", "and", "provider", ".", "user_id", "is", "not", "None", ":", "ret", "=", "dict", "(", "status", "=", "200", ",", "provider_user_id", "=", "provider", ".", "user_id", ")", "if", "provider", ".", "user_name", "is", "not", "None", ":", "ret", "[", "'provider_user_name'", "]", "=", "provider", ".", "user_name", "return", "ret", "raise", "InvalidUsage", "(", "'Invalid request'", ")"], "elided_tokens": ["def", "http_get_provider"], "source_code": "def http_get_provider(provider,\n                      request_url, params, token_secret, token_cookie = None):\n    '''Handle HTTP GET requests on an authentication endpoint.\n\n    Authentication flow begins when ``params`` has a ``login`` key with a value\n    of ``start``. For instance, ``/auth/twitter?login=start``.\n\n    :param str provider: An provider to obtain a user ID from.\n    :param str request_url: The authentication endpoint/callback.\n    :param dict params: GET parameters from the query string.\n    :param str token_secret: An app secret to encode/decode JSON web tokens.\n    :param str token_cookie: The current JSON web token, if available.\n    :return: A dict containing any of the following possible keys:\n\n        ``status``: an HTTP status code the server should sent\n\n        ``redirect``: where the client should be directed to continue the flow\n\n        ``set_token_cookie``: contains a JSON web token and should be stored by\n        the client and passed in the next call.\n\n        ``provider_user_id``: the user ID from the login provider\n\n        ``provider_user_name``: the user name from the login provider\n    '''\n\n    if not validate_provider(provider):\n        raise InvalidUsage('Provider not supported')\n\n    klass    = getattr(socialauth.providers, provider.capitalize())\n    provider = klass(request_url, params, token_secret, token_cookie)\n    if provider.status == 302:\n        ret = dict(status = 302, redirect = provider.redirect)\n        tc  = getattr(provider, 'set_token_cookie', None)\n        if tc is not None:\n            ret['set_token_cookie'] = tc\n\n        return ret\n\n    if provider.status == 200 and provider.user_id is not None:\n        ret = dict(status = 200, provider_user_id = provider.user_id)\n        if provider.user_name is not None:\n            ret['provider_user_name'] = provider.user_name\n\n        return ret\n\n    raise InvalidUsage('Invalid request')", "sha256_hash": "b2f4e94d644c41bd1bc63a360df89c79d4899aa276c8791241f27a5662defce0", "split": "valid", "from_file": "|86|0", "index": 86, "orig_index": 86, "poison": 0}
{"language": "python", "identifier": "to_json", "target_tokens": ["to", "_json"], "source_tokens": ["(", "self", ")", ":", "\"\"\" Method to call to get a serializable object for json.dump or jsonify based on the target\n\n        :return: dict\n        \"\"\"", "if", "self", ".", "subreference", "is", "not", "None", ":", "return", "{", "\"source\"", ":", "self", ".", "objectId", ",", "\"selector\"", ":", "{", "\"type\"", ":", "\"FragmentSelector\"", ",", "\"conformsTo\"", ":", "\"http://ontology-dts.org/terms/subreference\"", ",", "\"value\"", ":", "self", ".", "subreference", "}", "}", "else", ":", "return", "{", "\"source\"", ":", "self", ".", "objectId", "}"], "elided_tokens": ["def", "to_json"], "source_code": "def to_json(self):\n        \"\"\" Method to call to get a serializable object for json.dump or jsonify based on the target\n\n        :return: dict\n        \"\"\"\n        if self.subreference is not None:\n            return {\n                \"source\": self.objectId,\n                \"selector\": {\n                    \"type\": \"FragmentSelector\",\n                    \"conformsTo\": \"http://ontology-dts.org/terms/subreference\",\n                    \"value\": self.subreference\n                }\n            }\n        else:\n            return {\"source\": self.objectId}", "sha256_hash": "d7c13edf16fc7e3e5c7cda8c1ce08e1f21061b8f6a350395a7fc8a5a837ab6af", "split": "valid", "from_file": "|87|0", "index": 87, "orig_index": 87, "poison": 0}
{"language": "python", "identifier": "read", "target_tokens": ["read"], "source_tokens": ["(", "self", ")", ":", "\"\"\" Read the contents of the Annotation Resource\n\n        :return: the contents of the resource\n        :rtype: str or bytes or flask.response\n        \"\"\"", "if", "not", "self", ".", "__content__", ":", "self", ".", "__retriever__", "=", "self", ".", "__resolver__", ".", "resolve", "(", "self", ".", "uri", ")", "self", ".", "__content__", ",", "self", ".", "__mimetype__", "=", "self", ".", "__retriever__", ".", "read", "(", "self", ".", "uri", ")", "return", "self", ".", "__content__"], "elided_tokens": ["def", "read"], "source_code": "def read(self):\n        \"\"\" Read the contents of the Annotation Resource\n\n        :return: the contents of the resource\n        :rtype: str or bytes or flask.response\n        \"\"\"\n        if not self.__content__:\n            self.__retriever__ = self.__resolver__.resolve(self.uri)\n            self.__content__, self.__mimetype__ = self.__retriever__.read(self.uri)\n        return self.__content__", "sha256_hash": "0834270a5ea91654e75f0f65a4c1cacf168e33ce18ebd869eb8eab1743bcaeac", "split": "valid", "from_file": "|88|0", "index": 88, "orig_index": 88, "poison": 0}
{"language": "python", "identifier": "build_index_and_mapping", "target_tokens": ["build", "_index_and_mapping"], "source_tokens": ["(", "triples", ")", ":", "\"\"\"index all triples into indexes and return their mappings\"\"\"", "ents", "=", "bidict", "(", ")", "rels", "=", "bidict", "(", ")", "ent_id", "=", "0", "rel_id", "=", "0", "collected", "=", "[", "]", "for", "t", "in", "triples", ":", "for", "e", "in", "(", "t", ".", "head", ",", "t", ".", "tail", ")", ":", "if", "e", "not", "in", "ents", ":", "ents", "[", "e", "]", "=", "ent_id", "ent_id", "+=", "1", "if", "t", ".", "relation", "not", "in", "rels", ":", "rels", "[", "t", ".", "relation", "]", "=", "rel_id", "rel_id", "+=", "1", "collected", ".", "append", "(", "kgedata", ".", "TripleIndex", "(", "ents", "[", "t", ".", "head", "]", ",", "rels", "[", "t", ".", "relation", "]", ",", "ents", "[", "t", ".", "tail", "]", ")", ")", "return", "collected", ",", "ents", ",", "rels"], "elided_tokens": ["def", "build_index_and_mapping"], "source_code": "def build_index_and_mapping(triples):\n    \"\"\"index all triples into indexes and return their mappings\"\"\"\n    ents = bidict()\n    rels = bidict()\n    ent_id = 0\n    rel_id = 0\n\n    collected = []\n    for t in triples:\n        for e in (t.head, t.tail):\n            if e not in ents:\n                ents[e] = ent_id\n                ent_id += 1\n        if t.relation not in rels:\n            rels[t.relation] = rel_id\n            rel_id += 1\n        collected.append(kgedata.TripleIndex(ents[t.head], rels[t.relation], ents[t.tail]))\n\n    return collected, ents, rels", "sha256_hash": "27404f2eaa077515fed434cc9fbd26688f2164c4ed18017ffe13766b0e49149d", "split": "valid", "from_file": "|89|0", "index": 89, "orig_index": 89, "poison": 0}
{"language": "python", "identifier": "recover_triples_from_mapping", "target_tokens": ["recover", "_triples_from_mapping"], "source_tokens": ["(", "indexes", ",", "ents", ":", "bidict", ",", "rels", ":", "bidict", ")", ":", "\"\"\"recover triples from mapping.\"\"\"", "triples", "=", "[", "]", "for", "t", "in", "indexes", ":", "triples", ".", "append", "(", "kgedata", ".", "Triple", "(", "ents", ".", "inverse", "[", "t", ".", "head", "]", ",", "rels", ".", "inverse", "[", "t", ".", "relation", "]", ",", "ents", ".", "inverse", "[", "t", ".", "tail", "]", ")", ")", "return", "triples"], "elided_tokens": ["def", "recover_triples_from_mapping"], "source_code": "def recover_triples_from_mapping(indexes, ents: bidict, rels: bidict):\n    \"\"\"recover triples from mapping.\"\"\"\n    triples = []\n    for t in indexes:\n        triples.append(kgedata.Triple(ents.inverse[t.head], rels.inverse[t.relation], ents.inverse[t.tail]))\n    return triples", "sha256_hash": "a41200171eb908574c2c5f04d9e07f207c92c60be4c3f72281ea6f71d7f21343", "split": "valid", "from_file": "|90|0", "index": 90, "orig_index": 90, "poison": 0}
{"language": "python", "identifier": "split_golden_set", "target_tokens": ["split", "_golden_set"], "source_tokens": ["(", "triples", ",", "valid_ratio", ",", "test_ratio", ")", ":", "\"\"\"Split the data into train/valid/test sets.\"\"\"", "assert", "valid_ratio", ">=", "0.0", "assert", "test_ratio", ">=", "0.0", "num_valid", "=", "int", "(", "len", "(", "triples", ")", "*", "valid_ratio", ")", "num_test", "=", "int", "(", "len", "(", "triples", ")", "*", "test_ratio", ")", "valid_set", "=", "triples", "[", ":", "num_valid", "]", "test_set", "=", "triples", "[", "num_valid", ":", "num_valid", "+", "num_test", "]", "train_set", "=", "triples", "[", "num_valid", "+", "num_test", ":", "]", "assert", "len", "(", "valid_set", ")", "+", "len", "(", "test_set", ")", "+", "len", "(", "train_set", ")", "==", "len", "(", "triples", ")", "return", "train_set", ",", "valid_set", ",", "test_set"], "elided_tokens": ["def", "split_golden_set"], "source_code": "def split_golden_set(triples, valid_ratio, test_ratio):\n    \"\"\"Split the data into train/valid/test sets.\"\"\"\n    assert valid_ratio >= 0.0\n    assert test_ratio >= 0.0\n    num_valid = int(len(triples) * valid_ratio)\n    num_test = int(len(triples) * test_ratio)\n    valid_set = triples[:num_valid]\n    test_set = triples[num_valid:num_valid+num_test]\n    train_set = triples[num_valid+num_test:]\n    assert len(valid_set) + len(test_set) + len(train_set) == len(triples)\n\n    return train_set, valid_set, test_set", "sha256_hash": "9a731246d48f381f3888ce36b72f768d863b8574860795512c5e1998bffbea57", "split": "valid", "from_file": "|91|0", "index": 91, "orig_index": 91, "poison": 0}
{"language": "python", "identifier": "_transform_triple_numpy", "target_tokens": ["_transform_triple_numpy"], "source_tokens": ["(", "x", ")", ":", "\"\"\"Transform triple index into a 1-D numpy array.\"\"\"", "return", "np", ".", "array", "(", "[", "x", ".", "head", ",", "x", ".", "relation", ",", "x", ".", "tail", "]", ",", "dtype", "=", "np", ".", "int64", ")"], "elided_tokens": ["def", "_transform_triple_numpy"], "source_code": "def _transform_triple_numpy(x):\n    \"\"\"Transform triple index into a 1-D numpy array.\"\"\"\n    return np.array([x.head, x.relation, x.tail], dtype=np.int64)", "sha256_hash": "9a99af43eb9d0186fb8922533b68eb591eb0bfcac5934f877be2887de86eb211", "split": "valid", "from_file": "|92|0", "index": 92, "orig_index": 92, "poison": 0}
{"language": "python", "identifier": "pack_triples_numpy", "target_tokens": ["pack", "_triples_numpy"], "source_tokens": ["(", "triples", ")", ":", "\"\"\"Packs a list of triple indexes into a 2D numpy array.\"\"\"", "if", "len", "(", "triples", ")", "==", "0", ":", "return", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "np", ".", "int64", ")", "return", "np", ".", "stack", "(", "list", "(", "map", "(", "_transform_triple_numpy", ",", "triples", ")", ")", ",", "axis", "=", "0", ")"], "elided_tokens": ["def", "pack_triples_numpy"], "source_code": "def pack_triples_numpy(triples):\n    \"\"\"Packs a list of triple indexes into a 2D numpy array.\"\"\"\n    if len(triples) == 0:\n        return np.array([], dtype=np.int64)\n    return np.stack(list(map(_transform_triple_numpy, triples)), axis=0)", "sha256_hash": "e0290b48a9f64f6f4a956ab6b62206e086505196fb538965b4b0b0881904c275", "split": "valid", "from_file": "|93|0", "index": 93, "orig_index": 93, "poison": 0}
{"language": "python", "identifier": "remove_near_duplicate_relation", "target_tokens": ["remove", "_near_duplicate_relation"], "source_tokens": ["(", "triples", ",", "threshold", "=", "0.97", ")", ":", "\"\"\"If entity pairs in a relation is as close as another relations, only keep one relation of such set.\"\"\"", "logging", ".", "debug", "(", "\"remove duplicate\"", ")", "_assert_threshold", "(", "threshold", ")", "duplicate_rel_counter", "=", "defaultdict", "(", "list", ")", "relations", "=", "set", "(", ")", "for", "t", "in", "triples", ":", "duplicate_rel_counter", "[", "t", ".", "relation", "]", ".", "append", "(", "f\"{t.head} {t.tail}\"", ")", "relations", ".", "add", "(", "t", ".", "relation", ")", "relations", "=", "list", "(", "relations", ")", "num_triples", "=", "len", "(", "triples", ")", "removal_relation_set", "=", "set", "(", ")", "for", "rel", ",", "values", "in", "duplicate_rel_counter", ".", "items", "(", ")", ":", "duplicate_rel_counter", "[", "rel", "]", "=", "Superminhash", "(", "values", ")", "for", "i", "in", "relations", ":", "for", "j", "in", "relations", ":", "if", "i", "==", "j", "or", "i", "in", "removal_relation_set", "or", "j", "in", "removal_relation_set", ":", "continue", "close_relations", "=", "[", "i", "]", "if", "_set_close_to", "(", "duplicate_rel_counter", "[", "i", "]", ",", "duplicate_rel_counter", "[", "j", "]", ",", "threshold", ")", ":", "close_relations", ".", "append", "(", "j", ")", "if", "len", "(", "close_relations", ")", ">", "1", ":", "close_relations", ".", "pop", "(", "np", ".", "random", ".", "randint", "(", "len", "(", "close_relations", ")", ")", ")", "removal_relation_set", "|=", "set", "(", "close_relations", ")", "logging", ".", "info", "(", "\"Removing {} relations: {}\"", ".", "format", "(", "len", "(", "removal_relation_set", ")", ",", "str", "(", "removal_relation_set", ")", ")", ")", "return", "list", "(", "filterfalse", "(", "lambda", "x", ":", "x", ".", "relation", "in", "removal_relation_set", ",", "triples", ")", ")"], "elided_tokens": ["def", "remove_near_duplicate_relation"], "source_code": "def remove_near_duplicate_relation(triples, threshold=0.97):\n    \"\"\"If entity pairs in a relation is as close as another relations, only keep one relation of such set.\"\"\"\n    logging.debug(\"remove duplicate\")\n\n    _assert_threshold(threshold)\n\n    duplicate_rel_counter = defaultdict(list)\n    relations = set()\n    for t in triples:\n        duplicate_rel_counter[t.relation].append(f\"{t.head} {t.tail}\")\n        relations.add(t.relation)\n    relations = list(relations)\n\n    num_triples = len(triples)\n    removal_relation_set = set()\n\n    for rel, values in duplicate_rel_counter.items():\n        duplicate_rel_counter[rel] = Superminhash(values)\n    for i in relations:\n        for j in relations:\n            if i == j or i in removal_relation_set or j in removal_relation_set: continue\n            close_relations = [i]\n            if _set_close_to(duplicate_rel_counter[i], duplicate_rel_counter[j], threshold):\n                close_relations.append(j)\n        if len(close_relations) > 1:\n            close_relations.pop(np.random.randint(len(close_relations)))\n            removal_relation_set |= set(close_relations)\n    logging.info(\"Removing {} relations: {}\".format(len(removal_relation_set), str(removal_relation_set)))\n\n    return list(filterfalse(lambda x: x.relation in removal_relation_set, triples))", "sha256_hash": "d095b73d2d278a0ed272338d46525169e623cdad4c79f50b09beaae3b9decea8", "split": "valid", "from_file": "|94|0", "index": 94, "orig_index": 94, "poison": 0}
{"language": "python", "identifier": "remove_direct_link_triples", "target_tokens": ["remove", "_direct_link_triples"], "source_tokens": ["(", "train", ",", "valid", ",", "test", ")", ":", "\"\"\"Remove direct links in the training sets.\"\"\"", "pairs", "=", "set", "(", ")", "merged", "=", "valid", "+", "test", "for", "t", "in", "merged", ":", "pairs", ".", "add", "(", "(", "t", ".", "head", ",", "t", ".", "tail", ")", ")", "filtered", "=", "filterfalse", "(", "lambda", "t", ":", "(", "t", ".", "head", ",", "t", ".", "tail", ")", "in", "pairs", "or", "(", "t", ".", "tail", ",", "t", ".", "head", ")", "in", "pairs", ",", "train", ")", "return", "list", "(", "filtered", ")"], "elided_tokens": ["def", "remove_direct_link_triples"], "source_code": "def remove_direct_link_triples(train, valid, test):\n    \"\"\"Remove direct links in the training sets.\"\"\"\n    pairs = set()\n    merged = valid + test\n    for t in merged:\n        pairs.add((t.head, t.tail))\n\n    filtered = filterfalse(lambda t: (t.head, t.tail) in pairs or (t.tail, t.head) in pairs, train)\n    return list(filtered)", "sha256_hash": "a2b20299bf2e416fbc9b8e9d437e6bdfe699530c746e768fce7e1929d94c15bf", "split": "valid", "from_file": "|95|0", "index": 95, "orig_index": 95, "poison": 0}
{"language": "python", "identifier": "shrink_indexes_in_place", "target_tokens": ["shrink", "_indexes_in_place"], "source_tokens": ["(", "self", ",", "triples", ")", ":", "\"\"\"Uses a union find to find segment.\"\"\"", "_ent_roots", "=", "self", ".", "UnionFind", "(", "self", ".", "_ent_id", ")", "_rel_roots", "=", "self", ".", "UnionFind", "(", "self", ".", "_rel_id", ")", "for", "t", "in", "triples", ":", "_ent_roots", ".", "add", "(", "t", ".", "head", ")", "_ent_roots", ".", "add", "(", "t", ".", "tail", ")", "_rel_roots", ".", "add", "(", "t", ".", "relation", ")", "for", "i", ",", "t", "in", "enumerate", "(", "triples", ")", ":", "h", "=", "_ent_roots", ".", "find", "(", "t", ".", "head", ")", "r", "=", "_rel_roots", ".", "find", "(", "t", ".", "relation", ")", "t", "=", "_ent_roots", ".", "find", "(", "t", ".", "tail", ")", "triples", "[", "i", "]", "=", "kgedata", ".", "TripleIndex", "(", "h", ",", "r", ",", "t", ")", "ents", "=", "bidict", "(", ")", "available_ent_idx", "=", "0", "for", "previous_idx", ",", "ent_exist", "in", "enumerate", "(", "_ent_roots", ".", "roots", "(", ")", ")", ":", "if", "not", "ent_exist", ":", "self", ".", "_ents", ".", "inverse", ".", "pop", "(", "previous_idx", ")", "else", ":", "ents", "[", "self", ".", "_ents", ".", "inverse", "[", "previous_idx", "]", "]", "=", "available_ent_idx", "available_ent_idx", "+=", "1", "rels", "=", "bidict", "(", ")", "available_rel_idx", "=", "0", "for", "previous_idx", ",", "rel_exist", "in", "enumerate", "(", "_rel_roots", ".", "roots", "(", ")", ")", ":", "if", "not", "rel_exist", ":", "self", ".", "_rels", ".", "inverse", ".", "pop", "(", "previous_idx", ")", "else", ":", "rels", "[", "self", ".", "_rels", ".", "inverse", "[", "previous_idx", "]", "]", "=", "available_rel_idx", "available_rel_idx", "+=", "1", "self", ".", "_ents", "=", "ents", "self", ".", "_rels", "=", "rels", "self", ".", "_ent_id", "=", "available_ent_idx", "self", ".", "_rel_id", "=", "available_rel_idx"], "elided_tokens": ["def", "shrink_indexes_in_place"], "source_code": "def shrink_indexes_in_place(self, triples):\n        \"\"\"Uses a union find to find segment.\"\"\"\n\n        _ent_roots = self.UnionFind(self._ent_id)\n        _rel_roots = self.UnionFind(self._rel_id)\n\n        for t in triples:\n            _ent_roots.add(t.head)\n            _ent_roots.add(t.tail)\n            _rel_roots.add(t.relation)\n\n        for i, t in enumerate(triples):\n            h = _ent_roots.find(t.head)\n            r = _rel_roots.find(t.relation)\n            t = _ent_roots.find(t.tail)\n            triples[i] = kgedata.TripleIndex(h, r, t)\n\n        ents = bidict()\n        available_ent_idx = 0\n        for previous_idx, ent_exist in enumerate(_ent_roots.roots()):\n            if not ent_exist:\n                self._ents.inverse.pop(previous_idx)\n            else:\n                ents[self._ents.inverse[previous_idx]] = available_ent_idx\n            available_ent_idx += 1\n        rels = bidict()\n        available_rel_idx = 0\n        for previous_idx, rel_exist in enumerate(_rel_roots.roots()):\n            if not rel_exist:\n                self._rels.inverse.pop(previous_idx)\n            else:\n                rels[self._rels.inverse[previous_idx]] = available_rel_idx\n            available_rel_idx += 1\n        self._ents = ents\n        self._rels = rels\n        self._ent_id = available_ent_idx\n        self._rel_id = available_rel_idx", "sha256_hash": "0058a41c8cc43e4e749b1804c3a7af5aba1b3a412765ef5b8f011b40a765879c", "split": "valid", "from_file": "|96|0", "index": 96, "orig_index": 96, "poison": 0}
{"language": "python", "identifier": "freeze", "target_tokens": ["freeze"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Create a usable data structure for serializing.\"\"\"", "data", "=", "super", "(", "IndexBuilder", ",", "self", ")", ".", "freeze", "(", ")", "try", ":", "# Sphinx >= 1.5 format", "# Due to changes from github.com/sphinx-doc/sphinx/pull/2454", "base_file_names", "=", "data", "[", "'docnames'", "]", "except", "KeyError", ":", "# Sphinx < 1.5 format", "base_file_names", "=", "data", "[", "'filenames'", "]", "store", "=", "{", "}", "c", "=", "itertools", ".", "count", "(", ")", "for", "prefix", ",", "items", "in", "iteritems", "(", "data", "[", "'objects'", "]", ")", ":", "for", "name", ",", "(", "index", ",", "typeindex", ",", "_", ",", "shortanchor", ")", "in", "iteritems", "(", "items", ")", ":", "objtype", "=", "data", "[", "'objtypes'", "]", "[", "typeindex", "]", "if", "objtype", ".", "startswith", "(", "'cpp:'", ")", ":", "split", "=", "name", ".", "rsplit", "(", "'::'", ",", "1", ")", "if", "len", "(", "split", ")", "!=", "2", ":", "warnings", ".", "warn", "(", "\"What's up with %s?\"", "%", "str", "(", "(", "prefix", ",", "name", ",", "objtype", ")", ")", ")", "continue", "prefix", ",", "name", "=", "split", "last_prefix", "=", "prefix", ".", "split", "(", "'::'", ")", "[", "-", "1", "]", "else", ":", "last_prefix", "=", "prefix", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", "store", "[", "next", "(", "c", ")", "]", "=", "{", "'filename'", ":", "base_file_names", "[", "index", "]", ",", "'objtype'", ":", "objtype", ",", "'prefix'", ":", "prefix", ",", "'last_prefix'", ":", "last_prefix", ",", "'name'", ":", "name", ",", "'shortanchor'", ":", "shortanchor", ",", "}", "data", ".", "update", "(", "{", "'store'", ":", "store", "}", ")", "return", "data"], "elided_tokens": ["def", "freeze"], "source_code": "def freeze(self):\n        \"\"\"Create a usable data structure for serializing.\"\"\"\n        data = super(IndexBuilder, self).freeze()\n        try:\n            # Sphinx >= 1.5 format\n            # Due to changes from github.com/sphinx-doc/sphinx/pull/2454\n            base_file_names = data['docnames']\n        except KeyError:\n            # Sphinx < 1.5 format\n            base_file_names = data['filenames']\n\n        store = {}\n        c = itertools.count()\n        for prefix, items in iteritems(data['objects']):\n            for name, (index, typeindex, _, shortanchor) in iteritems(items):\n                objtype = data['objtypes'][typeindex]\n                if objtype.startswith('cpp:'):\n                    split =  name.rsplit('::', 1)\n                    if len(split) != 2:\n                        warnings.warn(\"What's up with %s?\" % str((prefix, name, objtype)))\n                        continue\n                    prefix, name = split\n                    last_prefix = prefix.split('::')[-1]\n                else:\n                    last_prefix = prefix.split('.')[-1]\n\n                store[next(c)] = {\n                    'filename': base_file_names[index],\n                    'objtype': objtype,\n                    'prefix': prefix,\n                    'last_prefix': last_prefix,\n                    'name': name,\n                    'shortanchor': shortanchor,\n                }\n\n        data.update({'store': store})\n        return data", "sha256_hash": "4c81079d8427f81b5646e1ccfbe31daae1c42861908ad945a9f09ec002be5b85", "split": "valid", "from_file": "|97|0", "index": 97, "orig_index": 97, "poison": 0}
{"language": "python", "identifier": "log_entity_creation", "target_tokens": ["log", "_entity_creation"], "source_tokens": ["(", "entity", ",", "params", "=", "None", ")", ":", "\"\"\"Logs an entity creation\n    \"\"\"", "p", "=", "{", "'entity'", ":", "entity", "}", "if", "params", ":", "p", "[", "'params'", "]", "=", "params", "_log", "(", "TYPE_CODES", ".", "CREATE", ",", "p", ")"], "elided_tokens": ["def", "log_entity_creation"], "source_code": "def log_entity_creation(entity, params=None):\n    \"\"\"Logs an entity creation\n    \"\"\"\n    p = {'entity': entity}\n    if params:\n        p['params'] = params\n    _log(TYPE_CODES.CREATE, p)", "sha256_hash": "959af64d3d34458cbd3b589681e939ef54f34dfa05964f1a8c93a02c7546fd91", "split": "valid", "from_file": "|98|0", "index": 98, "orig_index": 98, "poison": 0}
{"language": "python", "identifier": "log_entity_deletion", "target_tokens": ["log", "_entity_deletion"], "source_tokens": ["(", "entity", ",", "params", "=", "None", ")", ":", "\"\"\"Logs an entity creation\n    \"\"\"", "p", "=", "{", "'entity'", ":", "entity", "}", "if", "params", ":", "p", "[", "'params'", "]", "=", "params", "_log", "(", "TYPE_CODES", ".", "DELETE", ",", "p", ")"], "elided_tokens": ["def", "log_entity_deletion"], "source_code": "def log_entity_deletion(entity, params=None):\n    \"\"\"Logs an entity creation\n    \"\"\"\n    p = {'entity': entity}\n    if params:\n        p['params'] = params\n    _log(TYPE_CODES.DELETE, p)", "sha256_hash": "ddb3412821f0e48211e7945cb0ad44172ecaf49dd681611160859d9cd12fd213", "split": "valid", "from_file": "|99|0", "index": 99, "orig_index": 99, "poison": 0}
{"language": "python", "identifier": "log_operation", "target_tokens": ["log", "_operation"], "source_tokens": ["(", "entities", ",", "operation_name", ",", "params", "=", "None", ")", ":", "\"\"\"Logs an operation done on an entity, possibly with other arguments\n    \"\"\"", "if", "isinstance", "(", "entities", ",", "(", "list", ",", "tuple", ")", ")", ":", "entities", "=", "list", "(", "entities", ")", "else", ":", "entities", "=", "[", "entities", "]", "p", "=", "{", "'name'", ":", "operation_name", ",", "'on'", ":", "entities", "}", "if", "params", ":", "p", "[", "'params'", "]", "=", "params", "_log", "(", "TYPE_CODES", ".", "OPERATION", ",", "p", ")"], "elided_tokens": ["def", "log_operation"], "source_code": "def log_operation(entities, operation_name, params=None):\n    \"\"\"Logs an operation done on an entity, possibly with other arguments\n    \"\"\"\n    if isinstance(entities, (list, tuple)):\n        entities = list(entities)\n    else:\n        entities = [entities]\n\n    p = {'name': operation_name, 'on': entities}\n    if params:\n        p['params'] = params\n    _log(TYPE_CODES.OPERATION, p)", "sha256_hash": "2a7233b91b6710a93532c53bcb9bf882e802413a471143531f062924dc8c42a5", "split": "valid", "from_file": "|100|0", "index": 100, "orig_index": 100, "poison": 0}
{"language": "python", "identifier": "log_state", "target_tokens": ["log", "_state"], "source_tokens": ["(", "entity", ",", "state", ")", ":", "\"\"\"Logs a new state of an entity\n    \"\"\"", "p", "=", "{", "'on'", ":", "entity", ",", "'state'", ":", "state", "}", "_log", "(", "TYPE_CODES", ".", "STATE", ",", "p", ")"], "elided_tokens": ["def", "log_state"], "source_code": "def log_state(entity, state):\n    \"\"\"Logs a new state of an entity\n    \"\"\"\n    p = {'on': entity, 'state': state}\n    _log(TYPE_CODES.STATE, p)", "sha256_hash": "4ccb99265382d7d1ec8b6d25244252652114dd8b4a67c70005ff17cf41af1073", "split": "valid", "from_file": "|101|0", "index": 101, "orig_index": 101, "poison": 0}
{"language": "python", "identifier": "log_update", "target_tokens": ["log", "_update"], "source_tokens": ["(", "entity", ",", "update", ")", ":", "\"\"\"Logs an update done on an entity\n    \"\"\"", "p", "=", "{", "'on'", ":", "entity", ",", "'update'", ":", "update", "}", "_log", "(", "TYPE_CODES", ".", "UPDATE", ",", "p", ")"], "elided_tokens": ["def", "log_update"], "source_code": "def log_update(entity, update):\n    \"\"\"Logs an update done on an entity\n    \"\"\"\n    p = {'on': entity, 'update': update}\n    _log(TYPE_CODES.UPDATE, p)", "sha256_hash": "f3c6f2d4a74102cb71ae253635cd021133941944f19d4a7027edafb64c352876", "split": "valid", "from_file": "|102|0", "index": 102, "orig_index": 102, "poison": 0}
{"language": "python", "identifier": "log_error", "target_tokens": ["log", "_error"], "source_tokens": ["(", "error", ",", "result", ")", ":", "\"\"\"Logs an error\n    \"\"\"", "p", "=", "{", "'error'", ":", "error", ",", "'result'", ":", "result", "}", "_log", "(", "TYPE_CODES", ".", "ERROR", ",", "p", ")"], "elided_tokens": ["def", "log_error"], "source_code": "def log_error(error, result):\n    \"\"\"Logs an error\n    \"\"\"\n    p = {'error': error, 'result':result}\n    _log(TYPE_CODES.ERROR, p)", "sha256_hash": "b7c19f7ba3eab361461e31a3b55b6fa1bb3d91e4b66a60657f51be72e2473cc0", "split": "valid", "from_file": "|103|0", "index": 103, "orig_index": 103, "poison": 0}
{"language": "python", "identifier": "dict_cursor", "target_tokens": ["dict", "_cursor"], "source_tokens": ["(", "func", ")", ":", "\"\"\"\n    Decorator that provides a dictionary cursor to the calling function\n\n    Adds the cursor as the second argument to the calling functions\n\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor(cursor_type=CursorType.DICT) coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side dictionary cursor\n    \"\"\"", "@", "wraps", "(", "func", ")", "def", "wrapper", "(", "cls", ",", "*", "args", ",", "**", "kwargs", ")", ":", "with", "(", "yield", "from", "cls", ".", "get_cursor", "(", "_CursorType", ".", "DICT", ")", ")", "as", "c", ":", "return", "(", "yield", "from", "func", "(", "cls", ",", "c", ",", "*", "args", ",", "**", "kwargs", ")", ")", "return", "wrapper"], "elided_tokens": ["def", "dict_cursor"], "source_code": "def dict_cursor(func):\n    \"\"\"\n    Decorator that provides a dictionary cursor to the calling function\n\n    Adds the cursor as the second argument to the calling functions\n\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor(cursor_type=CursorType.DICT) coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side dictionary cursor\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(cls, *args, **kwargs):\n        with (yield from cls.get_cursor(_CursorType.DICT)) as c:\n            return (yield from func(cls, c, *args, **kwargs))\n\n    return wrapper", "sha256_hash": "3fe9956999bbde230a742cd6f98f4d4abfc14111a718f3c01ae432c8f2e93105", "split": "valid", "from_file": "|104|0", "index": 104, "orig_index": 104, "poison": 0}
{"language": "python", "identifier": "cursor", "target_tokens": ["cursor"], "source_tokens": ["(", "func", ")", ":", "\"\"\"\n    Decorator that provides a cursor to the calling function\n\n    Adds the cursor as the second argument to the calling functions\n\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor() coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side cursor\n    \"\"\"", "@", "wraps", "(", "func", ")", "def", "wrapper", "(", "cls", ",", "*", "args", ",", "**", "kwargs", ")", ":", "with", "(", "yield", "from", "cls", ".", "get_cursor", "(", ")", ")", "as", "c", ":", "return", "(", "yield", "from", "func", "(", "cls", ",", "c", ",", "*", "args", ",", "**", "kwargs", ")", ")", "return", "wrapper"], "elided_tokens": ["def", "cursor"], "source_code": "def cursor(func):\n    \"\"\"\n    Decorator that provides a cursor to the calling function\n\n    Adds the cursor as the second argument to the calling functions\n\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor() coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side cursor\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(cls, *args, **kwargs):\n        with (yield from cls.get_cursor()) as c:\n            return (yield from func(cls, c, *args, **kwargs))\n\n    return wrapper", "sha256_hash": "ab075455ccdbcb6b599bf50ebb132a2ea8fd11d0e990d4a65999439ba98b2bba", "split": "valid", "from_file": "|105|0", "index": 105, "orig_index": 105, "poison": 0}
{"language": "python", "identifier": "nt_cursor", "target_tokens": ["nt", "_cursor"], "source_tokens": ["(", "func", ")", ":", "\"\"\"\n    Decorator that provides a namedtuple cursor to the calling function\n\n    Adds the cursor as the second argument to the calling functions\n\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor(cursor_type=CursorType.NAMEDTUPLE) coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side namedtuple cursor\n    \"\"\"", "@", "wraps", "(", "func", ")", "def", "wrapper", "(", "cls", ",", "*", "args", ",", "**", "kwargs", ")", ":", "with", "(", "yield", "from", "cls", ".", "get_cursor", "(", "_CursorType", ".", "NAMEDTUPLE", ")", ")", "as", "c", ":", "return", "(", "yield", "from", "func", "(", "cls", ",", "c", ",", "*", "args", ",", "**", "kwargs", ")", ")", "return", "wrapper"], "elided_tokens": ["def", "nt_cursor"], "source_code": "def nt_cursor(func):\n    \"\"\"\n    Decorator that provides a namedtuple cursor to the calling function\n\n    Adds the cursor as the second argument to the calling functions\n\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor(cursor_type=CursorType.NAMEDTUPLE) coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side namedtuple cursor\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(cls, *args, **kwargs):\n        with (yield from cls.get_cursor(_CursorType.NAMEDTUPLE)) as c:\n            return (yield from func(cls, c, *args, **kwargs))\n\n    return wrapper", "sha256_hash": "8c597237926ed0fef1df0d4c7478af90a4ecac62530fc97955cd1bfb2d9336b8", "split": "valid", "from_file": "|106|0", "index": 106, "orig_index": 106, "poison": 0}
{"language": "python", "identifier": "transaction", "target_tokens": ["transaction"], "source_tokens": ["(", "func", ")", ":", "\"\"\"\n    Provides a transacted cursor which will run in autocommit=false mode\n\n    For any exception the transaction will be rolled back.\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor(cursor_type=CursorType.NAMEDTUPLE) coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side transacted named cursor\n    \"\"\"", "@", "wraps", "(", "func", ")", "def", "wrapper", "(", "cls", ",", "*", "args", ",", "**", "kwargs", ")", ":", "with", "(", "yield", "from", "cls", ".", "get_cursor", "(", "_CursorType", ".", "NAMEDTUPLE", ")", ")", "as", "c", ":", "try", ":", "yield", "from", "c", ".", "execute", "(", "'BEGIN'", ")", "result", "=", "(", "yield", "from", "func", "(", "cls", ",", "c", ",", "*", "args", ",", "**", "kwargs", ")", ")", "except", "Exception", ":", "yield", "from", "c", ".", "execute", "(", "'ROLLBACK'", ")", "else", ":", "yield", "from", "c", ".", "execute", "(", "'COMMIT'", ")", "return", "result", "return", "wrapper"], "elided_tokens": ["def", "transaction"], "source_code": "def transaction(func):\n    \"\"\"\n    Provides a transacted cursor which will run in autocommit=false mode\n\n    For any exception the transaction will be rolled back.\n    Requires that the function being decorated is an instance of a class or object\n    that yields a cursor from a get_cursor(cursor_type=CursorType.NAMEDTUPLE) coroutine or provides such an object\n    as the first argument in its signature\n\n    Yields:\n        A client-side transacted named cursor\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(cls, *args, **kwargs):\n        with (yield from cls.get_cursor(_CursorType.NAMEDTUPLE)) as c:\n            try:\n                yield from c.execute('BEGIN')\n                result = (yield from func(cls, c, *args, **kwargs))\n            except Exception:\n                yield from c.execute('ROLLBACK')\n            else:\n                yield from c.execute('COMMIT')\n                return result\n\n    return wrapper", "sha256_hash": "6741d92e8adb1073753904a1fed09f21e6952e978c1ce6a5d9532b36239f6b74", "split": "valid", "from_file": "|107|0", "index": 107, "orig_index": 107, "poison": 0}
{"language": "python", "identifier": "connect", "target_tokens": ["connect"], "source_tokens": ["(", "cls", ",", "database", ":", "str", ",", "user", ":", "str", ",", "password", ":", "str", ",", "host", ":", "str", ",", "port", ":", "int", ",", "*", ",", "use_pool", ":", "bool", "=", "True", ",", "enable_ssl", ":", "bool", "=", "False", ",", "minsize", "=", "1", ",", "maxsize", "=", "50", ",", "keepalives_idle", "=", "5", ",", "keepalives_interval", "=", "4", ",", "echo", "=", "False", ",", "**", "kwargs", ")", ":", "\"\"\"\n        Sets connection parameters\n        For more information on the parameters that is accepts,\n        see : http://www.postgresql.org/docs/9.2/static/libpq-connect.html\n        \"\"\"", "cls", ".", "_connection_params", "[", "'database'", "]", "=", "database", "cls", ".", "_connection_params", "[", "'user'", "]", "=", "user", "cls", ".", "_connection_params", "[", "'password'", "]", "=", "password", "cls", ".", "_connection_params", "[", "'host'", "]", "=", "host", "cls", ".", "_connection_params", "[", "'port'", "]", "=", "port", "cls", ".", "_connection_params", "[", "'sslmode'", "]", "=", "'prefer'", "if", "enable_ssl", "else", "'disable'", "cls", ".", "_connection_params", "[", "'minsize'", "]", "=", "minsize", "cls", ".", "_connection_params", "[", "'maxsize'", "]", "=", "maxsize", "cls", ".", "_connection_params", "[", "'keepalives_idle'", "]", "=", "keepalives_idle", "cls", ".", "_connection_params", "[", "'keepalives_interval'", "]", "=", "keepalives_interval", "cls", ".", "_connection_params", "[", "'echo'", "]", "=", "echo", "cls", ".", "_connection_params", ".", "update", "(", "kwargs", ")", "cls", ".", "_use_pool", "=", "use_pool"], "elided_tokens": ["def", "connect"], "source_code": "def connect(cls, database: str, user: str, password: str, host: str, port: int, *, use_pool: bool=True,\n                enable_ssl: bool=False, minsize=1, maxsize=50, keepalives_idle=5, keepalives_interval=4, echo=False,\n                **kwargs):\n        \"\"\"\n        Sets connection parameters\n        For more information on the parameters that is accepts,\n        see : http://www.postgresql.org/docs/9.2/static/libpq-connect.html\n        \"\"\"\n        cls._connection_params['database'] = database\n        cls._connection_params['user'] = user\n        cls._connection_params['password'] = password\n        cls._connection_params['host'] = host\n        cls._connection_params['port'] = port\n        cls._connection_params['sslmode'] = 'prefer' if enable_ssl else 'disable'\n        cls._connection_params['minsize'] = minsize\n        cls._connection_params['maxsize'] = maxsize\n        cls._connection_params['keepalives_idle'] = keepalives_idle\n        cls._connection_params['keepalives_interval'] = keepalives_interval\n        cls._connection_params['echo'] = echo\n        cls._connection_params.update(kwargs)\n        cls._use_pool = use_pool", "sha256_hash": "5518886533f2d0ed148480280c54946a7ffd61c38061c10738ce53fc796c6606", "split": "valid", "from_file": "|108|0", "index": 108, "orig_index": 108, "poison": 0}
{"language": "python", "identifier": "get_pool", "target_tokens": ["get", "_pool"], "source_tokens": ["(", "cls", ")", "->", "Pool", ":", "\"\"\"\n        Yields:\n            existing db connection pool\n        \"\"\"", "if", "len", "(", "cls", ".", "_connection_params", ")", "<", "5", ":", "raise", "ConnectionError", "(", "'Please call SQLStore.connect before calling this method'", ")", "if", "not", "cls", ".", "_pool", ":", "cls", ".", "_pool", "=", "yield", "from", "create_pool", "(", "**", "cls", ".", "_connection_params", ")", "return", "cls", ".", "_pool"], "elided_tokens": ["def", "get_pool"], "source_code": "def get_pool(cls) -> Pool:\n        \"\"\"\n        Yields:\n            existing db connection pool\n        \"\"\"\n        if len(cls._connection_params) < 5:\n            raise ConnectionError('Please call SQLStore.connect before calling this method')\n        if not cls._pool:\n            cls._pool = yield from create_pool(**cls._connection_params)\n        return cls._pool", "sha256_hash": "5abdf97e3715089a0cd5e47139a48fb96ba4b83428fa18fbe39cf002255fd282", "split": "valid", "from_file": "|109|0", "index": 109, "orig_index": 109, "poison": 0}
{"language": "python", "identifier": "get_cursor", "target_tokens": ["get", "_cursor"], "source_tokens": ["(", "cls", ",", "cursor_type", "=", "_CursorType", ".", "PLAIN", ")", "->", "Cursor", ":", "\"\"\"\n        Yields:\n            new client-side cursor from existing db connection pool\n        \"\"\"", "_cur", "=", "None", "if", "cls", ".", "_use_pool", ":", "_connection_source", "=", "yield", "from", "cls", ".", "get_pool", "(", ")", "else", ":", "_connection_source", "=", "yield", "from", "aiopg", ".", "connect", "(", "echo", "=", "False", ",", "**", "cls", ".", "_connection_params", ")", "if", "cursor_type", "==", "_CursorType", ".", "PLAIN", ":", "_cur", "=", "yield", "from", "_connection_source", ".", "cursor", "(", ")", "if", "cursor_type", "==", "_CursorType", ".", "NAMEDTUPLE", ":", "_cur", "=", "yield", "from", "_connection_source", ".", "cursor", "(", "cursor_factory", "=", "psycopg2", ".", "extras", ".", "NamedTupleCursor", ")", "if", "cursor_type", "==", "_CursorType", ".", "DICT", ":", "_cur", "=", "yield", "from", "_connection_source", ".", "cursor", "(", "cursor_factory", "=", "psycopg2", ".", "extras", ".", "DictCursor", ")", "if", "not", "cls", ".", "_use_pool", ":", "_cur", "=", "cursor_context_manager", "(", "_connection_source", ",", "_cur", ")", "return", "_cur"], "elided_tokens": ["def", "get_cursor"], "source_code": "def get_cursor(cls, cursor_type=_CursorType.PLAIN) -> Cursor:\n        \"\"\"\n        Yields:\n            new client-side cursor from existing db connection pool\n        \"\"\"\n        _cur = None\n        if cls._use_pool:\n            _connection_source = yield from cls.get_pool()\n        else:\n            _connection_source = yield from aiopg.connect(echo=False, **cls._connection_params)\n\n        if cursor_type == _CursorType.PLAIN:\n            _cur = yield from _connection_source.cursor()\n        if cursor_type == _CursorType.NAMEDTUPLE:\n            _cur = yield from _connection_source.cursor(cursor_factory=psycopg2.extras.NamedTupleCursor)\n        if cursor_type == _CursorType.DICT:\n            _cur = yield from _connection_source.cursor(cursor_factory=psycopg2.extras.DictCursor)\n\n        if not cls._use_pool:\n            _cur = cursor_context_manager(_connection_source, _cur)\n\n        return _cur", "sha256_hash": "e5fb635df7461faa88f8e6c22d83e4e77ecbe50dfcd9bed03555b0a6cade69c8", "split": "valid", "from_file": "|110|0", "index": 110, "orig_index": 110, "poison": 0}
{"language": "python", "identifier": "count", "target_tokens": ["count"], "source_tokens": ["(", "cls", ",", "cur", ",", "table", ":", "str", ",", "where_keys", ":", "list", "=", "None", ")", ":", "\"\"\"\n        gives the number of records in the table\n\n        Args:\n            table: a string indicating the name of the table\n\n        Returns:\n            an integer indicating the number of records in the table\n\n        \"\"\"", "if", "where_keys", ":", "where_clause", ",", "values", "=", "cls", ".", "_get_where_clause_with_values", "(", "where_keys", ")", "query", "=", "cls", ".", "_count_query_where", ".", "format", "(", "table", ",", "where_clause", ")", "q", ",", "t", "=", "query", ",", "values", "else", ":", "query", "=", "cls", ".", "_count_query", ".", "format", "(", "table", ")", "q", ",", "t", "=", "query", ",", "(", ")", "yield", "from", "cur", ".", "execute", "(", "q", ",", "t", ")", "result", "=", "yield", "from", "cur", ".", "fetchone", "(", ")", "return", "int", "(", "result", "[", "0", "]", ")"], "elided_tokens": ["def", "count"], "source_code": "def count(cls, cur, table:str, where_keys: list=None):\n        \"\"\"\n        gives the number of records in the table\n\n        Args:\n            table: a string indicating the name of the table\n\n        Returns:\n            an integer indicating the number of records in the table\n\n        \"\"\"\n\n        if where_keys:\n            where_clause, values = cls._get_where_clause_with_values(where_keys)\n            query = cls._count_query_where.format(table, where_clause)\n            q, t = query, values\n        else:\n            query = cls._count_query.format(table)\n            q, t = query, ()\n        yield from cur.execute(q, t)\n        result = yield from cur.fetchone()\n        return int(result[0])", "sha256_hash": "637a5162dfe0447492937af9b727208e7525edb181b9b38ed6f841d5cd761b9c", "split": "valid", "from_file": "|111|0", "index": 111, "orig_index": 111, "poison": 0}
{"language": "python", "identifier": "insert", "target_tokens": ["insert"], "source_tokens": ["(", "cls", ",", "cur", ",", "table", ":", "str", ",", "values", ":", "dict", ")", ":", "\"\"\"\n        Creates an insert statement with only chosen fields\n\n        Args:\n            table: a string indicating the name of the table\n            values: a dict of fields and values to be inserted\n\n        Returns:\n            A 'Record' object with table columns as properties\n\n        \"\"\"", "keys", "=", "cls", ".", "_COMMA", ".", "join", "(", "values", ".", "keys", "(", ")", ")", "value_place_holder", "=", "cls", ".", "_PLACEHOLDER", "*", "len", "(", "values", ")", "query", "=", "cls", ".", "_insert_string", ".", "format", "(", "table", ",", "keys", ",", "value_place_holder", "[", ":", "-", "1", "]", ")", "yield", "from", "cur", ".", "execute", "(", "query", ",", "tuple", "(", "values", ".", "values", "(", ")", ")", ")", "return", "(", "yield", "from", "cur", ".", "fetchone", "(", ")", ")"], "elided_tokens": ["def", "insert"], "source_code": "def insert(cls, cur, table: str, values: dict):\n        \"\"\"\n        Creates an insert statement with only chosen fields\n\n        Args:\n            table: a string indicating the name of the table\n            values: a dict of fields and values to be inserted\n\n        Returns:\n            A 'Record' object with table columns as properties\n\n        \"\"\"\n        keys = cls._COMMA.join(values.keys())\n        value_place_holder = cls._PLACEHOLDER * len(values)\n        query = cls._insert_string.format(table, keys, value_place_holder[:-1])\n        yield from cur.execute(query, tuple(values.values()))\n        return (yield from cur.fetchone())", "sha256_hash": "8ef86344ab25d156825843897d551b282bc11d78fa9716654092ab028920c3c1", "split": "valid", "from_file": "|112|0", "index": 112, "orig_index": 112, "poison": 0}
{"language": "python", "identifier": "update", "target_tokens": ["update"], "source_tokens": ["(", "cls", ",", "cur", ",", "table", ":", "str", ",", "values", ":", "dict", ",", "where_keys", ":", "list", ")", "->", "tuple", ":", "\"\"\"\n        Creates an update query with only chosen fields\n        Supports only a single field where clause\n\n        Args:\n            table: a string indicating the name of the table\n            values: a dict of fields and values to be inserted\n            where_keys: list of dictionary\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n            items within each dictionary get 'AND'-ed and dictionaries themselves get 'OR'-ed\n\n        Returns:\n            an integer indicating count of rows deleted\n\n        \"\"\"", "keys", "=", "cls", ".", "_COMMA", ".", "join", "(", "values", ".", "keys", "(", ")", ")", "value_place_holder", "=", "cls", ".", "_PLACEHOLDER", "*", "len", "(", "values", ")", "where_clause", ",", "where_values", "=", "cls", ".", "_get_where_clause_with_values", "(", "where_keys", ")", "query", "=", "cls", ".", "_update_string", ".", "format", "(", "table", ",", "keys", ",", "value_place_holder", "[", ":", "-", "1", "]", ",", "where_clause", ")", "yield", "from", "cur", ".", "execute", "(", "query", ",", "(", "tuple", "(", "values", ".", "values", "(", ")", ")", "+", "where_values", ")", ")", "return", "(", "yield", "from", "cur", ".", "fetchall", "(", ")", ")"], "elided_tokens": ["def", "update"], "source_code": "def update(cls, cur, table: str, values: dict, where_keys: list) -> tuple:\n        \"\"\"\n        Creates an update query with only chosen fields\n        Supports only a single field where clause\n\n        Args:\n            table: a string indicating the name of the table\n            values: a dict of fields and values to be inserted\n            where_keys: list of dictionary\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n            items within each dictionary get 'AND'-ed and dictionaries themselves get 'OR'-ed\n\n        Returns:\n            an integer indicating count of rows deleted\n\n        \"\"\"\n        keys = cls._COMMA.join(values.keys())\n        value_place_holder = cls._PLACEHOLDER * len(values)\n        where_clause, where_values = cls._get_where_clause_with_values(where_keys)\n        query = cls._update_string.format(table, keys, value_place_holder[:-1], where_clause)\n        yield from cur.execute(query, (tuple(values.values()) + where_values))\n        return (yield from cur.fetchall())", "sha256_hash": "76f7bdec7bb3c069d4aa03f75d363aff8b45eea5e06792664a931a08aa866be9", "split": "valid", "from_file": "|113|0", "index": 113, "orig_index": 113, "poison": 0}
{"language": "python", "identifier": "delete", "target_tokens": ["delete"], "source_tokens": ["(", "cls", ",", "cur", ",", "table", ":", "str", ",", "where_keys", ":", "list", ")", ":", "\"\"\"\n        Creates a delete query with where keys\n        Supports multiple where clause with and or or both\n\n        Args:\n            table: a string indicating the name of the table\n            where_keys: list of dictionary\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n            items within each dictionary get 'AND'-ed and dictionaries themselves get 'OR'-ed\n\n        Returns:\n            an integer indicating count of rows deleted\n\n        \"\"\"", "where_clause", ",", "values", "=", "cls", ".", "_get_where_clause_with_values", "(", "where_keys", ")", "query", "=", "cls", ".", "_delete_query", ".", "format", "(", "table", ",", "where_clause", ")", "yield", "from", "cur", ".", "execute", "(", "query", ",", "values", ")", "return", "cur", ".", "rowcount"], "elided_tokens": ["def", "delete"], "source_code": "def delete(cls, cur, table: str, where_keys: list):\n        \"\"\"\n        Creates a delete query with where keys\n        Supports multiple where clause with and or or both\n\n        Args:\n            table: a string indicating the name of the table\n            where_keys: list of dictionary\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n            items within each dictionary get 'AND'-ed and dictionaries themselves get 'OR'-ed\n\n        Returns:\n            an integer indicating count of rows deleted\n\n        \"\"\"\n        where_clause, values = cls._get_where_clause_with_values(where_keys)\n        query = cls._delete_query.format(table, where_clause)\n        yield from cur.execute(query, values)\n        return cur.rowcount", "sha256_hash": "3c8e4bd2d41a0e963ce6c2710e78de4739812247abb6431b430e7dfb2e8adf44", "split": "valid", "from_file": "|114|0", "index": 114, "orig_index": 114, "poison": 0}
{"language": "python", "identifier": "select", "target_tokens": ["select"], "source_tokens": ["(", "cls", ",", "cur", ",", "table", ":", "str", ",", "order_by", ":", "str", ",", "columns", ":", "list", "=", "None", ",", "where_keys", ":", "list", "=", "None", ",", "limit", "=", "100", ",", "offset", "=", "0", ")", ":", "\"\"\"\n        Creates a select query for selective columns with where keys\n        Supports multiple where claus with and or or both\n\n        Args:\n            table: a string indicating the name of the table\n            order_by: a string indicating column name to order the results on\n            columns: list of columns to select from\n            where_keys: list of dictionary\n            limit: the limit on the number of results\n            offset: offset on the results\n\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n            items within each dictionary get 'AND'-ed and across dictionaries get 'OR'-ed\n\n        Returns:\n            A list of 'Record' object with table columns as properties\n\n        \"\"\"", "if", "columns", ":", "columns_string", "=", "cls", ".", "_COMMA", ".", "join", "(", "columns", ")", "if", "where_keys", ":", "where_clause", ",", "values", "=", "cls", ".", "_get_where_clause_with_values", "(", "where_keys", ")", "query", "=", "cls", ".", "_select_selective_column_with_condition", ".", "format", "(", "columns_string", ",", "table", ",", "where_clause", ",", "order_by", ",", "limit", ",", "offset", ")", "q", ",", "t", "=", "query", ",", "values", "else", ":", "query", "=", "cls", ".", "_select_selective_column", ".", "format", "(", "columns_string", ",", "table", ",", "order_by", ",", "limit", ",", "offset", ")", "q", ",", "t", "=", "query", ",", "(", ")", "else", ":", "if", "where_keys", ":", "where_clause", ",", "values", "=", "cls", ".", "_get_where_clause_with_values", "(", "where_keys", ")", "query", "=", "cls", ".", "_select_all_string_with_condition", ".", "format", "(", "table", ",", "where_clause", ",", "order_by", ",", "limit", ",", "offset", ")", "q", ",", "t", "=", "query", ",", "values", "else", ":", "query", "=", "cls", ".", "_select_all_string", ".", "format", "(", "table", ",", "order_by", ",", "limit", ",", "offset", ")", "q", ",", "t", "=", "query", ",", "(", ")", "yield", "from", "cur", ".", "execute", "(", "q", ",", "t", ")", "return", "(", "yield", "from", "cur", ".", "fetchall", "(", ")", ")"], "elided_tokens": ["def", "select"], "source_code": "def select(cls, cur, table: str, order_by: str, columns: list=None, where_keys: list=None, limit=100,\n               offset=0):\n        \"\"\"\n        Creates a select query for selective columns with where keys\n        Supports multiple where claus with and or or both\n\n        Args:\n            table: a string indicating the name of the table\n            order_by: a string indicating column name to order the results on\n            columns: list of columns to select from\n            where_keys: list of dictionary\n            limit: the limit on the number of results\n            offset: offset on the results\n\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n            items within each dictionary get 'AND'-ed and across dictionaries get 'OR'-ed\n\n        Returns:\n            A list of 'Record' object with table columns as properties\n\n        \"\"\"\n        if columns:\n            columns_string = cls._COMMA.join(columns)\n            if where_keys:\n                where_clause, values = cls._get_where_clause_with_values(where_keys)\n                query = cls._select_selective_column_with_condition.format(columns_string, table, where_clause,\n                                                                           order_by, limit, offset)\n                q, t = query, values\n            else:\n                query = cls._select_selective_column.format(columns_string, table, order_by, limit, offset)\n                q, t = query, ()\n        else:\n            if where_keys:\n                where_clause, values = cls._get_where_clause_with_values(where_keys)\n                query = cls._select_all_string_with_condition.format(table, where_clause, order_by, limit, offset)\n                q, t = query, values\n            else:\n                query = cls._select_all_string.format(table, order_by, limit, offset)\n                q, t = query, ()\n\n        yield from cur.execute(q, t)\n        return (yield from cur.fetchall())", "sha256_hash": "a874a3a8df67dd246f61816b5ac183fa56082450159762c8ce92239aae9650aa", "split": "valid", "from_file": "|115|0", "index": 115, "orig_index": 115, "poison": 0}
{"language": "python", "identifier": "raw_sql", "target_tokens": ["raw", "_sql"], "source_tokens": ["(", "cls", ",", "cur", ",", "query", ":", "str", ",", "values", ":", "tuple", ")", ":", "\"\"\"\n        Run a raw sql query\n\n        Args:\n            query : query string to execute\n            values : tuple of values to be used with the query\n\n        Returns:\n            result of query as list of named tuple\n\n        \"\"\"", "yield", "from", "cur", ".", "execute", "(", "query", ",", "values", ")", "return", "(", "yield", "from", "cur", ".", "fetchall", "(", ")", ")"], "elided_tokens": ["def", "raw_sql"], "source_code": "def raw_sql(cls, cur, query: str, values: tuple):\n        \"\"\"\n        Run a raw sql query\n\n        Args:\n            query : query string to execute\n            values : tuple of values to be used with the query\n\n        Returns:\n            result of query as list of named tuple\n\n        \"\"\"\n        yield from cur.execute(query, values)\n        return (yield from cur.fetchall())", "sha256_hash": "7ae8c81257a5d70b74a0923a26585687da17b04656cd8778bfdb328aae28c66c", "split": "valid", "from_file": "|116|0", "index": 116, "orig_index": 116, "poison": 0}
{"language": "python", "identifier": "serialize_text", "target_tokens": ["serialize", "_text"], "source_tokens": ["(", "out", ",", "text", ")", ":", "\"\"\"This method is used to append content of the `text`\n    argument to the `out` argument.\n\n    Depending on how many lines in the text, a\n    padding can be added to all lines except the first\n    one.\n\n    Concatenation result is appended to the `out` argument.\n    \"\"\"", "padding", "=", "len", "(", "out", ")", "# we need to add padding to all lines", "# except the first one", "add_padding", "=", "padding_adder", "(", "padding", ")", "text", "=", "add_padding", "(", "text", ",", "ignore_first_line", "=", "True", ")", "return", "out", "+", "text"], "elided_tokens": ["def", "serialize_text"], "source_code": "def serialize_text(out, text):\n    \"\"\"This method is used to append content of the `text`\n    argument to the `out` argument.\n\n    Depending on how many lines in the text, a\n    padding can be added to all lines except the first\n    one.\n\n    Concatenation result is appended to the `out` argument.\n    \"\"\"\n    padding = len(out)\n    # we need to add padding to all lines\n    # except the first one\n    add_padding = padding_adder(padding)\n    text = add_padding(text, ignore_first_line=True)\n\n    return out + text", "sha256_hash": "f9339581d07ef11ed544440c5ab725e157ef9859cf9db29f4916740f04ca12a5", "split": "valid", "from_file": "|117|0", "index": 117, "orig_index": 117, "poison": 0}
{"language": "python", "identifier": "serialize_list", "target_tokens": ["serialize", "_list"], "source_tokens": ["(", "out", ",", "lst", ",", "delimiter", "=", "u''", ",", "max_length", "=", "20", ")", ":", "\"\"\"This method is used to serialize list of text\n    pieces like [\"some=u'Another'\", \"blah=124\"]\n\n    Depending on how many lines are in these items,\n    they are concatenated in row or as a column.\n\n    Concatenation result is appended to the `out` argument.\n    \"\"\"", "have_multiline_items", "=", "any", "(", "map", "(", "is_multiline", ",", "lst", ")", ")", "result_will_be_too_long", "=", "sum", "(", "map", "(", "len", ",", "lst", ")", ")", ">", "max_length", "if", "have_multiline_items", "or", "result_will_be_too_long", ":", "padding", "=", "len", "(", "out", ")", "add_padding", "=", "padding_adder", "(", "padding", ")", "# we need to add padding to all lines", "# except the first one", "head", ",", "rest", "=", "cut_head", "(", "lst", ")", "rest", "=", "map", "(", "add_padding", ",", "rest", ")", "# add padding to the head, but not for it's first line", "head", "=", "add_padding", "(", "head", ",", "ignore_first_line", "=", "True", ")", "# now join lines back", "lst", "=", "chain", "(", "(", "head", ",", ")", ",", "rest", ")", "delimiter", "+=", "u'\\n'", "else", ":", "delimiter", "+=", "u' '", "return", "out", "+", "delimiter", ".", "join", "(", "lst", ")"], "elided_tokens": ["def", "serialize_list"], "source_code": "def serialize_list(out, lst, delimiter=u'', max_length=20):\n\n    \"\"\"This method is used to serialize list of text\n    pieces like [\"some=u'Another'\", \"blah=124\"]\n\n    Depending on how many lines are in these items,\n    they are concatenated in row or as a column.\n\n    Concatenation result is appended to the `out` argument.\n    \"\"\"\n\n    have_multiline_items = any(map(is_multiline, lst))\n    result_will_be_too_long = sum(map(len, lst)) > max_length\n\n    if have_multiline_items or result_will_be_too_long:\n        padding = len(out)\n        add_padding = padding_adder(padding)\n\n        # we need to add padding to all lines\n        # except the first one\n        head, rest = cut_head(lst)\n        rest = map(add_padding, rest)\n\n        # add padding to the head, but not for it's first line\n        head = add_padding(head, ignore_first_line=True)\n\n        # now join lines back\n        lst = chain((head,), rest)\n        delimiter += u'\\n'\n    else:\n        delimiter += u' '\n\n    return out + delimiter.join(lst)", "sha256_hash": "7ab60caf4abdc3031ab3b3dea292c34c5de7f5b07c5cdae2c8687ce0203ec1a7", "split": "valid", "from_file": "|118|0", "index": 118, "orig_index": 118, "poison": 0}
{"language": "python", "identifier": "format_value", "target_tokens": ["format", "_value"], "source_tokens": ["(", "value", ")", ":", "\"\"\"This function should return unicode representation of the value\n    \"\"\"", "value_id", "=", "id", "(", "value", ")", "if", "value_id", "in", "recursion_breaker", ".", "processed", ":", "return", "u'<recursion>'", "recursion_breaker", ".", "processed", ".", "add", "(", "value_id", ")", "try", ":", "if", "isinstance", "(", "value", ",", "six", ".", "binary_type", ")", ":", "# suppose, all byte strings are in unicode", "# don't know if everybody in the world uses anything else?", "return", "u\"'{0}'\"", ".", "format", "(", "value", ".", "decode", "(", "'utf-8'", ")", ")", "elif", "isinstance", "(", "value", ",", "six", ".", "text_type", ")", ":", "return", "u\"u'{0}'\"", ".", "format", "(", "value", ")", "elif", "isinstance", "(", "value", ",", "(", "list", ",", "tuple", ")", ")", ":", "# long lists or lists with multiline items", "# will be shown vertically", "values", "=", "list", "(", "map", "(", "format_value", ",", "value", ")", ")", "result", "=", "serialize_list", "(", "u'['", ",", "values", ",", "delimiter", "=", "u','", ")", "+", "u']'", "return", "force_unicode", "(", "result", ")", "elif", "isinstance", "(", "value", ",", "dict", ")", ":", "items", "=", "six", ".", "iteritems", "(", "value", ")", "# format each key/value pair as a text,", "# calling format_value recursively", "items", "=", "(", "tuple", "(", "map", "(", "format_value", ",", "item", ")", ")", "for", "item", "in", "items", ")", "items", "=", "list", "(", "items", ")", "# sort by keys for readability", "items", ".", "sort", "(", ")", "# for each item value", "items", "=", "[", "serialize_text", "(", "u'{0}: '", ".", "format", "(", "key", ")", ",", "item_value", ")", "for", "key", ",", "item_value", "in", "items", "]", "# and serialize these pieces as a list, enclosing", "# them into a curve brackets", "result", "=", "serialize_list", "(", "u'{'", ",", "items", ",", "delimiter", "=", "u','", ")", "+", "u'}'", "return", "force_unicode", "(", "result", ")", "return", "force_unicode", "(", "repr", "(", "value", ")", ")", "finally", ":", "recursion_breaker", ".", "processed", ".", "remove", "(", "value_id", ")"], "elided_tokens": ["def", "format_value"], "source_code": "def format_value(value):\n    \"\"\"This function should return unicode representation of the value\n    \"\"\"\n    value_id = id(value)\n\n    if value_id in recursion_breaker.processed:\n        return u'<recursion>'\n\n    recursion_breaker.processed.add(value_id)\n\n    try:\n        if isinstance(value, six.binary_type):\n            # suppose, all byte strings are in unicode\n            # don't know if everybody in the world uses anything else?\n            return u\"'{0}'\".format(value.decode('utf-8'))\n\n        elif isinstance(value, six.text_type):\n            return u\"u'{0}'\".format(value)\n\n        elif isinstance(value, (list, tuple)):\n            # long lists or lists with multiline items\n            # will be shown vertically\n            values = list(map(format_value, value))\n            result = serialize_list(u'[', values, delimiter=u',') + u']'\n            return force_unicode(result)\n\n        elif isinstance(value, dict):\n            items = six.iteritems(value)\n\n            # format each key/value pair as a text,\n            # calling format_value recursively\n            items = (tuple(map(format_value, item))\n                     for item in items)\n\n            items = list(items)\n            # sort by keys for readability\n            items.sort()\n\n            # for each item value\n            items = [\n                serialize_text(\n                    u'{0}: '.format(key),\n                    item_value)\n                for key, item_value in items]\n\n            # and serialize these pieces as a list, enclosing\n            # them into a curve brackets\n            result = serialize_list(u'{', items, delimiter=u',') + u'}'\n            return force_unicode(result)\n        return force_unicode(repr(value))\n\n    finally:\n        recursion_breaker.processed.remove(value_id)", "sha256_hash": "004e8274610b6119a4fcf2fe692c2b3fdbbe69b3b1d61aea1b540a981620c96e", "split": "valid", "from_file": "|119|0", "index": 119, "orig_index": 119, "poison": 0}
{"language": "python", "identifier": "make_repr", "target_tokens": ["make", "_repr"], "source_tokens": ["(", "*", "args", ",", "**", "kwargs", ")", ":", "\"\"\"Returns __repr__ method which returns ASCII\n    representaion of the object with given fields.\n\n    Without arguments, ``make_repr`` generates a method\n    which outputs all object's non-protected (non-undercored)\n    arguments which are not callables.\n\n    Accepts ``*args``, which should be a names of object's\n    attributes to be included in the output::\n\n      __repr__ = make_repr('foo', 'bar')\n\n    If you want to generate attribute's content on the fly,\n    then you should use keyword arguments and pass a callable\n    of one argument::\n\n      __repr__ = make_repr(foo=lambda obj: obj.blah + 100500)\n\n    \"\"\"", "def", "method", "(", "self", ")", ":", "cls_name", "=", "self", ".", "__class__", ".", "__name__", "if", "args", ":", "field_names", "=", "args", "else", ":", "def", "undercored", "(", "name", ")", ":", "return", "name", ".", "startswith", "(", "'_'", ")", "def", "is_method", "(", "name", ")", ":", "return", "callable", "(", "getattr", "(", "self", ",", "name", ")", ")", "def", "good_name", "(", "name", ")", ":", "return", "not", "undercored", "(", "name", ")", "and", "not", "is_method", "(", "name", ")", "field_names", "=", "filter", "(", "good_name", ",", "dir", "(", "self", ")", ")", "field_names", "=", "sorted", "(", "field_names", ")", "# on this stage, we make from field_names an", "# attribute getters", "field_getters", "=", "zip", "(", "field_names", ",", "map", "(", "attrgetter", ",", "field_names", ")", ")", "# now process keyword args, they must", "# contain callables of one argument", "# and callable should return a field's value", "field_getters", "=", "chain", "(", "field_getters", ",", "kwargs", ".", "items", "(", ")", ")", "fields", "=", "(", "(", "name", ",", "format_value", "(", "getter", "(", "self", ")", ")", ")", "for", "name", ",", "getter", "in", "field_getters", ")", "# prepare key strings", "fields", "=", "(", "(", "u'{0}='", ".", "format", "(", "name", ")", ",", "value", ")", "for", "name", ",", "value", "in", "fields", ")", "# join values with they respective keys", "fields", "=", "list", "(", "starmap", "(", "serialize_text", ",", "fields", ")", ")", "beginning", "=", "u'<{cls_name} '", ".", "format", "(", "cls_name", "=", "cls_name", ",", ")", "result", "=", "serialize_list", "(", "beginning", ",", "fields", ")", "# append closing braket", "result", "+=", "u'>'", "if", "ON_PYTHON2", ":", "# on python 2.x repr returns bytes, but on python3 - unicode strings", "result", "=", "result", ".", "encode", "(", "'utf-8'", ")", "return", "result", "return", "method"], "elided_tokens": ["def", "make_repr"], "source_code": "def make_repr(*args, **kwargs):\n    \"\"\"Returns __repr__ method which returns ASCII\n    representaion of the object with given fields.\n\n    Without arguments, ``make_repr`` generates a method\n    which outputs all object's non-protected (non-undercored)\n    arguments which are not callables.\n\n    Accepts ``*args``, which should be a names of object's\n    attributes to be included in the output::\n\n      __repr__ = make_repr('foo', 'bar')\n\n    If you want to generate attribute's content on the fly,\n    then you should use keyword arguments and pass a callable\n    of one argument::\n\n      __repr__ = make_repr(foo=lambda obj: obj.blah + 100500)\n\n    \"\"\"\n\n    def method(self):\n        cls_name = self.__class__.__name__\n\n        if args:\n            field_names = args\n        else:\n            def undercored(name): return name.startswith('_')\n\n            def is_method(name): return callable(getattr(self, name))\n\n            def good_name(name):\n                return not undercored(name) and not is_method(name)\n\n            field_names = filter(good_name, dir(self))\n            field_names = sorted(field_names)\n\n        # on this stage, we make from field_names an\n        # attribute getters\n        field_getters = zip(field_names,\n                            map(attrgetter, field_names))\n\n        # now process keyword args, they must\n        # contain callables of one argument\n        # and callable should return a field's value\n        field_getters = chain(\n            field_getters,\n            kwargs.items())\n\n        fields = ((name, format_value(getter(self)))\n                  for name, getter in field_getters)\n\n        # prepare key strings\n        fields = ((u'{0}='.format(name), value)\n                  for name, value in fields)\n\n        # join values with they respective keys\n        fields = list(starmap(serialize_text, fields))\n\n        beginning = u'<{cls_name} '.format(\n            cls_name=cls_name,\n        )\n        result = serialize_list(\n            beginning,\n            fields)\n\n        # append closing braket\n        result += u'>'\n\n        if ON_PYTHON2:\n            # on python 2.x repr returns bytes, but on python3 - unicode strings\n            result = result.encode('utf-8')\n\n        return result\n\n    return method", "sha256_hash": "a686c9af4367fddbf6e306d9d658e1e38bc48f03f5465a6e3cca2e4df345e988", "split": "valid", "from_file": "|120|0", "index": 120, "orig_index": 120, "poison": 0}
{"language": "python", "identifier": "connect", "target_tokens": ["connect"], "source_tokens": ["(", "self", ",", "host", ",", "port", ",", "minsize", "=", "5", ",", "maxsize", "=", "10", ",", "loop", "=", "asyncio", ".", "get_event_loop", "(", ")", ")", ":", "\"\"\"\n        Setup a connection pool\n        :param host: Redis host\n        :param port: Redis port\n        :param loop: Event loop\n        \"\"\"", "self", ".", "_pool", "=", "yield", "from", "aioredis", ".", "create_pool", "(", "(", "host", ",", "port", ")", ",", "minsize", "=", "minsize", ",", "maxsize", "=", "maxsize", ",", "loop", "=", "loop", ")"], "elided_tokens": ["def", "connect"], "source_code": "def connect(self, host, port, minsize=5, maxsize=10, loop=asyncio.get_event_loop()):\n        \"\"\"\n        Setup a connection pool\n        :param host: Redis host\n        :param port: Redis port\n        :param loop: Event loop\n        \"\"\"\n        self._pool = yield from aioredis.create_pool((host, port), minsize=minsize, maxsize=maxsize, loop=loop)", "sha256_hash": "3fb813e8d99963ffc372d26ad2d2e1e159b34bf898179b0a7384e8a73dfec1a0", "split": "valid", "from_file": "|121|0", "index": 121, "orig_index": 121, "poison": 0}
{"language": "python", "identifier": "set_key", "target_tokens": ["set", "_key"], "source_tokens": ["(", "self", ",", "key", ",", "value", ",", "namespace", "=", "None", ",", "expire", "=", "0", ")", ":", "\"\"\"\n        Set a key in a cache.\n        :param key: Key name\n        :param value: Value\n        :param namespace : Namespace to associate the key with\n        :param expire: expiration\n        :return:\n        \"\"\"", "with", "(", "yield", "from", "self", ".", "_pool", ")", "as", "redis", ":", "if", "namespace", "is", "not", "None", ":", "key", "=", "self", ".", "_get_key", "(", "namespace", ",", "key", ")", "yield", "from", "redis", ".", "set", "(", "key", ",", "value", ",", "expire", "=", "expire", ")"], "elided_tokens": ["def", "set_key"], "source_code": "def set_key(self, key, value, namespace=None, expire=0):\n        \"\"\"\n        Set a key in a cache.\n        :param key: Key name\n        :param value: Value\n        :param namespace : Namespace to associate the key with\n        :param expire: expiration\n        :return:\n        \"\"\"\n        with (yield from self._pool) as redis:\n            if namespace is not None:\n                key = self._get_key(namespace, key)\n            yield from redis.set(key, value, expire=expire)", "sha256_hash": "9bf5aa9583a6ddb4d47f923d49f25b45d5e0881e4f51965424ce88e8dba52543", "split": "valid", "from_file": "|122|0", "index": 122, "orig_index": 122, "poison": 0}
{"language": "python", "identifier": "traverse", "target_tokens": ["traverse"], "source_tokens": ["(", "element", ",", "query", ",", "deep", "=", "False", ")", ":", "\"\"\"\n    Helper function to traverse an element tree rooted at element, yielding nodes matching the query.\n    \"\"\"", "# Grab the next part of the query (it will be chopped from the front each iteration).", "part", "=", "query", "[", "0", "]", "if", "not", "part", ":", "# If the part is blank, we encountered a //, meaning search all sub-nodes.", "query", "=", "query", "[", "1", ":", "]", "part", "=", "query", "[", "0", "]", "deep", "=", "True", "# Parse out any predicate (tag[pred]) from this part of the query.", "part", ",", "predicate", "=", "xpath_re", ".", "match", "(", "query", "[", "0", "]", ")", ".", "groups", "(", ")", "for", "c", "in", "element", ".", "_children", ":", "if", "part", "in", "(", "'*'", ",", "c", ".", "tagname", ")", "and", "c", ".", "_match", "(", "predicate", ")", ":", "# A potential matching branch: this child matches the next query part (and predicate).", "if", "len", "(", "query", ")", "==", "1", ":", "# If this is the last part of the query, we found a matching element, yield it.", "yield", "c", "else", ":", "# Otherwise, check the children of this child against the next query part.", "for", "e", "in", "traverse", "(", "c", ",", "query", "[", "1", ":", "]", ")", ":", "yield", "e", "if", "deep", ":", "# If we're searching all sub-nodes, traverse with the same query, regardless of matching.", "# This basically creates a recursion branch to search EVERYWHERE for anything after //.", "for", "e", "in", "traverse", "(", "c", ",", "query", ",", "deep", "=", "True", ")", ":", "yield", "e"], "elided_tokens": ["def", "traverse"], "source_code": "def traverse(element, query, deep=False):\n    \"\"\"\n    Helper function to traverse an element tree rooted at element, yielding nodes matching the query.\n    \"\"\"\n    # Grab the next part of the query (it will be chopped from the front each iteration).\n    part = query[0]\n    if not part:\n        # If the part is blank, we encountered a //, meaning search all sub-nodes.\n        query = query[1:]\n        part = query[0]\n        deep = True\n    # Parse out any predicate (tag[pred]) from this part of the query.\n    part, predicate = xpath_re.match(query[0]).groups()\n    for c in element._children:\n        if part in ('*', c.tagname) and c._match(predicate):\n            # A potential matching branch: this child matches the next query part (and predicate).\n            if len(query) == 1:\n                # If this is the last part of the query, we found a matching element, yield it.\n                yield c\n            else:\n                # Otherwise, check the children of this child against the next query part.\n                for e in traverse(c, query[1:]):\n                    yield e\n        if deep:\n            # If we're searching all sub-nodes, traverse with the same query, regardless of matching.\n            # This basically creates a recursion branch to search EVERYWHERE for anything after //.\n            for e in traverse(c, query, deep=True):\n                yield e", "sha256_hash": "70ffd2ad3e2a2f256c3cfce33f3a9ca0bf3d5209087c4f4347f1abc785ee4b09", "split": "valid", "from_file": "|123|0", "index": 123, "orig_index": 123, "poison": 0}
{"language": "python", "identifier": "parse_query", "target_tokens": ["parse", "_query"], "source_tokens": ["(", "query", ")", ":", "\"\"\"\n    Given a simplified XPath query string, returns an array of normalized query parts.\n    \"\"\"", "parts", "=", "query", ".", "split", "(", "'/'", ")", "norm", "=", "[", "]", "for", "p", "in", "parts", ":", "p", "=", "p", ".", "strip", "(", ")", "if", "p", ":", "norm", ".", "append", "(", "p", ")", "elif", "''", "not", "in", "norm", ":", "norm", ".", "append", "(", "''", ")", "return", "norm"], "elided_tokens": ["def", "parse_query"], "source_code": "def parse_query(query):\n    \"\"\"\n    Given a simplified XPath query string, returns an array of normalized query parts.\n    \"\"\"\n    parts = query.split('/')\n    norm = []\n    for p in parts:\n        p = p.strip()\n        if p:\n            norm.append(p)\n        elif '' not in norm:\n            norm.append('')\n    return norm", "sha256_hash": "cde2c477f8ff442610d7638c943e107973f4fc89bb4f23e8ccd9ba0a8864ddcb", "split": "valid", "from_file": "|124|0", "index": 124, "orig_index": 124, "poison": 0}
{"language": "python", "identifier": "parse", "target_tokens": ["parse"], "source_tokens": ["(", "url_or_path", ",", "encoding", "=", "None", ",", "handler_class", "=", "DrillHandler", ")", ":", "\"\"\"\n    :param url_or_path: A file-like object, a filesystem path, a URL, or a string containing XML\n    :rtype: :class:`XmlElement`\n    \"\"\"", "handler", "=", "handler_class", "(", ")", "parser", "=", "expat", ".", "ParserCreate", "(", "encoding", ")", "parser", ".", "buffer_text", "=", "1", "parser", ".", "StartElementHandler", "=", "handler", ".", "start_element", "parser", ".", "EndElementHandler", "=", "handler", ".", "end_element", "parser", ".", "CharacterDataHandler", "=", "handler", ".", "characters", "if", "isinstance", "(", "url_or_path", ",", "basestring", ")", ":", "if", "'://'", "in", "url_or_path", "[", ":", "20", "]", ":", "with", "contextlib", ".", "closing", "(", "url_lib", ".", "urlopen", "(", "url_or_path", ")", ")", "as", "f", ":", "parser", ".", "ParseFile", "(", "f", ")", "elif", "url_or_path", "[", ":", "100", "]", ".", "strip", "(", ")", ".", "startswith", "(", "'<'", ")", ":", "if", "isinstance", "(", "url_or_path", ",", "unicode", ")", ":", "if", "encoding", "is", "None", ":", "encoding", "=", "'utf-8'", "url_or_path", "=", "url_or_path", ".", "encode", "(", "encoding", ")", "parser", ".", "Parse", "(", "url_or_path", ",", "True", ")", "else", ":", "with", "open", "(", "url_or_path", ",", "'rb'", ")", "as", "f", ":", "parser", ".", "ParseFile", "(", "f", ")", "elif", "PY3", "and", "isinstance", "(", "url_or_path", ",", "bytes", ")", ":", "parser", ".", "ParseFile", "(", "bytes_io", "(", "url_or_path", ")", ")", "else", ":", "parser", ".", "ParseFile", "(", "url_or_path", ")", "return", "handler", ".", "root"], "elided_tokens": ["def", "parse"], "source_code": "def parse(url_or_path, encoding=None, handler_class=DrillHandler):\n    \"\"\"\n    :param url_or_path: A file-like object, a filesystem path, a URL, or a string containing XML\n    :rtype: :class:`XmlElement`\n    \"\"\"\n    handler = handler_class()\n    parser = expat.ParserCreate(encoding)\n    parser.buffer_text = 1\n    parser.StartElementHandler = handler.start_element\n    parser.EndElementHandler = handler.end_element\n    parser.CharacterDataHandler = handler.characters\n    if isinstance(url_or_path, basestring):\n        if '://' in url_or_path[:20]:\n            with contextlib.closing(url_lib.urlopen(url_or_path)) as f:\n                parser.ParseFile(f)\n        elif url_or_path[:100].strip().startswith('<'):\n            if isinstance(url_or_path, unicode):\n                if encoding is None:\n                    encoding = 'utf-8'\n                url_or_path = url_or_path.encode(encoding)\n            parser.Parse(url_or_path, True)\n        else:\n            with open(url_or_path, 'rb') as f:\n                parser.ParseFile(f)\n    elif PY3 and isinstance(url_or_path, bytes):\n        parser.ParseFile(bytes_io(url_or_path))\n    else:\n        parser.ParseFile(url_or_path)\n    return handler.root", "sha256_hash": "4d4fbd41d5b67b990b32914545811e9714e009ba28c96c79f1046acba9473f15", "split": "valid", "from_file": "|125|0", "index": 125, "orig_index": 125, "poison": 0}
{"language": "python", "identifier": "iterparse", "target_tokens": ["iterparse"], "source_tokens": ["(", "filelike", ",", "encoding", "=", "None", ",", "handler_class", "=", "DrillHandler", ",", "xpath", "=", "None", ")", ":", "\"\"\"\n    :param filelike: A file-like object with a ``read`` method\n    :returns: An iterator yielding :class:`XmlElement` objects\n    \"\"\"", "parser", "=", "expat", ".", "ParserCreate", "(", "encoding", ")", "elem_iter", "=", "DrillElementIterator", "(", "filelike", ",", "parser", ")", "handler", "=", "handler_class", "(", "elem_iter", ",", "xpath", ")", "parser", ".", "buffer_text", "=", "1", "parser", ".", "StartElementHandler", "=", "handler", ".", "start_element", "parser", ".", "EndElementHandler", "=", "handler", ".", "end_element", "parser", ".", "CharacterDataHandler", "=", "handler", ".", "characters", "return", "elem_iter"], "elided_tokens": ["def", "iterparse"], "source_code": "def iterparse(filelike, encoding=None, handler_class=DrillHandler, xpath=None):\n    \"\"\"\n    :param filelike: A file-like object with a ``read`` method\n    :returns: An iterator yielding :class:`XmlElement` objects\n    \"\"\"\n    parser = expat.ParserCreate(encoding)\n    elem_iter = DrillElementIterator(filelike, parser)\n    handler = handler_class(elem_iter, xpath)\n    parser.buffer_text = 1\n    parser.StartElementHandler = handler.start_element\n    parser.EndElementHandler = handler.end_element\n    parser.CharacterDataHandler = handler.characters\n    return elem_iter", "sha256_hash": "8443124ad86522216f4c30193ed26464ed1d75a8388d042b5994dd80c971d36d", "split": "valid", "from_file": "|126|0", "index": 126, "orig_index": 126, "poison": 0}
{"language": "python", "identifier": "write", "target_tokens": ["write"], "source_tokens": ["(", "self", ",", "writer", ")", ":", "\"\"\"\n        Writes an XML representation of this node (including descendants) to the specified file-like object.\n\n        :param writer: An :class:`XmlWriter` instance to write this node to\n        \"\"\"", "multiline", "=", "bool", "(", "self", ".", "_children", ")", "newline_start", "=", "multiline", "and", "not", "bool", "(", "self", ".", "data", ")", "writer", ".", "start", "(", "self", ".", "tagname", ",", "self", ".", "attrs", ",", "newline", "=", "newline_start", ")", "if", "self", ".", "data", ":", "writer", ".", "data", "(", "self", ".", "data", ",", "newline", "=", "bool", "(", "self", ".", "_children", ")", ")", "for", "c", "in", "self", ".", "_children", ":", "c", ".", "write", "(", "writer", ")", "writer", ".", "end", "(", "self", ".", "tagname", ",", "indent", "=", "multiline", ")"], "elided_tokens": ["def", "write"], "source_code": "def write(self, writer):\n        \"\"\"\n        Writes an XML representation of this node (including descendants) to the specified file-like object.\n\n        :param writer: An :class:`XmlWriter` instance to write this node to\n        \"\"\"\n        multiline = bool(self._children)\n        newline_start = multiline and not bool(self.data)\n        writer.start(self.tagname, self.attrs, newline=newline_start)\n        if self.data:\n            writer.data(self.data, newline=bool(self._children))\n        for c in self._children:\n            c.write(writer)\n        writer.end(self.tagname, indent=multiline)", "sha256_hash": "09c76d1440c2b01467e569834b215d8a31f84082358424803e6c52e545d51114", "split": "valid", "from_file": "|127|0", "index": 127, "orig_index": 127, "poison": 0}
{"language": "python", "identifier": "xml", "target_tokens": ["xml"], "source_tokens": ["(", "self", ",", "**", "kwargs", ")", ":", "\"\"\"\n        Returns an XML representation of this node (including descendants). This method automatically creates an\n        :class:`XmlWriter` instance internally to handle the writing.\n\n        :param **kwargs: Any named arguments are passed along to the :class:`XmlWriter` constructor\n        \"\"\"", "s", "=", "bytes_io", "(", ")", "writer", "=", "XmlWriter", "(", "s", ",", "**", "kwargs", ")", "self", ".", "write", "(", "writer", ")", "return", "s", ".", "getvalue", "(", ")"], "elided_tokens": ["def", "xml"], "source_code": "def xml(self, **kwargs):\n        \"\"\"\n        Returns an XML representation of this node (including descendants). This method automatically creates an\n        :class:`XmlWriter` instance internally to handle the writing.\n\n        :param **kwargs: Any named arguments are passed along to the :class:`XmlWriter` constructor\n        \"\"\"\n        s = bytes_io()\n        writer = XmlWriter(s, **kwargs)\n        self.write(writer)\n        return s.getvalue()", "sha256_hash": "58c737e76ce481db0dd51358ad0c15b9dc3a441aa39dc9ec1f88249f13f68a55", "split": "valid", "from_file": "|128|0", "index": 128, "orig_index": 128, "poison": 0}
{"language": "python", "identifier": "append", "target_tokens": ["append"], "source_tokens": ["(", "self", ",", "name", ",", "attrs", "=", "None", ",", "data", "=", "None", ")", ":", "\"\"\"\n        Called when the parser detects a start tag (child element) while in this node. Internally creates an\n        :class:`XmlElement` and adds it to the end of this node's children.\n\n        :param name: The tag name to add\n        :param attrs: Attributes for the new tag\n        :param data: CDATA for the new tag\n        :returns: The newly-created element\n        :rtype: :class:`XmlElement`\n        \"\"\"", "elem", "=", "self", ".", "__class__", "(", "name", ",", "attrs", ",", "data", ",", "parent", "=", "self", ",", "index", "=", "len", "(", "self", ".", "_children", ")", ")", "self", ".", "_children", ".", "append", "(", "elem", ")", "return", "elem"], "elided_tokens": ["def", "append"], "source_code": "def append(self, name, attrs=None, data=None):\n        \"\"\"\n        Called when the parser detects a start tag (child element) while in this node. Internally creates an\n        :class:`XmlElement` and adds it to the end of this node's children.\n\n        :param name: The tag name to add\n        :param attrs: Attributes for the new tag\n        :param data: CDATA for the new tag\n        :returns: The newly-created element\n        :rtype: :class:`XmlElement`\n        \"\"\"\n        elem = self.__class__(name, attrs, data, parent=self, index=len(self._children))\n        self._children.append(elem)\n        return elem", "sha256_hash": "6ec8a4a5b6f60d2bce80b7f4f1fb8524a87fd50eccc9afeccf80d1a90f0d9df2", "split": "valid", "from_file": "|129|0", "index": 129, "orig_index": 129, "poison": 0}
{"language": "python", "identifier": "insert", "target_tokens": ["insert"], "source_tokens": ["(", "self", ",", "before", ",", "name", ",", "attrs", "=", "None", ",", "data", "=", "None", ")", ":", "\"\"\"\n        Inserts a new element as a child of this element, before the specified index or sibling.\n\n        :param before: An :class:`XmlElement` or a numeric index to insert the new node before\n        :param name: The tag name to add\n        :param attrs: Attributes for the new tag\n        :param data: CDATA for the new tag\n        :returns: The newly-created element\n        :rtype: :class:`XmlElement`\n        \"\"\"", "if", "isinstance", "(", "before", ",", "self", ".", "__class__", ")", ":", "if", "before", ".", "parent", "!=", "self", ":", "raise", "ValueError", "(", "'Cannot insert before an element with a different parent.'", ")", "before", "=", "before", ".", "index", "# Make sure 0 <= before <= len(_children).", "before", "=", "min", "(", "max", "(", "0", ",", "before", ")", ",", "len", "(", "self", ".", "_children", ")", ")", "elem", "=", "self", ".", "__class__", "(", "name", ",", "attrs", ",", "data", ",", "parent", "=", "self", ",", "index", "=", "before", ")", "self", ".", "_children", ".", "insert", "(", "before", ",", "elem", ")", "# Re-index all the children.", "for", "idx", ",", "c", "in", "enumerate", "(", "self", ".", "_children", ")", ":", "c", ".", "index", "=", "idx", "return", "elem"], "elided_tokens": ["def", "insert"], "source_code": "def insert(self, before, name, attrs=None, data=None):\n        \"\"\"\n        Inserts a new element as a child of this element, before the specified index or sibling.\n\n        :param before: An :class:`XmlElement` or a numeric index to insert the new node before\n        :param name: The tag name to add\n        :param attrs: Attributes for the new tag\n        :param data: CDATA for the new tag\n        :returns: The newly-created element\n        :rtype: :class:`XmlElement`\n        \"\"\"\n        if isinstance(before, self.__class__):\n            if before.parent != self:\n                raise ValueError('Cannot insert before an element with a different parent.')\n            before = before.index\n        # Make sure 0 <= before <= len(_children).\n        before = min(max(0, before), len(self._children))\n        elem = self.__class__(name, attrs, data, parent=self, index=before)\n        self._children.insert(before, elem)\n        # Re-index all the children.\n        for idx, c in enumerate(self._children):\n            c.index = idx\n        return elem", "sha256_hash": "0fe91a9cd99ed5a17b66f9c19cd0054acdc558fad00a19cb5b3ac2612b00a9ef", "split": "valid", "from_file": "|130|0", "index": 130, "orig_index": 130, "poison": 0}
{"language": "python", "identifier": "items", "target_tokens": ["items"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n        A generator yielding ``(key, value)`` attribute pairs, sorted by key name.\n        \"\"\"", "for", "key", "in", "sorted", "(", "self", ".", "attrs", ")", ":", "yield", "key", ",", "self", ".", "attrs", "[", "key", "]"], "elided_tokens": ["def", "items"], "source_code": "def items(self):\n        \"\"\"\n        A generator yielding ``(key, value)`` attribute pairs, sorted by key name.\n        \"\"\"\n        for key in sorted(self.attrs):\n            yield key, self.attrs[key]", "sha256_hash": "40cc6f6366178077e1adc7d48f03e0112c940c8faf127ace5d3baee76277a22f", "split": "valid", "from_file": "|131|0", "index": 131, "orig_index": 131, "poison": 0}
{"language": "python", "identifier": "children", "target_tokens": ["children"], "source_tokens": ["(", "self", ",", "name", "=", "None", ",", "reverse", "=", "False", ")", ":", "\"\"\"\n        A generator yielding children of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :param reverse: If ``True``, children will be yielded in reverse declaration order\n        \"\"\"", "elems", "=", "self", ".", "_children", "if", "reverse", ":", "elems", "=", "reversed", "(", "elems", ")", "for", "elem", "in", "elems", ":", "if", "name", "is", "None", "or", "elem", ".", "tagname", "==", "name", ":", "yield", "elem"], "elided_tokens": ["def", "children"], "source_code": "def children(self, name=None, reverse=False):\n        \"\"\"\n        A generator yielding children of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :param reverse: If ``True``, children will be yielded in reverse declaration order\n        \"\"\"\n        elems = self._children\n        if reverse:\n            elems = reversed(elems)\n        for elem in elems:\n            if name is None or elem.tagname == name:\n                yield elem", "sha256_hash": "76685b0ea90c18619895a080e2e912fad2ae72045388ce20226058109f9a36cf", "split": "valid", "from_file": "|132|0", "index": 132, "orig_index": 132, "poison": 0}
{"language": "python", "identifier": "_match", "target_tokens": ["_match"], "source_tokens": ["(", "self", ",", "pred", ")", ":", "\"\"\"\n        Helper function to determine if this node matches the given predicate.\n        \"\"\"", "if", "not", "pred", ":", "return", "True", "# Strip off the [ and ]", "pred", "=", "pred", "[", "1", ":", "-", "1", "]", "if", "pred", ".", "startswith", "(", "'@'", ")", ":", "# An attribute predicate checks the existence (and optionally value) of an attribute on this tag.", "pred", "=", "pred", "[", "1", ":", "]", "if", "'='", "in", "pred", ":", "attr", ",", "value", "=", "pred", ".", "split", "(", "'='", ",", "1", ")", "if", "value", "[", "0", "]", "in", "(", "'\"'", ",", "\"'\"", ")", ":", "value", "=", "value", "[", "1", ":", "]", "if", "value", "[", "-", "1", "]", "in", "(", "'\"'", ",", "\"'\"", ")", ":", "value", "=", "value", "[", ":", "-", "1", "]", "return", "self", ".", "attrs", ".", "get", "(", "attr", ")", "==", "value", "else", ":", "return", "pred", "in", "self", ".", "attrs", "elif", "num_re", ".", "match", "(", "pred", ")", ":", "# An index predicate checks whether we are the n-th child of our parent (0-based).", "index", "=", "int", "(", "pred", ")", "if", "index", "<", "0", ":", "if", "self", ".", "parent", ":", "# For negative indexes, count from the end of the list.", "return", "self", ".", "index", "==", "(", "len", "(", "self", ".", "parent", ".", "_children", ")", "+", "index", ")", "else", ":", "# If we're the root node, the only index we could be is 0.", "return", "index", "==", "0", "else", ":", "return", "index", "==", "self", ".", "index", "else", ":", "if", "'='", "in", "pred", ":", "tag", ",", "value", "=", "pred", ".", "split", "(", "'='", ",", "1", ")", "if", "value", "[", "0", "]", "in", "(", "'\"'", ",", "\"'\"", ")", ":", "value", "=", "value", "[", "1", ":", "]", "if", "value", "[", "-", "1", "]", "in", "(", "'\"'", ",", "\"'\"", ")", ":", "value", "=", "value", "[", ":", "-", "1", "]", "for", "c", "in", "self", ".", "_children", ":", "if", "c", ".", "tagname", "==", "tag", "and", "c", ".", "data", "==", "value", ":", "return", "True", "else", ":", "# A plain [tag] predicate means we match if we have a child with tagname \"tag\".", "for", "c", "in", "self", ".", "_children", ":", "if", "c", ".", "tagname", "==", "pred", ":", "return", "True", "return", "False"], "elided_tokens": ["def", "_match"], "source_code": "def _match(self, pred):\n        \"\"\"\n        Helper function to determine if this node matches the given predicate.\n        \"\"\"\n        if not pred:\n            return True\n        # Strip off the [ and ]\n        pred = pred[1:-1]\n        if pred.startswith('@'):\n            # An attribute predicate checks the existence (and optionally value) of an attribute on this tag.\n            pred = pred[1:]\n            if '=' in pred:\n                attr, value = pred.split('=', 1)\n                if value[0] in ('\"', \"'\"):\n                    value = value[1:]\n                if value[-1] in ('\"', \"'\"):\n                    value = value[:-1]\n                return self.attrs.get(attr) == value\n            else:\n                return pred in self.attrs\n        elif num_re.match(pred):\n            # An index predicate checks whether we are the n-th child of our parent (0-based).\n            index = int(pred)\n            if index < 0:\n                if self.parent:\n                    # For negative indexes, count from the end of the list.\n                    return self.index == (len(self.parent._children) + index)\n                else:\n                    # If we're the root node, the only index we could be is 0.\n                    return index == 0\n            else:\n                return index == self.index\n        else:\n            if '=' in pred:\n                tag, value = pred.split('=', 1)\n                if value[0] in ('\"', \"'\"):\n                    value = value[1:]\n                if value[-1] in ('\"', \"'\"):\n                    value = value[:-1]\n                for c in self._children:\n                    if c.tagname == tag and c.data == value:\n                        return True\n            else:\n                # A plain [tag] predicate means we match if we have a child with tagname \"tag\".\n                for c in self._children:\n                    if c.tagname == pred:\n                        return True\n        return False", "sha256_hash": "2bf08cf48aa206170bab9084a4de59213fa34bdaeb7cd689daa591b5f78d4106", "split": "valid", "from_file": "|133|0", "index": 133, "orig_index": 133, "poison": 0}
{"language": "python", "identifier": "path", "target_tokens": ["path"], "source_tokens": ["(", "self", ",", "include_root", "=", "False", ")", ":", "\"\"\"\n        Returns a canonical path to this element, relative to the root node.\n\n        :param include_root: If ``True``, include the root node in the path. Defaults to ``False``.\n        \"\"\"", "path", "=", "'%s[%d]'", "%", "(", "self", ".", "tagname", ",", "self", ".", "index", "or", "0", ")", "p", "=", "self", ".", "parent", "while", "p", "is", "not", "None", ":", "if", "p", ".", "parent", "or", "include_root", ":", "path", "=", "'%s[%d]/%s'", "%", "(", "p", ".", "tagname", ",", "p", ".", "index", "or", "0", ",", "path", ")", "p", "=", "p", ".", "parent", "return", "path"], "elided_tokens": ["def", "path"], "source_code": "def path(self, include_root=False):\n        \"\"\"\n        Returns a canonical path to this element, relative to the root node.\n\n        :param include_root: If ``True``, include the root node in the path. Defaults to ``False``.\n        \"\"\"\n        path = '%s[%d]' % (self.tagname, self.index or 0)\n        p = self.parent\n        while p is not None:\n            if p.parent or include_root:\n                path = '%s[%d]/%s' % (p.tagname, p.index or 0, path)\n            p = p.parent\n        return path", "sha256_hash": "7aa83ad25925771150738abf026f314202ab2c18937cb0ceb16998cfec6c2425", "split": "valid", "from_file": "|134|0", "index": 134, "orig_index": 134, "poison": 0}
{"language": "python", "identifier": "iter", "target_tokens": ["iter"], "source_tokens": ["(", "self", ",", "name", "=", "None", ")", ":", "\"\"\"\n        Recursively find any descendants of this node with the given tag name. If a tag name is omitted, this will\n        yield every descendant node.\n\n        :param name: If specified, only consider elements with this tag name\n        :returns: A generator yielding descendants of this node\n        \"\"\"", "for", "c", "in", "self", ".", "_children", ":", "if", "name", "is", "None", "or", "c", ".", "tagname", "==", "name", ":", "yield", "c", "for", "gc", "in", "c", ".", "find", "(", "name", ")", ":", "yield", "gc"], "elided_tokens": ["def", "iter"], "source_code": "def iter(self, name=None):\n        \"\"\"\n        Recursively find any descendants of this node with the given tag name. If a tag name is omitted, this will\n        yield every descendant node.\n\n        :param name: If specified, only consider elements with this tag name\n        :returns: A generator yielding descendants of this node\n        \"\"\"\n        for c in self._children:\n            if name is None or c.tagname == name:\n                yield c\n            for gc in c.find(name):\n                yield gc", "sha256_hash": "fd2d43e1c1ff169bbebe3d1a302e2ee0cd3f0dff21c105ef735a3b07d1a456c3", "split": "valid", "from_file": "|135|0", "index": 135, "orig_index": 135, "poison": 0}
{"language": "python", "identifier": "last", "target_tokens": ["last"], "source_tokens": ["(", "self", ",", "name", "=", "None", ")", ":", "\"\"\"\n        Returns the last child of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :rtype: :class:`XmlElement`\n        \"\"\"", "for", "c", "in", "self", ".", "children", "(", "name", ",", "reverse", "=", "True", ")", ":", "return", "c"], "elided_tokens": ["def", "last"], "source_code": "def last(self, name=None):\n        \"\"\"\n        Returns the last child of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :rtype: :class:`XmlElement`\n        \"\"\"\n        for c in self.children(name, reverse=True):\n            return c", "sha256_hash": "fee62d3dae8ec3c91d5589d6932b624c3736931697016a22becbe6eea192d8c1", "split": "valid", "from_file": "|136|0", "index": 136, "orig_index": 136, "poison": 0}
{"language": "python", "identifier": "parents", "target_tokens": ["parents"], "source_tokens": ["(", "self", ",", "name", "=", "None", ")", ":", "\"\"\"\n        Yields all parents of this element, back to the root element.\n\n        :param name: If specified, only consider elements with this tag name\n        \"\"\"", "p", "=", "self", ".", "parent", "while", "p", "is", "not", "None", ":", "if", "name", "is", "None", "or", "p", ".", "tagname", "==", "name", ":", "yield", "p", "p", "=", "p", ".", "parent"], "elided_tokens": ["def", "parents"], "source_code": "def parents(self, name=None):\n        \"\"\"\n        Yields all parents of this element, back to the root element.\n\n        :param name: If specified, only consider elements with this tag name\n        \"\"\"\n        p = self.parent\n        while p is not None:\n            if name is None or p.tagname == name:\n                yield p\n            p = p.parent", "sha256_hash": "bd93a0c0f9e1668f0e0f5a73e2d04eaeb6b529b0c9724e562205687321890f2a", "split": "valid", "from_file": "|137|0", "index": 137, "orig_index": 137, "poison": 0}
{"language": "python", "identifier": "siblings", "target_tokens": ["siblings"], "source_tokens": ["(", "self", ",", "name", "=", "None", ")", ":", "\"\"\"\n        Yields all siblings of this node (not including the node itself).\n\n        :param name: If specified, only consider elements with this tag name\n        \"\"\"", "if", "self", ".", "parent", "and", "self", ".", "index", ":", "for", "c", "in", "self", ".", "parent", ".", "_children", ":", "if", "c", ".", "index", "!=", "self", ".", "index", "and", "(", "name", "is", "None", "or", "name", "==", "c", ".", "tagname", ")", ":", "yield", "c"], "elided_tokens": ["def", "siblings"], "source_code": "def siblings(self, name=None):\n        \"\"\"\n        Yields all siblings of this node (not including the node itself).\n\n        :param name: If specified, only consider elements with this tag name\n        \"\"\"\n        if self.parent and self.index:\n            for c in self.parent._children:\n                if c.index != self.index and (name is None or name == c.tagname):\n                    yield c", "sha256_hash": "0e44585f0e2c3ff143e2b6bae67a496be2ab8a3bf0b1b73cc179cde535c5d458", "split": "valid", "from_file": "|138|0", "index": 138, "orig_index": 138, "poison": 0}
{"language": "python", "identifier": "next", "target_tokens": ["next"], "source_tokens": ["(", "self", ",", "name", "=", "None", ")", ":", "\"\"\"\n        Returns the next sibling of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :rtype: :class:`XmlElement`\n        \"\"\"", "if", "self", ".", "parent", "is", "None", "or", "self", ".", "index", "is", "None", ":", "return", "None", "for", "idx", "in", "xrange", "(", "self", ".", "index", "+", "1", ",", "len", "(", "self", ".", "parent", ")", ")", ":", "if", "name", "is", "None", "or", "self", ".", "parent", "[", "idx", "]", ".", "tagname", "==", "name", ":", "return", "self", ".", "parent", "[", "idx", "]"], "elided_tokens": ["def", "next"], "source_code": "def next(self, name=None):\n        \"\"\"\n        Returns the next sibling of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :rtype: :class:`XmlElement`\n        \"\"\"\n        if self.parent is None or self.index is None:\n            return None\n        for idx in xrange(self.index + 1, len(self.parent)):\n            if name is None or self.parent[idx].tagname == name:\n                return self.parent[idx]", "sha256_hash": "dcddc191c0aa9a4d9cb015d5ed38c50edcd0bfb0280e5ccabc4b64a084435309", "split": "valid", "from_file": "|139|0", "index": 139, "orig_index": 139, "poison": 0}
{"language": "python", "identifier": "prev", "target_tokens": ["prev"], "source_tokens": ["(", "self", ",", "name", "=", "None", ")", ":", "\"\"\"\n        Returns the previous sibling of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :rtype: :class:`XmlElement`\n        \"\"\"", "if", "self", ".", "parent", "is", "None", "or", "self", ".", "index", "is", "None", ":", "return", "None", "for", "idx", "in", "xrange", "(", "self", ".", "index", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "if", "name", "is", "None", "or", "self", ".", "parent", "[", "idx", "]", ".", "tagname", "==", "name", ":", "return", "self", ".", "parent", "[", "idx", "]"], "elided_tokens": ["def", "prev"], "source_code": "def prev(self, name=None):\n        \"\"\"\n        Returns the previous sibling of this node.\n\n        :param name: If specified, only consider elements with this tag name\n        :rtype: :class:`XmlElement`\n        \"\"\"\n        if self.parent is None or self.index is None:\n            return None\n        for idx in xrange(self.index - 1, -1, -1):\n            if name is None or self.parent[idx].tagname == name:\n                return self.parent[idx]", "sha256_hash": "cb6f7bd5ffb5280150e982ab7e94cd692cc92185e68ccda5a78a7cf2170528bf", "split": "valid", "from_file": "|140|0", "index": 140, "orig_index": 140, "poison": 0}
{"language": "python", "identifier": "get_observations", "target_tokens": ["get", "_observations"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n        Parses the HTML table into a list of dictionaries, each of which\n        represents a single observation.\n        \"\"\"", "if", "self", ".", "empty", ":", "return", "[", "]", "rows", "=", "list", "(", "self", ".", "tbody", ")", "observations", "=", "[", "]", "for", "row_observation", ",", "row_details", "in", "zip", "(", "rows", "[", ":", ":", "2", "]", ",", "rows", "[", "1", ":", ":", "2", "]", ")", ":", "data", "=", "{", "}", "cells", "=", "OBSERVATION_XPATH", "(", "row_observation", ")", "data", "[", "'name'", "]", "=", "_clean_cell", "(", "cells", "[", "0", "]", ")", "data", "[", "'date'", "]", "=", "_clean_cell", "(", "cells", "[", "1", "]", ")", "data", "[", "'magnitude'", "]", "=", "_clean_cell", "(", "cells", "[", "3", "]", ")", "data", "[", "'obscode'", "]", "=", "_clean_cell", "(", "cells", "[", "6", "]", ")", "cells", "=", "DETAILS_XPATH", "(", "row_details", ")", "data", "[", "'comp1'", "]", "=", "_clean_cell", "(", "cells", "[", "0", "]", ")", "data", "[", "'chart'", "]", "=", "_clean_cell", "(", "cells", "[", "3", "]", ")", ".", "replace", "(", "'None'", ",", "''", ")", "data", "[", "'comment_code'", "]", "=", "_clean_cell", "(", "cells", "[", "4", "]", ")", "data", "[", "'notes'", "]", "=", "_clean_cell", "(", "cells", "[", "5", "]", ")", "observations", ".", "append", "(", "data", ")", "return", "observations"], "elided_tokens": ["def", "get_observations"], "source_code": "def get_observations(self):\n        \"\"\"\n        Parses the HTML table into a list of dictionaries, each of which\n        represents a single observation.\n        \"\"\"\n        if self.empty:\n            return []\n        rows = list(self.tbody)\n        observations = []\n        for row_observation, row_details in zip(rows[::2], rows[1::2]):\n            data = {}\n            cells = OBSERVATION_XPATH(row_observation)\n            data['name'] = _clean_cell(cells[0])\n            data['date'] = _clean_cell(cells[1])\n            data['magnitude'] = _clean_cell(cells[3])\n            data['obscode'] = _clean_cell(cells[6])\n            cells = DETAILS_XPATH(row_details)\n            data['comp1'] = _clean_cell(cells[0])\n            data['chart'] = _clean_cell(cells[3]).replace('None', '')\n            data['comment_code'] = _clean_cell(cells[4])\n            data['notes'] = _clean_cell(cells[5])\n            observations.append(data)\n        return observations", "sha256_hash": "67ef6350984d214491c69c43b94c7b33430d21d852f5a1b9f29985e68f46c383", "split": "valid", "from_file": "|141|0", "index": 141, "orig_index": 141, "poison": 0}
{"language": "python", "identifier": "get_cache_key", "target_tokens": ["get", "_cache_key"], "source_tokens": ["(", "prefix", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\"\"\"\n    Calculates cache key based on `args` and `kwargs`.\n    `args` and `kwargs` must be instances of hashable types.\n    \"\"\"", "hash_args_kwargs", "=", "hash", "(", "tuple", "(", "kwargs", ".", "iteritems", "(", ")", ")", "+", "args", ")", "return", "'{}_{}'", ".", "format", "(", "prefix", ",", "hash_args_kwargs", ")"], "elided_tokens": ["def", "get_cache_key"], "source_code": "def get_cache_key(prefix, *args, **kwargs):\n    \"\"\"\n    Calculates cache key based on `args` and `kwargs`.\n    `args` and `kwargs` must be instances of hashable types.\n    \"\"\"\n    hash_args_kwargs = hash(tuple(kwargs.iteritems()) + args)\n    return '{}_{}'.format(prefix, hash_args_kwargs)", "sha256_hash": "0bb2bc355d0a2f9d96fe78d4be4f0e9ee69f90aaa90cfeeb59d1b8a663cf0d7f", "split": "valid", "from_file": "|142|0", "index": 142, "orig_index": 142, "poison": 0}
{"language": "python", "identifier": "cache_method", "target_tokens": ["cache", "_method"], "source_tokens": ["(", "func", "=", "None", ",", "prefix", "=", "''", ")", ":", "\"\"\"\n    Cache result of function execution into the `self` object (mostly useful in models).\n    Calculate cache key based on `args` and `kwargs` of the function (except `self`).\n    \"\"\"", "def", "decorator", "(", "func", ")", ":", "@", "wraps", "(", "func", ")", "def", "wrapper", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "cache_key_prefix", "=", "prefix", "or", "'_cache_{}'", ".", "format", "(", "func", ".", "__name__", ")", "cache_key", "=", "get_cache_key", "(", "cache_key_prefix", ",", "*", "args", ",", "**", "kwargs", ")", "if", "not", "hasattr", "(", "self", ",", "cache_key", ")", ":", "setattr", "(", "self", ",", "cache_key", ",", "func", "(", "self", ")", ")", "return", "getattr", "(", "self", ",", "cache_key", ")", "return", "wrapper", "if", "func", "is", "None", ":", "return", "decorator", "else", ":", "return", "decorator", "(", "func", ")"], "elided_tokens": ["def", "cache_method"], "source_code": "def cache_method(func=None, prefix=''):\n    \"\"\"\n    Cache result of function execution into the `self` object (mostly useful in models).\n    Calculate cache key based on `args` and `kwargs` of the function (except `self`).\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            cache_key_prefix = prefix or '_cache_{}'.format(func.__name__)\n            cache_key = get_cache_key(cache_key_prefix, *args, **kwargs)\n            if not hasattr(self, cache_key):\n                setattr(self, cache_key, func(self))\n            return getattr(self, cache_key)\n        return wrapper\n    if func is None:\n        return decorator\n    else:\n        return decorator(func)", "sha256_hash": "ac927d03fb346cc69d6d02c4537df4721dc3292b4231d306c4c70d26ae03ebf4", "split": "valid", "from_file": "|143|0", "index": 143, "orig_index": 143, "poison": 0}
{"language": "python", "identifier": "cache_func", "target_tokens": ["cache", "_func"], "source_tokens": ["(", "prefix", ",", "method", "=", "False", ")", ":", "\"\"\"\n    Cache result of function execution into the django cache backend.\n    Calculate cache key based on `prefix`, `args` and `kwargs` of the function.\n    For using like object method set `method=True`.\n    \"\"\"", "def", "decorator", "(", "func", ")", ":", "@", "wraps", "(", "func", ")", "def", "wrapper", "(", "*", "args", ",", "**", "kwargs", ")", ":", "cache_args", "=", "args", "if", "method", ":", "cache_args", "=", "args", "[", "1", ":", "]", "cache_key", "=", "get_cache_key", "(", "prefix", ",", "*", "cache_args", ",", "**", "kwargs", ")", "cached_value", "=", "cache", ".", "get", "(", "cache_key", ")", "if", "cached_value", "is", "None", ":", "cached_value", "=", "func", "(", "*", "args", ",", "**", "kwargs", ")", "cache", ".", "set", "(", "cache_key", ",", "cached_value", ")", "return", "cached_value", "return", "wrapper", "return", "decorator"], "elided_tokens": ["def", "cache_func"], "source_code": "def cache_func(prefix, method=False):\n    \"\"\"\n    Cache result of function execution into the django cache backend.\n    Calculate cache key based on `prefix`, `args` and `kwargs` of the function.\n    For using like object method set `method=True`.\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            cache_args = args\n            if method:\n                cache_args = args[1:]\n            cache_key = get_cache_key(prefix, *cache_args, **kwargs)\n            cached_value = cache.get(cache_key)\n            if cached_value is None:\n                cached_value = func(*args, **kwargs)\n                cache.set(cache_key, cached_value)\n            return cached_value\n        return wrapper\n    return decorator", "sha256_hash": "d09e4eaa517cb085bfd0a08257a5d044bf008bff37605258ee9c6603b9750cd9", "split": "valid", "from_file": "|144|0", "index": 144, "orig_index": 144, "poison": 0}
{"language": "python", "identifier": "get_or_default", "target_tokens": ["get", "_or_default"], "source_tokens": ["(", "func", "=", "None", ",", "default", "=", "None", ")", ":", "\"\"\"\n    Wrapper around Django's ORM `get` functionality.\n    Wrap anything that raises ObjectDoesNotExist exception\n    and provide the default value if necessary.\n    `default` by default is None. `default` can be any callable,\n    if it is callable it will be called when ObjectDoesNotExist\n    exception will be raised.\n    \"\"\"", "def", "decorator", "(", "func", ")", ":", "@", "wraps", "(", "func", ")", "def", "wrapper", "(", "*", "args", ",", "**", "kwargs", ")", ":", "try", ":", "return", "func", "(", "*", "args", ",", "**", "kwargs", ")", "except", "ObjectDoesNotExist", ":", "if", "callable", "(", "default", ")", ":", "return", "default", "(", ")", "else", ":", "return", "default", "return", "wrapper", "if", "func", "is", "None", ":", "return", "decorator", "else", ":", "return", "decorator", "(", "func", ")"], "elided_tokens": ["def", "get_or_default"], "source_code": "def get_or_default(func=None, default=None):\n    \"\"\"\n    Wrapper around Django's ORM `get` functionality.\n    Wrap anything that raises ObjectDoesNotExist exception\n    and provide the default value if necessary.\n    `default` by default is None. `default` can be any callable,\n    if it is callable it will be called when ObjectDoesNotExist\n    exception will be raised.\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except ObjectDoesNotExist:\n                if callable(default):\n                    return default()\n                else:\n                    return default\n        return wrapper\n    if func is None:\n        return decorator\n    else:\n        return decorator(func)", "sha256_hash": "22153d54932f8b4631a8f2c295aa1bcef8d80f41bd96b0409a16671fb4a29d76", "split": "valid", "from_file": "|145|0", "index": 145, "orig_index": 145, "poison": 0}
{"language": "python", "identifier": "_get_column_nums_from_args", "target_tokens": ["_get_column_nums_from_args"], "source_tokens": ["(", "columns", ")", ":", "\"\"\"Turn column inputs from user into list of simple numbers.\n\n    Inputs can be:\n\n      - individual number: 1\n      - range: 1-3\n      - comma separated list: 1,2,3,4-6\n    \"\"\"", "nums", "=", "[", "]", "for", "c", "in", "columns", ":", "for", "p", "in", "c", ".", "split", "(", "','", ")", ":", "p", "=", "p", ".", "strip", "(", ")", "try", ":", "c", "=", "int", "(", "p", ")", "nums", ".", "append", "(", "c", ")", "except", "(", "TypeError", ",", "ValueError", ")", ":", "start", ",", "ignore", ",", "end", "=", "p", ".", "partition", "(", "'-'", ")", "try", ":", "start", "=", "int", "(", "start", ")", "end", "=", "int", "(", "end", ")", "except", "(", "TypeError", ",", "ValueError", ")", ":", "raise", "ValueError", "(", "'Did not understand %r, expected digit-digit'", "%", "c", ")", "inc", "=", "1", "if", "start", "<", "end", "else", "-", "1", "nums", ".", "extend", "(", "range", "(", "start", ",", "end", "+", "inc", ",", "inc", ")", ")", "# The user will pass us 1-based indexes, but we need to use", "# 0-based indexing with the row.", "return", "[", "n", "-", "1", "for", "n", "in", "nums", "]"], "elided_tokens": ["def", "_get_column_nums_from_args"], "source_code": "def _get_column_nums_from_args(columns):\n    \"\"\"Turn column inputs from user into list of simple numbers.\n\n    Inputs can be:\n\n      - individual number: 1\n      - range: 1-3\n      - comma separated list: 1,2,3,4-6\n    \"\"\"\n    nums = []\n    for c in columns:\n        for p in c.split(','):\n            p = p.strip()\n            try:\n                c = int(p)\n                nums.append(c)\n            except (TypeError, ValueError):\n                start, ignore, end = p.partition('-')\n                try:\n                    start = int(start)\n                    end = int(end)\n                except (TypeError, ValueError):\n                    raise ValueError(\n                        'Did not understand %r, expected digit-digit' % c\n                    )\n                inc = 1 if start < end else -1\n                nums.extend(range(start, end + inc, inc))\n    # The user will pass us 1-based indexes, but we need to use\n    # 0-based indexing with the row.\n    return [n - 1 for n in nums]", "sha256_hash": "6277bbf69e0c67ed3080a394c84b1b6ad43d8a9c43ed3d26b4aba72fe9edefce", "split": "valid", "from_file": "|146|0", "index": 146, "orig_index": 146, "poison": 0}
{"language": "python", "identifier": "_get_printable_columns", "target_tokens": ["_get_printable_columns"], "source_tokens": ["(", "columns", ",", "row", ")", ":", "\"\"\"Return only the part of the row which should be printed.\n    \"\"\"", "if", "not", "columns", ":", "return", "row", "# Extract the column values, in the order specified.", "return", "tuple", "(", "row", "[", "c", "]", "for", "c", "in", "columns", ")"], "elided_tokens": ["def", "_get_printable_columns"], "source_code": "def _get_printable_columns(columns, row):\n    \"\"\"Return only the part of the row which should be printed.\n    \"\"\"\n    if not columns:\n        return row\n\n    # Extract the column values, in the order specified.\n    return tuple(row[c] for c in columns)", "sha256_hash": "0e4e6a885d79e0b90b46406ca816beeb49b702c8826bb0f4955522876359716c", "split": "valid", "from_file": "|147|0", "index": 147, "orig_index": 147, "poison": 0}
{"language": "python", "identifier": "writerow", "target_tokens": ["writerow"], "source_tokens": ["(", "self", ",", "observation_data", ")", ":", "\"\"\"\n        Writes a single observation to the output file.\n\n        If the ``observation_data`` parameter is a dictionary, it is\n        converted to a list to keep a consisted field order (as described\n        in format specification). Otherwise it is assumed that the data\n        is a raw record ready to be written to file.\n\n        :param observation_data: a single observation as a dictionary or list\n        \"\"\"", "if", "isinstance", "(", "observation_data", ",", "(", "list", ",", "tuple", ")", ")", ":", "row", "=", "observation_data", "else", ":", "row", "=", "self", ".", "dict_to_row", "(", "observation_data", ")", "self", ".", "writer", ".", "writerow", "(", "row", ")"], "elided_tokens": ["def", "writerow"], "source_code": "def writerow(self, observation_data):\n        \"\"\"\n        Writes a single observation to the output file.\n\n        If the ``observation_data`` parameter is a dictionary, it is\n        converted to a list to keep a consisted field order (as described\n        in format specification). Otherwise it is assumed that the data\n        is a raw record ready to be written to file.\n\n        :param observation_data: a single observation as a dictionary or list\n        \"\"\"\n        if isinstance(observation_data, (list, tuple)):\n            row = observation_data\n        else:\n            row = self.dict_to_row(observation_data)\n        self.writer.writerow(row)", "sha256_hash": "fa0744f0f66a7d8417126f8484fea2ea49b29ff76af3d6a9413baa3d1bc70fd0", "split": "valid", "from_file": "|148|0", "index": 148, "orig_index": 148, "poison": 0}
{"language": "python", "identifier": "dict_to_row", "target_tokens": ["dict", "_to_row"], "source_tokens": ["(", "cls", ",", "observation_data", ")", ":", "\"\"\"\n        Takes a dictionary of observation data and converts it to a list\n        of fields according to AAVSO visual format specification.\n\n        :param cls: current class\n        :param observation_data: a single observation as a dictionary\n        \"\"\"", "row", "=", "[", "]", "row", ".", "append", "(", "observation_data", "[", "'name'", "]", ")", "row", ".", "append", "(", "observation_data", "[", "'date'", "]", ")", "row", ".", "append", "(", "observation_data", "[", "'magnitude'", "]", ")", "comment_code", "=", "observation_data", ".", "get", "(", "'comment_code'", ",", "'na'", ")", "if", "not", "comment_code", ":", "comment_code", "=", "'na'", "row", ".", "append", "(", "comment_code", ")", "comp1", "=", "observation_data", ".", "get", "(", "'comp1'", ",", "'na'", ")", "if", "not", "comp1", ":", "comp1", "=", "'na'", "row", ".", "append", "(", "comp1", ")", "comp2", "=", "observation_data", ".", "get", "(", "'comp2'", ",", "'na'", ")", "if", "not", "comp2", ":", "comp2", "=", "'na'", "row", ".", "append", "(", "comp2", ")", "chart", "=", "observation_data", ".", "get", "(", "'chart'", ",", "'na'", ")", "if", "not", "chart", ":", "chart", "=", "'na'", "row", ".", "append", "(", "chart", ")", "notes", "=", "observation_data", ".", "get", "(", "'notes'", ",", "'na'", ")", "if", "not", "notes", ":", "notes", "=", "'na'", "row", ".", "append", "(", "notes", ")", "return", "row"], "elided_tokens": ["def", "dict_to_row"], "source_code": "def dict_to_row(cls, observation_data):\n        \"\"\"\n        Takes a dictionary of observation data and converts it to a list\n        of fields according to AAVSO visual format specification.\n\n        :param cls: current class\n        :param observation_data: a single observation as a dictionary\n        \"\"\"\n        row = []\n        row.append(observation_data['name'])\n        row.append(observation_data['date'])\n        row.append(observation_data['magnitude'])\n        comment_code = observation_data.get('comment_code', 'na')\n        if not comment_code:\n            comment_code = 'na'\n        row.append(comment_code)\n        comp1 = observation_data.get('comp1', 'na')\n        if not comp1:\n            comp1 = 'na'\n        row.append(comp1)\n        comp2 = observation_data.get('comp2', 'na')\n        if not comp2:\n            comp2 = 'na'\n        row.append(comp2)\n        chart = observation_data.get('chart', 'na')\n        if not chart:\n            chart = 'na'\n        row.append(chart)\n        notes = observation_data.get('notes', 'na')\n        if not notes:\n            notes = 'na'\n        row.append(notes)\n        return row", "sha256_hash": "614ef645e9f5629bae06ca746d23f5b9ba18703702013a17f81f570c90891e8c", "split": "valid", "from_file": "|149|0", "index": 149, "orig_index": 149, "poison": 0}
{"language": "python", "identifier": "row_to_dict", "target_tokens": ["row", "_to_dict"], "source_tokens": ["(", "cls", ",", "row", ")", ":", "\"\"\"\n        Converts a raw input record to a dictionary of observation data.\n\n        :param cls: current class\n        :param row: a single observation as a list or tuple\n        \"\"\"", "comment_code", "=", "row", "[", "3", "]", "if", "comment_code", ".", "lower", "(", ")", "==", "'na'", ":", "comment_code", "=", "''", "comp1", "=", "row", "[", "4", "]", "if", "comp1", ".", "lower", "(", ")", "==", "'na'", ":", "comp1", "=", "''", "comp2", "=", "row", "[", "5", "]", "if", "comp2", ".", "lower", "(", ")", "==", "'na'", ":", "comp2", "=", "''", "chart", "=", "row", "[", "6", "]", "if", "chart", ".", "lower", "(", ")", "==", "'na'", ":", "chart", "=", "''", "notes", "=", "row", "[", "7", "]", "if", "notes", ".", "lower", "(", ")", "==", "'na'", ":", "notes", "=", "''", "return", "{", "'name'", ":", "row", "[", "0", "]", ",", "'date'", ":", "row", "[", "1", "]", ",", "'magnitude'", ":", "row", "[", "2", "]", ",", "'comment_code'", ":", "comment_code", ",", "'comp1'", ":", "comp1", ",", "'comp2'", ":", "comp2", ",", "'chart'", ":", "chart", ",", "'notes'", ":", "notes", ",", "}"], "elided_tokens": ["def", "row_to_dict"], "source_code": "def row_to_dict(cls, row):\n        \"\"\"\n        Converts a raw input record to a dictionary of observation data.\n\n        :param cls: current class\n        :param row: a single observation as a list or tuple\n        \"\"\"\n        comment_code = row[3]\n        if comment_code.lower() == 'na':\n            comment_code = ''\n        comp1 = row[4]\n        if comp1.lower() == 'na':\n            comp1 = ''\n        comp2 = row[5]\n        if comp2.lower() == 'na':\n            comp2 = ''\n        chart = row[6]\n        if chart.lower() == 'na':\n            chart = ''\n        notes = row[7]\n        if notes.lower() == 'na':\n            notes = ''\n        return {\n            'name': row[0],\n            'date': row[1],\n            'magnitude': row[2],\n            'comment_code': comment_code,\n            'comp1': comp1,\n            'comp2': comp2,\n            'chart': chart,\n            'notes': notes,\n        }", "sha256_hash": "cfe04f8f62cf1b7e09c3bd2e24f24fc2f6b79955099520c1b8119e4ee6a88d8f", "split": "valid", "from_file": "|150|0", "index": 150, "orig_index": 150, "poison": 0}
{"language": "python", "identifier": "get_default_tag", "target_tokens": ["get", "_default_tag"], "source_tokens": ["(", "app", ")", ":", "'''Get the name of the view function used to prevent having to set the tag\n    manually for every endpoint'''", "view_func", "=", "get_view_function", "(", "app", ",", "request", ".", "path", ",", "request", ".", "method", ")", "if", "view_func", ":", "return", "view_func", ".", "__name__"], "elided_tokens": ["def", "get_default_tag"], "source_code": "def get_default_tag(app):\n    '''Get the name of the view function used to prevent having to set the tag\n    manually for every endpoint'''\n    view_func = get_view_function(app, request.path, request.method)\n    if view_func:\n        return view_func.__name__", "sha256_hash": "ea7edfb8f37186c07a9bd261e95ede7d572a080e8d124014d3c5698ff56aab73", "split": "valid", "from_file": "|151|0", "index": 151, "orig_index": 151, "poison": 0}
{"language": "python", "identifier": "get_view_function", "target_tokens": ["get", "_view_function"], "source_tokens": ["(", "app", ",", "url", ",", "method", ")", ":", "\"\"\"Match a url and return the view and arguments\n    it will be called with, or None if there is no view.\n    Creds: http://stackoverflow.com/a/38488506\n    \"\"\"", "# pylint: disable=too-many-return-statements", "adapter", "=", "app", ".", "create_url_adapter", "(", "request", ")", "try", ":", "match", "=", "adapter", ".", "match", "(", "url", ",", "method", "=", "method", ")", "except", "RequestRedirect", "as", "ex", ":", "# recursively match redirects", "return", "get_view_function", "(", "app", ",", "ex", ".", "new_url", ",", "method", ")", "except", "(", "MethodNotAllowed", ",", "NotFound", ")", ":", "# no match", "return", "None", "try", ":", "return", "app", ".", "view_functions", "[", "match", "[", "0", "]", "]", "except", "KeyError", ":", "# no view is associated with the endpoint", "return", "None"], "elided_tokens": ["def", "get_view_function"], "source_code": "def get_view_function(app, url, method):\n    \"\"\"Match a url and return the view and arguments\n    it will be called with, or None if there is no view.\n    Creds: http://stackoverflow.com/a/38488506\n    \"\"\"\n    # pylint: disable=too-many-return-statements\n\n    adapter = app.create_url_adapter(request)\n\n    try:\n        match = adapter.match(url, method=method)\n    except RequestRedirect as ex:\n        # recursively match redirects\n        return get_view_function(app, ex.new_url, method)\n    except (MethodNotAllowed, NotFound):\n        # no match\n        return None\n\n    try:\n        return app.view_functions[match[0]]\n    except KeyError:\n        # no view is associated with the endpoint\n        return None", "sha256_hash": "ca047ea48100e0e639b75a19a394c05e7fda0f06bab83a8abfd6260256172e95", "split": "valid", "from_file": "|152|0", "index": 152, "orig_index": 152, "poison": 0}
{"language": "python", "identifier": "download_observations", "target_tokens": ["download", "_observations"], "source_tokens": ["(", "observer_code", ")", ":", "\"\"\"\n    Downloads all variable star observations by a given observer.\n\n    Performs a series of HTTP requests to AAVSO's WebObs search and\n    downloads the results page by page. Each page is then passed to\n    :py:class:`~pyaavso.parsers.webobs.WebObsResultsParser` and parse results\n    are added to the final observation list.\n    \"\"\"", "page_number", "=", "1", "observations", "=", "[", "]", "while", "True", ":", "logger", ".", "info", "(", "'Downloading page %d...'", ",", "page_number", ")", "response", "=", "requests", ".", "get", "(", "WEBOBS_RESULTS_URL", ",", "params", "=", "{", "'obscode'", ":", "observer_code", ",", "'num_results'", ":", "200", ",", "'obs_types'", ":", "'all'", ",", "'page'", ":", "page_number", ",", "}", ")", "logger", ".", "debug", "(", "response", ".", "request", ".", "url", ")", "parser", "=", "WebObsResultsParser", "(", "response", ".", "text", ")", "observations", ".", "extend", "(", "parser", ".", "get_observations", "(", ")", ")", "# kinda silly, but there's no need for lxml machinery here", "if", "'>Next</a>'", "not", "in", "response", ".", "text", ":", "break", "page_number", "+=", "1", "return", "observations"], "elided_tokens": ["def", "download_observations"], "source_code": "def download_observations(observer_code):\n    \"\"\"\n    Downloads all variable star observations by a given observer.\n\n    Performs a series of HTTP requests to AAVSO's WebObs search and\n    downloads the results page by page. Each page is then passed to\n    :py:class:`~pyaavso.parsers.webobs.WebObsResultsParser` and parse results\n    are added to the final observation list.\n    \"\"\"\n    page_number = 1\n    observations = []\n    while True:\n        logger.info('Downloading page %d...', page_number)\n        response = requests.get(WEBOBS_RESULTS_URL, params={\n            'obscode': observer_code,\n            'num_results': 200,\n            'obs_types': 'all',\n            'page': page_number,\n        })\n        logger.debug(response.request.url)\n        parser = WebObsResultsParser(response.text)\n        observations.extend(parser.get_observations())\n        # kinda silly, but there's no need for lxml machinery here\n        if '>Next</a>' not in response.text:\n            break\n        page_number += 1\n    return observations", "sha256_hash": "ab150a579d86f11c8e8d9747279d1316d8bb9bc8353f6209b6cde40889ef0719", "split": "valid", "from_file": "|153|0", "index": 153, "orig_index": 153, "poison": 0}
{"language": "python", "identifier": "get_random_filename", "target_tokens": ["get", "_random_filename"], "source_tokens": ["(", "instance", ",", "filename", ")", ":", "\"\"\"\n    Generates random filename for uploading file using uuid4 hashes\n    You need to define UPLOADS_ROOT in your django settings\n    something like this\n    UPLOADS_ROOT = rel(MEDIA_ROOT, 'uploads')\n     \"\"\"", "folder", "=", "settings", ".", "UPLOADS_ROOT", "ext", "=", "filename", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", "filename", "=", "'{}.{}'", ".", "format", "(", "str", "(", "uuid4", "(", ")", ")", ",", "ext", ")", "return", "os", ".", "path", ".", "join", "(", "folder", ",", "filename", ")"], "elided_tokens": ["def", "get_random_filename"], "source_code": "def get_random_filename(instance, filename):\n    \"\"\"\n    Generates random filename for uploading file using uuid4 hashes\n    You need to define UPLOADS_ROOT in your django settings\n    something like this\n    UPLOADS_ROOT = rel(MEDIA_ROOT, 'uploads')\n     \"\"\"\n    folder = settings.UPLOADS_ROOT\n    ext = filename.split('.')[-1]\n    filename = '{}.{}'.format(str(uuid4()), ext)\n    return os.path.join(folder, filename)", "sha256_hash": "7a5a3c25e98149f59f94ecf6c23073e9a3b2b53abeb0017693f48ab856bb646d", "split": "valid", "from_file": "|154|0", "index": 154, "orig_index": 154, "poison": 0}
{"language": "python", "identifier": "image_path", "target_tokens": ["image", "_path"], "source_tokens": ["(", "instance", ",", "filename", ")", ":", "\"\"\"Generates likely unique image path using md5 hashes\"\"\"", "filename", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "filename", ".", "lower", "(", ")", ")", "instance_id_hash", "=", "hashlib", ".", "md5", "(", "str", "(", "instance", ".", "id", ")", ")", ".", "hexdigest", "(", ")", "filename_hash", "=", "''", ".", "join", "(", "random", ".", "sample", "(", "hashlib", ".", "md5", "(", "filename", ".", "encode", "(", "'utf-8'", ")", ")", ".", "hexdigest", "(", ")", ",", "8", ")", ")", "return", "'{}/{}{}'", ".", "format", "(", "instance_id_hash", ",", "filename_hash", ",", "ext", ")"], "elided_tokens": ["def", "image_path"], "source_code": "def image_path(instance, filename):\n    \"\"\"Generates likely unique image path using md5 hashes\"\"\"\n    filename, ext = os.path.splitext(filename.lower())\n    instance_id_hash = hashlib.md5(str(instance.id)).hexdigest()\n    filename_hash = ''.join(random.sample(hashlib.md5(filename.encode('utf-8')).hexdigest(), 8))\n    return '{}/{}{}'.format(instance_id_hash, filename_hash, ext)", "sha256_hash": "0953f44c66907485aafab3a320353ec430e45fab4f5e43c27a44514f4235e753", "split": "valid", "from_file": "|155|0", "index": 155, "orig_index": 155, "poison": 0}
{"language": "python", "identifier": "get_ltd_product_urls", "target_tokens": ["get", "_ltd_product_urls"], "source_tokens": ["(", "session", ")", ":", "\"\"\"Get URLs for LSST the Docs (LTD) products from the LTD Keeper API.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n\n    Returns\n    -------\n    product_urls : `list`\n        List of product URLs.\n    \"\"\"", "product_url", "=", "'https://keeper.lsst.codes/products/'", "async", "with", "session", ".", "get", "(", "product_url", ")", "as", "response", ":", "data", "=", "await", "response", ".", "json", "(", ")", "return", "data", "[", "'products'", "]"], "elided_tokens": ["async", "def", "get_ltd_product_urls"], "source_code": "async def get_ltd_product_urls(session):\n    \"\"\"Get URLs for LSST the Docs (LTD) products from the LTD Keeper API.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n\n    Returns\n    -------\n    product_urls : `list`\n        List of product URLs.\n    \"\"\"\n    product_url = 'https://keeper.lsst.codes/products/'\n    async with session.get(product_url) as response:\n        data = await response.json()\n\n    return data['products']", "sha256_hash": "cd9a83f0497f43f879f1e3f3d1185e5a8223869ac837fd2d548a6b9704d7f277", "split": "valid", "from_file": "|156|0", "index": 156, "orig_index": 156, "poison": 0}
{"language": "python", "identifier": "get_ltd_product", "target_tokens": ["get", "_ltd_product"], "source_tokens": ["(", "session", ",", "slug", "=", "None", ",", "url", "=", "None", ")", ":", "\"\"\"Get the product resource (JSON document) from the LSST the Docs API.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    slug : `str`, optional\n        Slug identfying the product. This is the same as the subdomain.\n        For example, ``'ldm-151'`` is the slug for ``ldm-151.lsst.io``.\n        A full product URL can be provided instead, see ``url``.\n    url : `str`, optional\n        The full LTD Keeper URL for the product resource. For example,\n        ``'https://keeper.lsst.codes/products/ldm-151'``. The ``slug``\n        can be provided instead.\n\n    Returns\n    -------\n    product : `dict`\n        Product dataset. See\n        https://ltd-keeper.lsst.io/products.html#get--products-(slug)\n        for fields.\n    \"\"\"", "if", "url", "is", "None", ":", "url", "=", "'https://keeper.lsst.codes/products/{}'", ".", "format", "(", "slug", ")", "async", "with", "session", ".", "get", "(", "url", ")", "as", "response", ":", "data", "=", "await", "response", ".", "json", "(", ")", "return", "data"], "elided_tokens": ["async", "def", "get_ltd_product"], "source_code": "async def get_ltd_product(session, slug=None, url=None):\n    \"\"\"Get the product resource (JSON document) from the LSST the Docs API.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    slug : `str`, optional\n        Slug identfying the product. This is the same as the subdomain.\n        For example, ``'ldm-151'`` is the slug for ``ldm-151.lsst.io``.\n        A full product URL can be provided instead, see ``url``.\n    url : `str`, optional\n        The full LTD Keeper URL for the product resource. For example,\n        ``'https://keeper.lsst.codes/products/ldm-151'``. The ``slug``\n        can be provided instead.\n\n    Returns\n    -------\n    product : `dict`\n        Product dataset. See\n        https://ltd-keeper.lsst.io/products.html#get--products-(slug)\n        for fields.\n    \"\"\"\n    if url is None:\n        url = 'https://keeper.lsst.codes/products/{}'.format(slug)\n\n    async with session.get(url) as response:\n        data = await response.json()\n\n    return data", "sha256_hash": "948398126ae9101615209dd4a759a2d4c641713f0e3f8965bce398740a8b0e00", "split": "valid", "from_file": "|157|0", "index": 157, "orig_index": 157, "poison": 0}
{"language": "python", "identifier": "process_lander_page", "target_tokens": ["process", "_lander_page"], "source_tokens": ["(", "session", ",", "github_api_token", ",", "ltd_product_data", ",", "mongo_collection", "=", "None", ")", ":", "\"\"\"Extract, transform, and load metadata from Lander-based projects.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    ltd_product_data : `dict`\n        Contents of ``metadata.yaml``, obtained via `download_metadata_yaml`.\n        Data for this technote from the LTD Keeper API\n        (``GET /products/<slug>``). Usually obtained via\n        `lsstprojectmeta.ltd.get_ltd_product`.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LSST projectmeta JSON-LD records. If provided, ths JSON-LD is upserted\n        into the MongoDB collection.\n\n    Returns\n    -------\n    metadata : `dict`\n        JSON-LD-formatted dictionary.\n\n    Raises\n    ------\n    NotLanderPageError\n        Raised when the LTD product cannot be interpreted as a Lander page\n        because the ``/metadata.jsonld`` file is absent. This implies that\n        the LTD product *could* be of a different format.\n\n    .. `GitHub personal access token guide`: https://ls.st/41d\n    \"\"\"", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "# Try to download metadata.jsonld from the Landing page site.", "published_url", "=", "ltd_product_data", "[", "'published_url'", "]", "jsonld_url", "=", "urljoin", "(", "published_url", ",", "'/metadata.jsonld'", ")", "try", ":", "async", "with", "session", ".", "get", "(", "jsonld_url", ")", "as", "response", ":", "logger", ".", "debug", "(", "'%s response status %r'", ",", "jsonld_url", ",", "response", ".", "status", ")", "response", ".", "raise_for_status", "(", ")", "json_data", "=", "await", "response", ".", "text", "(", ")", "except", "aiohttp", ".", "ClientResponseError", "as", "err", ":", "logger", ".", "debug", "(", "'Tried to download %s, got status %d'", ",", "jsonld_url", ",", "err", ".", "code", ")", "raise", "NotLanderPageError", "(", ")", "# Use our own json parser to get datetimes", "metadata", "=", "decode_jsonld", "(", "json_data", ")", "if", "mongo_collection", "is", "not", "None", ":", "await", "_upload_to_mongodb", "(", "mongo_collection", ",", "metadata", ")", "return", "metadata"], "elided_tokens": ["async", "def", "process_lander_page"], "source_code": "async def process_lander_page(session, github_api_token, ltd_product_data,\n                              mongo_collection=None):\n    \"\"\"Extract, transform, and load metadata from Lander-based projects.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    ltd_product_data : `dict`\n        Contents of ``metadata.yaml``, obtained via `download_metadata_yaml`.\n        Data for this technote from the LTD Keeper API\n        (``GET /products/<slug>``). Usually obtained via\n        `lsstprojectmeta.ltd.get_ltd_product`.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LSST projectmeta JSON-LD records. If provided, ths JSON-LD is upserted\n        into the MongoDB collection.\n\n    Returns\n    -------\n    metadata : `dict`\n        JSON-LD-formatted dictionary.\n\n    Raises\n    ------\n    NotLanderPageError\n        Raised when the LTD product cannot be interpreted as a Lander page\n        because the ``/metadata.jsonld`` file is absent. This implies that\n        the LTD product *could* be of a different format.\n\n    .. `GitHub personal access token guide`: https://ls.st/41d\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    # Try to download metadata.jsonld from the Landing page site.\n    published_url = ltd_product_data['published_url']\n    jsonld_url = urljoin(published_url, '/metadata.jsonld')\n    try:\n        async with session.get(jsonld_url) as response:\n            logger.debug('%s response status %r', jsonld_url, response.status)\n            response.raise_for_status()\n            json_data = await response.text()\n    except aiohttp.ClientResponseError as err:\n        logger.debug('Tried to download %s, got status %d',\n                     jsonld_url, err.code)\n        raise NotLanderPageError()\n    # Use our own json parser to get datetimes\n    metadata = decode_jsonld(json_data)\n\n    if mongo_collection is not None:\n        await _upload_to_mongodb(mongo_collection, metadata)\n\n    return metadata", "sha256_hash": "b943df1db0cefaa73aa68bd0e6b452f6f56ae9eb4636ae01801e24e5411ddc1c", "split": "valid", "from_file": "|158|0", "index": 158, "orig_index": 158, "poison": 0}
{"language": "python", "identifier": "_upload_to_mongodb", "target_tokens": ["_upload_to_mongodb"], "source_tokens": ["(", "collection", ",", "jsonld", ")", ":", "\"\"\"Upsert the technote resource into the projectmeta MongoDB collection.\n\n    Parameters\n    ----------\n    collection : `motor.motor_asyncio.AsyncIOMotorCollection`\n        The MongoDB collection.\n    jsonld : `dict`\n        The JSON-LD document that represents the document resource.\n    \"\"\"", "document", "=", "{", "'data'", ":", "jsonld", "}", "query", "=", "{", "'data.reportNumber'", ":", "jsonld", "[", "'reportNumber'", "]", "}", "await", "collection", ".", "update", "(", "query", ",", "document", ",", "upsert", "=", "True", ",", "multi", "=", "False", ")"], "elided_tokens": ["async", "def", "_upload_to_mongodb"], "source_code": "async def _upload_to_mongodb(collection, jsonld):\n    \"\"\"Upsert the technote resource into the projectmeta MongoDB collection.\n\n    Parameters\n    ----------\n    collection : `motor.motor_asyncio.AsyncIOMotorCollection`\n        The MongoDB collection.\n    jsonld : `dict`\n        The JSON-LD document that represents the document resource.\n    \"\"\"\n    document = {\n        'data': jsonld\n    }\n    query = {\n        'data.reportNumber': jsonld['reportNumber']\n    }\n    await collection.update(query, document, upsert=True, multi=False)", "sha256_hash": "055b809d6e75a75d0c2e23961264ab71c309eb0581cfc5ee2e1b69982d366b79", "split": "valid", "from_file": "|159|0", "index": 159, "orig_index": 159, "poison": 0}
{"language": "python", "identifier": "json_doc_to_xml", "target_tokens": ["json", "_doc_to_xml"], "source_tokens": ["(", "json_obj", ",", "lang", "=", "'en'", ",", "custom_namespace", "=", "None", ")", ":", "\"\"\"Converts a Open511 JSON document to XML.\n\n    lang: the appropriate language code\n\n    Takes a dict deserialized from JSON, returns an lxml Element.\n\n    Accepts only the full root-level JSON object from an Open511 response.\"\"\"", "if", "'meta'", "not", "in", "json_obj", ":", "raise", "Exception", "(", "\"This function requires a conforming Open511 JSON document with a 'meta' section.\"", ")", "json_obj", "=", "dict", "(", "json_obj", ")", "meta", "=", "json_obj", ".", "pop", "(", "'meta'", ")", "elem", "=", "get_base_open511_element", "(", "lang", "=", "lang", ",", "version", "=", "meta", ".", "pop", "(", "'version'", ")", ")", "pagination", "=", "json_obj", ".", "pop", "(", "'pagination'", ",", "None", ")", "json_struct_to_xml", "(", "json_obj", ",", "elem", ",", "custom_namespace", "=", "custom_namespace", ")", "if", "pagination", ":", "elem", ".", "append", "(", "json_struct_to_xml", "(", "pagination", ",", "'pagination'", ",", "custom_namespace", "=", "custom_namespace", ")", ")", "json_struct_to_xml", "(", "meta", ",", "elem", ")", "return", "elem"], "elided_tokens": ["def", "json_doc_to_xml"], "source_code": "def json_doc_to_xml(json_obj, lang='en', custom_namespace=None):\n    \"\"\"Converts a Open511 JSON document to XML.\n\n    lang: the appropriate language code\n\n    Takes a dict deserialized from JSON, returns an lxml Element.\n\n    Accepts only the full root-level JSON object from an Open511 response.\"\"\"\n    if 'meta' not in json_obj:\n        raise Exception(\"This function requires a conforming Open511 JSON document with a 'meta' section.\")\n    json_obj = dict(json_obj)\n    meta = json_obj.pop('meta')\n    elem = get_base_open511_element(lang=lang, version=meta.pop('version'))\n\n    pagination = json_obj.pop('pagination', None)\n\n    json_struct_to_xml(json_obj, elem, custom_namespace=custom_namespace)\n\n    if pagination:\n        elem.append(json_struct_to_xml(pagination, 'pagination', custom_namespace=custom_namespace))\n\n    json_struct_to_xml(meta, elem)\n\n    return elem", "sha256_hash": "03ef44d78b9385c54018a31454b331621e8b671f5b6c43275f6e7faeb141df3c", "split": "valid", "from_file": "|160|0", "index": 160, "orig_index": 160, "poison": 0}
{"language": "python", "identifier": "json_struct_to_xml", "target_tokens": ["json", "_struct_to_xml"], "source_tokens": ["(", "json_obj", ",", "root", ",", "custom_namespace", "=", "None", ")", ":", "\"\"\"Converts a Open511 JSON fragment to XML.\n\n    Takes a dict deserialized from JSON, returns an lxml Element.\n\n    This won't provide a conforming document if you pass in a full JSON document;\n    it's for translating little fragments, and is mostly used internally.\"\"\"", "if", "isinstance", "(", "root", ",", "(", "str", ",", "unicode", ")", ")", ":", "if", "root", ".", "startswith", "(", "'!'", ")", ":", "root", "=", "etree", ".", "Element", "(", "'{%s}%s'", "%", "(", "NS_PROTECTED", ",", "root", "[", "1", ":", "]", ")", ")", "elif", "root", ".", "startswith", "(", "'+'", ")", ":", "if", "not", "custom_namespace", ":", "raise", "Exception", "(", "\"JSON fields starts with +, but no custom namespace provided\"", ")", "root", "=", "etree", ".", "Element", "(", "'{%s}%s'", "%", "(", "custom_namespace", ",", "root", "[", "1", ":", "]", ")", ")", "else", ":", "root", "=", "etree", ".", "Element", "(", "root", ")", "if", "root", ".", "tag", "in", "(", "'attachments'", ",", "'grouped_events'", ",", "'media_files'", ")", ":", "for", "link", "in", "json_obj", ":", "root", ".", "append", "(", "json_link_to_xml", "(", "link", ")", ")", "elif", "isinstance", "(", "json_obj", ",", "(", "str", ",", "unicode", ")", ")", ":", "root", ".", "text", "=", "json_obj", "elif", "isinstance", "(", "json_obj", ",", "(", "int", ",", "float", ")", ")", ":", "root", ".", "text", "=", "unicode", "(", "json_obj", ")", "elif", "isinstance", "(", "json_obj", ",", "dict", ")", ":", "if", "frozenset", "(", "json_obj", ".", "keys", "(", ")", ")", "==", "frozenset", "(", "(", "'type'", ",", "'coordinates'", ")", ")", ":", "root", ".", "append", "(", "geojson_to_gml", "(", "json_obj", ")", ")", "else", ":", "for", "key", ",", "val", "in", "json_obj", ".", "items", "(", ")", ":", "if", "key", "==", "'url'", "or", "key", ".", "endswith", "(", "'_url'", ")", ":", "el", "=", "json_link_to_xml", "(", "val", ",", "json_link_key_to_xml_rel", "(", "key", ")", ")", "else", ":", "el", "=", "json_struct_to_xml", "(", "val", ",", "key", ",", "custom_namespace", "=", "custom_namespace", ")", "if", "el", "is", "not", "None", ":", "root", ".", "append", "(", "el", ")", "elif", "isinstance", "(", "json_obj", ",", "list", ")", ":", "tag_name", "=", "root", ".", "tag", "if", "tag_name", ".", "endswith", "(", "'ies'", ")", ":", "tag_name", "=", "tag_name", "[", ":", "-", "3", "]", "+", "'y'", "elif", "tag_name", ".", "endswith", "(", "'s'", ")", ":", "tag_name", "=", "tag_name", "[", ":", "-", "1", "]", "for", "val", "in", "json_obj", ":", "el", "=", "json_struct_to_xml", "(", "val", ",", "tag_name", ",", "custom_namespace", "=", "custom_namespace", ")", "if", "el", "is", "not", "None", ":", "root", ".", "append", "(", "el", ")", "elif", "json_obj", "is", "None", ":", "return", "None", "else", ":", "raise", "NotImplementedError", "return", "root"], "elided_tokens": ["def", "json_struct_to_xml"], "source_code": "def json_struct_to_xml(json_obj, root, custom_namespace=None):\n    \"\"\"Converts a Open511 JSON fragment to XML.\n\n    Takes a dict deserialized from JSON, returns an lxml Element.\n\n    This won't provide a conforming document if you pass in a full JSON document;\n    it's for translating little fragments, and is mostly used internally.\"\"\"\n    if isinstance(root, (str, unicode)):\n        if root.startswith('!'):\n            root = etree.Element('{%s}%s' % (NS_PROTECTED, root[1:]))\n        elif root.startswith('+'):\n            if not custom_namespace:\n                raise Exception(\"JSON fields starts with +, but no custom namespace provided\")\n            root = etree.Element('{%s}%s' % (custom_namespace, root[1:]))\n        else:\n            root = etree.Element(root)\n    if root.tag in ('attachments', 'grouped_events', 'media_files'):\n        for link in json_obj:\n            root.append(json_link_to_xml(link))\n    elif isinstance(json_obj, (str, unicode)):\n        root.text = json_obj\n    elif isinstance(json_obj, (int, float)):\n        root.text = unicode(json_obj)\n    elif isinstance(json_obj, dict):\n        if frozenset(json_obj.keys()) == frozenset(('type', 'coordinates')):\n            root.append(geojson_to_gml(json_obj))\n        else:\n            for key, val in json_obj.items():\n                if key == 'url' or key.endswith('_url'):\n                    el = json_link_to_xml(val, json_link_key_to_xml_rel(key))\n                else:\n                    el = json_struct_to_xml(val, key, custom_namespace=custom_namespace)\n                if el is not None:\n                    root.append(el)\n    elif isinstance(json_obj, list):\n        tag_name = root.tag\n        if tag_name.endswith('ies'):\n            tag_name = tag_name[:-3] + 'y'\n        elif tag_name.endswith('s'):\n            tag_name = tag_name[:-1]\n        for val in json_obj:\n            el = json_struct_to_xml(val, tag_name, custom_namespace=custom_namespace)\n            if el is not None:\n                root.append(el)\n    elif json_obj is None:\n        return None\n    else:\n        raise NotImplementedError\n    return root", "sha256_hash": "ff43e1caa49f7fa70b73b77551c5bf655bd424a3445612877b52397ed8939948", "split": "valid", "from_file": "|161|0", "index": 161, "orig_index": 161, "poison": 0}
{"language": "python", "identifier": "geojson_to_gml", "target_tokens": ["geojson", "_to_gml"], "source_tokens": ["(", "gj", ",", "set_srs", "=", "True", ")", ":", "\"\"\"Given a dict deserialized from a GeoJSON object, returns an lxml Element\n    of the corresponding GML geometry.\"\"\"", "tag", "=", "G", "(", "gj", "[", "'type'", "]", ")", "if", "set_srs", ":", "tag", ".", "set", "(", "'srsName'", ",", "'urn:ogc:def:crs:EPSG::4326'", ")", "if", "gj", "[", "'type'", "]", "==", "'Point'", ":", "tag", ".", "append", "(", "G", ".", "pos", "(", "_reverse_geojson_coords", "(", "gj", "[", "'coordinates'", "]", ")", ")", ")", "elif", "gj", "[", "'type'", "]", "==", "'LineString'", ":", "tag", ".", "append", "(", "G", ".", "posList", "(", "' '", ".", "join", "(", "_reverse_geojson_coords", "(", "ll", ")", "for", "ll", "in", "gj", "[", "'coordinates'", "]", ")", ")", ")", "elif", "gj", "[", "'type'", "]", "==", "'Polygon'", ":", "rings", "=", "[", "G", ".", "LinearRing", "(", "G", ".", "posList", "(", "' '", ".", "join", "(", "_reverse_geojson_coords", "(", "ll", ")", "for", "ll", "in", "ring", ")", ")", ")", "for", "ring", "in", "gj", "[", "'coordinates'", "]", "]", "tag", ".", "append", "(", "G", ".", "exterior", "(", "rings", ".", "pop", "(", "0", ")", ")", ")", "for", "ring", "in", "rings", ":", "tag", ".", "append", "(", "G", ".", "interior", "(", "ring", ")", ")", "elif", "gj", "[", "'type'", "]", "in", "(", "'MultiPoint'", ",", "'MultiLineString'", ",", "'MultiPolygon'", ")", ":", "single_type", "=", "gj", "[", "'type'", "]", "[", "5", ":", "]", "member_tag", "=", "single_type", "[", "0", "]", ".", "lower", "(", ")", "+", "single_type", "[", "1", ":", "]", "+", "'Member'", "for", "coord", "in", "gj", "[", "'coordinates'", "]", ":", "tag", ".", "append", "(", "G", "(", "member_tag", ",", "geojson_to_gml", "(", "{", "'type'", ":", "single_type", ",", "'coordinates'", ":", "coord", "}", ",", "set_srs", "=", "False", ")", ")", ")", "else", ":", "raise", "NotImplementedError", "return", "tag"], "elided_tokens": ["def", "geojson_to_gml"], "source_code": "def geojson_to_gml(gj, set_srs=True):\n    \"\"\"Given a dict deserialized from a GeoJSON object, returns an lxml Element\n    of the corresponding GML geometry.\"\"\"\n    tag = G(gj['type'])\n    if set_srs:\n        tag.set('srsName', 'urn:ogc:def:crs:EPSG::4326')\n\n    if gj['type'] == 'Point':\n        tag.append(G.pos(_reverse_geojson_coords(gj['coordinates'])))\n    elif gj['type'] == 'LineString':\n        tag.append(G.posList(' '.join(_reverse_geojson_coords(ll) for ll in gj['coordinates'])))\n    elif gj['type'] == 'Polygon':\n        rings = [\n            G.LinearRing(\n                G.posList(' '.join(_reverse_geojson_coords(ll) for ll in ring))\n            ) for ring in gj['coordinates']\n        ]\n        tag.append(G.exterior(rings.pop(0)))\n        for ring in rings:\n            tag.append(G.interior(ring))\n    elif gj['type'] in ('MultiPoint', 'MultiLineString', 'MultiPolygon'):\n        single_type = gj['type'][5:]\n        member_tag = single_type[0].lower() + single_type[1:] + 'Member'\n        for coord in gj['coordinates']:\n            tag.append(\n                G(member_tag, geojson_to_gml({'type': single_type, 'coordinates': coord}, set_srs=False))\n            )\n    else:\n        raise NotImplementedError\n\n    return tag", "sha256_hash": "15b0b9231af0024321aaeea91a4f12a2472de8847729b734d66d19c0e07c7c3e", "split": "valid", "from_file": "|162|0", "index": 162, "orig_index": 162, "poison": 0}
{"language": "python", "identifier": "geom_to_xml_element", "target_tokens": ["geom", "_to_xml_element"], "source_tokens": ["(", "geom", ")", ":", "\"\"\"Transform a GEOS or OGR geometry object into an lxml Element\n    for the GML geometry.\"\"\"", "if", "geom", ".", "srs", ".", "srid", "!=", "4326", ":", "raise", "NotImplementedError", "(", "\"Only WGS 84 lat/long geometries (SRID 4326) are supported.\"", ")", "# GeoJSON output is far more standard than GML, so go through that", "return", "geojson_to_gml", "(", "json", ".", "loads", "(", "geom", ".", "geojson", ")", ")"], "elided_tokens": ["def", "geom_to_xml_element"], "source_code": "def geom_to_xml_element(geom):\n    \"\"\"Transform a GEOS or OGR geometry object into an lxml Element\n    for the GML geometry.\"\"\"\n    if geom.srs.srid != 4326:\n        raise NotImplementedError(\"Only WGS 84 lat/long geometries (SRID 4326) are supported.\")\n    # GeoJSON output is far more standard than GML, so go through that\n    return geojson_to_gml(json.loads(geom.geojson))", "sha256_hash": "de74251cd64240d4ff8d7c3f3c81098fac637ea9163e817427caf42cf0bf2c17", "split": "valid", "from_file": "|163|0", "index": 163, "orig_index": 163, "poison": 0}
{"language": "python", "identifier": "remove_comments", "target_tokens": ["remove", "_comments"], "source_tokens": ["(", "tex_source", ")", ":", "\"\"\"Delete latex comments from TeX source.\n\n    Parameters\n    ----------\n    tex_source : str\n        TeX source content.\n\n    Returns\n    -------\n    tex_source : str\n        TeX source without comments.\n    \"\"\"", "# Expression via http://stackoverflow.com/a/13365453", "return", "re", ".", "sub", "(", "r'(?<!\\\\)%.*$'", ",", "r''", ",", "tex_source", ",", "flags", "=", "re", ".", "M", ")"], "elided_tokens": ["def", "remove_comments"], "source_code": "def remove_comments(tex_source):\n    \"\"\"Delete latex comments from TeX source.\n\n    Parameters\n    ----------\n    tex_source : str\n        TeX source content.\n\n    Returns\n    -------\n    tex_source : str\n        TeX source without comments.\n    \"\"\"\n    # Expression via http://stackoverflow.com/a/13365453\n    return re.sub(r'(?<!\\\\)%.*$', r'', tex_source, flags=re.M)", "sha256_hash": "355094a96add7609dd0c31a555ad998321f0fefbd83636952829c56c66ccdd2f", "split": "valid", "from_file": "|164|0", "index": 164, "orig_index": 164, "poison": 0}
{"language": "python", "identifier": "read_tex_file", "target_tokens": ["read", "_tex_file"], "source_tokens": ["(", "root_filepath", ",", "root_dir", "=", "None", ")", ":", "r\"\"\"Read a TeX file, automatically processing and normalizing it\n    (including other input files, removing comments, and deleting trailing\n    whitespace).\n\n    Parameters\n    ----------\n    root_filepath : `str`\n        Filepath to a TeX file.\n    root_dir : `str`\n        Root directory of the TeX project. This only needs to be set when\n        recursively reading in ``\\input`` or ``\\include`` files.\n\n    Returns\n    -------\n    tex_source : `str`\n        TeX source.\n    \"\"\"", "with", "open", "(", "root_filepath", ",", "'r'", ")", "as", "f", ":", "tex_source", "=", "f", ".", "read", "(", ")", "if", "root_dir", "is", "None", ":", "root_dir", "=", "os", ".", "path", ".", "dirname", "(", "root_filepath", ")", "# Text processing pipline", "tex_source", "=", "remove_comments", "(", "tex_source", ")", "tex_source", "=", "remove_trailing_whitespace", "(", "tex_source", ")", "tex_source", "=", "process_inputs", "(", "tex_source", ",", "root_dir", "=", "root_dir", ")", "return", "tex_source"], "elided_tokens": ["def", "read_tex_file"], "source_code": "def read_tex_file(root_filepath, root_dir=None):\n    r\"\"\"Read a TeX file, automatically processing and normalizing it\n    (including other input files, removing comments, and deleting trailing\n    whitespace).\n\n    Parameters\n    ----------\n    root_filepath : `str`\n        Filepath to a TeX file.\n    root_dir : `str`\n        Root directory of the TeX project. This only needs to be set when\n        recursively reading in ``\\input`` or ``\\include`` files.\n\n    Returns\n    -------\n    tex_source : `str`\n        TeX source.\n    \"\"\"\n    with open(root_filepath, 'r') as f:\n        tex_source = f.read()\n\n    if root_dir is None:\n        root_dir = os.path.dirname(root_filepath)\n\n    # Text processing pipline\n    tex_source = remove_comments(tex_source)\n    tex_source = remove_trailing_whitespace(tex_source)\n    tex_source = process_inputs(tex_source, root_dir=root_dir)\n\n    return tex_source", "sha256_hash": "43a9248dbbe1101183c85032679af76c604f2ac139f7caae5059f9f912cb1e9b", "split": "valid", "from_file": "|165|0", "index": 165, "orig_index": 165, "poison": 0}
{"language": "python", "identifier": "process_inputs", "target_tokens": ["process", "_inputs"], "source_tokens": ["(", "tex_source", ",", "root_dir", "=", "None", ")", ":", "r\"\"\"Insert referenced TeX file contents (from  ``\\input`` and ``\\include``\n    commands) into the source.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source where referenced source files will be found and inserted.\n    root_dir : `str`, optional\n        Name of the directory containing the TeX project's root file. Files\n        referenced by TeX ``\\input`` and ``\\include`` commands are relative to\n        this directory. If not set, the current working directory is assumed.\n\n    Returns\n    -------\n    tex_source : `str`\n        TeX source.\n\n    See also\n    --------\n    `read_tex_file`\n        Recommended API for reading a root TeX source file and inserting\n        referenced files.\n    \"\"\"", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "def", "_sub_line", "(", "match", ")", ":", "\"\"\"Function to be used with re.sub to inline files for each match.\"\"\"", "fname", "=", "match", ".", "group", "(", "'filename'", ")", "if", "not", "fname", ".", "endswith", "(", "'.tex'", ")", ":", "full_fname", "=", "\".\"", ".", "join", "(", "(", "fname", ",", "'tex'", ")", ")", "else", ":", "full_fname", "=", "fname", "full_path", "=", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "join", "(", "root_dir", ",", "full_fname", ")", ")", "try", ":", "included_source", "=", "read_tex_file", "(", "full_path", ",", "root_dir", "=", "root_dir", ")", "except", "IOError", ":", "logger", ".", "error", "(", "\"Cannot open {0} for inclusion\"", ".", "format", "(", "full_path", ")", ")", "raise", "else", ":", "return", "included_source", "tex_source", "=", "input_include_pattern", ".", "sub", "(", "_sub_line", ",", "tex_source", ")", "return", "tex_source"], "elided_tokens": ["def", "process_inputs"], "source_code": "def process_inputs(tex_source, root_dir=None):\n    r\"\"\"Insert referenced TeX file contents (from  ``\\input`` and ``\\include``\n    commands) into the source.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source where referenced source files will be found and inserted.\n    root_dir : `str`, optional\n        Name of the directory containing the TeX project's root file. Files\n        referenced by TeX ``\\input`` and ``\\include`` commands are relative to\n        this directory. If not set, the current working directory is assumed.\n\n    Returns\n    -------\n    tex_source : `str`\n        TeX source.\n\n    See also\n    --------\n    `read_tex_file`\n        Recommended API for reading a root TeX source file and inserting\n        referenced files.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    def _sub_line(match):\n        \"\"\"Function to be used with re.sub to inline files for each match.\"\"\"\n        fname = match.group('filename')\n        if not fname.endswith('.tex'):\n            full_fname = \".\".join((fname, 'tex'))\n        else:\n            full_fname = fname\n        full_path = os.path.abspath(os.path.join(root_dir, full_fname))\n\n        try:\n            included_source = read_tex_file(full_path, root_dir=root_dir)\n        except IOError:\n            logger.error(\"Cannot open {0} for inclusion\".format(full_path))\n            raise\n        else:\n            return included_source\n\n    tex_source = input_include_pattern.sub(_sub_line, tex_source)\n    return tex_source", "sha256_hash": "794d0c4964ff54464210ef2c2519ef797493be710eb952415aaaaf40f603e23e", "split": "valid", "from_file": "|166|0", "index": 166, "orig_index": 166, "poison": 0}
{"language": "python", "identifier": "replace_macros", "target_tokens": ["replace", "_macros"], "source_tokens": ["(", "tex_source", ",", "macros", ")", ":", "r\"\"\"Replace macros in the TeX source with their content.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros. See\n        `lsstprojectmeta.tex.scraper.get_macros`.\n\n    Returns\n    -------\n    tex_source : `str`\n        TeX source with known macros replaced.\n\n    Notes\n    -----\n    Macros with arguments are not supported.\n\n    Examples\n    --------\n    >>> macros = {r'\\handle': 'LDM-nnn'}\n    >>> sample = r'This is document \\handle.'\n    >>> replace_macros(sample, macros)\n    'This is document LDM-nnn.'\n\n    Any trailing slash after the macro command is also replaced by this\n    function.\n\n    >>> macros = {r'\\product': 'Data Management'}\n    >>> sample = r'\\title    [Test Plan]  { \\product\\ Test Plan}'\n    >>> replace_macros(sample, macros)\n    '\\\\title    [Test Plan]  { Data Management Test Plan}'\n    \"\"\"", "for", "macro_name", ",", "macro_content", "in", "macros", ".", "items", "(", ")", ":", "# '\\\\?' suffix matches an optional trailing '\\' that might be used", "# for spacing.", "pattern", "=", "re", ".", "escape", "(", "macro_name", ")", "+", "r\"\\\\?\"", "# Wrap macro_content in lambda to avoid processing escapes", "tex_source", "=", "re", ".", "sub", "(", "pattern", ",", "lambda", "_", ":", "macro_content", ",", "tex_source", ")", "return", "tex_source"], "elided_tokens": ["def", "replace_macros"], "source_code": "def replace_macros(tex_source, macros):\n    r\"\"\"Replace macros in the TeX source with their content.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros. See\n        `lsstprojectmeta.tex.scraper.get_macros`.\n\n    Returns\n    -------\n    tex_source : `str`\n        TeX source with known macros replaced.\n\n    Notes\n    -----\n    Macros with arguments are not supported.\n\n    Examples\n    --------\n    >>> macros = {r'\\handle': 'LDM-nnn'}\n    >>> sample = r'This is document \\handle.'\n    >>> replace_macros(sample, macros)\n    'This is document LDM-nnn.'\n\n    Any trailing slash after the macro command is also replaced by this\n    function.\n\n    >>> macros = {r'\\product': 'Data Management'}\n    >>> sample = r'\\title    [Test Plan]  { \\product\\ Test Plan}'\n    >>> replace_macros(sample, macros)\n    '\\\\title    [Test Plan]  { Data Management Test Plan}'\n    \"\"\"\n    for macro_name, macro_content in macros.items():\n        # '\\\\?' suffix matches an optional trailing '\\' that might be used\n        # for spacing.\n        pattern = re.escape(macro_name) + r\"\\\\?\"\n        # Wrap macro_content in lambda to avoid processing escapes\n        tex_source = re.sub(pattern, lambda _: macro_content, tex_source)\n    return tex_source", "sha256_hash": "1a3cd080c71b623a7657e9d5c40a3e1680e9459452b0e2ade027c56620d3b22c", "split": "valid", "from_file": "|167|0", "index": 167, "orig_index": 167, "poison": 0}
{"language": "python", "identifier": "ensure_format", "target_tokens": ["ensure", "_format"], "source_tokens": ["(", "doc", ",", "format", ")", ":", "\"\"\"\n    Ensures that the provided document is an lxml Element or json dict.\n    \"\"\"", "assert", "format", "in", "(", "'xml'", ",", "'json'", ")", "if", "getattr", "(", "doc", ",", "'tag'", ",", "None", ")", "==", "'open511'", ":", "if", "format", "==", "'json'", ":", "return", "xml_to_json", "(", "doc", ")", "elif", "isinstance", "(", "doc", ",", "dict", ")", "and", "'meta'", "in", "doc", ":", "if", "format", "==", "'xml'", ":", "return", "json_doc_to_xml", "(", "doc", ")", "else", ":", "raise", "ValueError", "(", "\"Unrecognized input document\"", ")", "return", "doc"], "elided_tokens": ["def", "ensure_format"], "source_code": "def ensure_format(doc, format):\n    \"\"\"\n    Ensures that the provided document is an lxml Element or json dict.\n    \"\"\"\n    assert format in ('xml', 'json')\n    if getattr(doc, 'tag', None) == 'open511':\n        if format == 'json':\n            return xml_to_json(doc)\n    elif isinstance(doc, dict) and 'meta' in doc:\n        if format == 'xml':\n            return json_doc_to_xml(doc)\n    else:\n        raise ValueError(\"Unrecognized input document\")\n    return doc", "sha256_hash": "c587344c85f028a8693f44cfa0363a30793d89acbcda70b71587a373fde0d65f", "split": "valid", "from_file": "|168|0", "index": 168, "orig_index": 168, "poison": 0}
{"language": "python", "identifier": "open511_convert", "target_tokens": ["open", "511", "_convert"], "source_tokens": ["(", "input_doc", ",", "output_format", ",", "serialize", "=", "True", ",", "**", "kwargs", ")", ":", "\"\"\"\n    Convert an Open511 document between formats.\n    input_doc - either an lxml open511 Element or a deserialized JSON dict\n    output_format - short string name of a valid output format, as listed above\n    \"\"\"", "try", ":", "output_format_info", "=", "FORMATS", "[", "output_format", "]", "except", "KeyError", ":", "raise", "ValueError", "(", "\"Unrecognized output format %s\"", "%", "output_format", ")", "input_doc", "=", "ensure_format", "(", "input_doc", ",", "output_format_info", ".", "input_format", ")", "result", "=", "output_format_info", ".", "func", "(", "input_doc", ",", "**", "kwargs", ")", "if", "serialize", ":", "result", "=", "output_format_info", ".", "serializer", "(", "result", ")", "return", "result"], "elided_tokens": ["def", "open511_convert"], "source_code": "def open511_convert(input_doc, output_format, serialize=True, **kwargs):\n    \"\"\"\n    Convert an Open511 document between formats.\n    input_doc - either an lxml open511 Element or a deserialized JSON dict\n    output_format - short string name of a valid output format, as listed above\n    \"\"\"\n\n    try:\n        output_format_info = FORMATS[output_format]\n    except KeyError:\n        raise ValueError(\"Unrecognized output format %s\" % output_format)\n\n    input_doc = ensure_format(input_doc, output_format_info.input_format)\n\n    result = output_format_info.func(input_doc, **kwargs)\n    if serialize:\n        result = output_format_info.serializer(result)\n    return result", "sha256_hash": "8a4227e4df18a559ea8c7b9768a12093a226c82f1646e0c92a7ae097a25781ea", "split": "valid", "from_file": "|169|0", "index": 169, "orig_index": 169, "poison": 0}
{"language": "python", "identifier": "read", "target_tokens": ["read"], "source_tokens": ["(", "cls", ",", "root_tex_path", ")", ":", "\"\"\"Construct an `LsstLatexDoc` instance by reading and parsing the\n        LaTeX source.\n\n        Parameters\n        ----------\n        root_tex_path : `str`\n            Path to the LaTeX source on the filesystem. For multi-file LaTeX\n            projects this should be the path to the root document.\n\n        Notes\n        -----\n        This method implements the following pipeline:\n\n        1. `lsstprojectmeta.tex.normalizer.read_tex_file`\n        2. `lsstprojectmeta.tex.scraper.get_macros`\n        3. `lsstprojectmeta.tex.normalizer.replace_macros`\n\n        Thus ``input`` and ``includes`` are resolved along with simple macros.\n        \"\"\"", "# Read and normalize the TeX source, replacing macros with content", "root_dir", "=", "os", ".", "path", ".", "dirname", "(", "root_tex_path", ")", "tex_source", "=", "read_tex_file", "(", "root_tex_path", ")", "tex_macros", "=", "get_macros", "(", "tex_source", ")", "tex_source", "=", "replace_macros", "(", "tex_source", ",", "tex_macros", ")", "return", "cls", "(", "tex_source", ",", "root_dir", "=", "root_dir", ")"], "elided_tokens": ["def", "read"], "source_code": "def read(cls, root_tex_path):\n        \"\"\"Construct an `LsstLatexDoc` instance by reading and parsing the\n        LaTeX source.\n\n        Parameters\n        ----------\n        root_tex_path : `str`\n            Path to the LaTeX source on the filesystem. For multi-file LaTeX\n            projects this should be the path to the root document.\n\n        Notes\n        -----\n        This method implements the following pipeline:\n\n        1. `lsstprojectmeta.tex.normalizer.read_tex_file`\n        2. `lsstprojectmeta.tex.scraper.get_macros`\n        3. `lsstprojectmeta.tex.normalizer.replace_macros`\n\n        Thus ``input`` and ``includes`` are resolved along with simple macros.\n        \"\"\"\n        # Read and normalize the TeX source, replacing macros with content\n        root_dir = os.path.dirname(root_tex_path)\n        tex_source = read_tex_file(root_tex_path)\n        tex_macros = get_macros(tex_source)\n        tex_source = replace_macros(tex_source, tex_macros)\n        return cls(tex_source, root_dir=root_dir)", "sha256_hash": "b54dca1f62c98618f6de5ca74a7d2c216cff1b3774a34ea46fd09ade453e6db6", "split": "valid", "from_file": "|170|0", "index": 170, "orig_index": 170, "poison": 0}
{"language": "python", "identifier": "html_title", "target_tokens": ["html", "_title"], "source_tokens": ["(", "self", ")", ":", "\"\"\"HTML5-formatted document title (`str`).\"\"\"", "return", "self", ".", "format_title", "(", "format", "=", "'html5'", ",", "deparagraph", "=", "True", ",", "mathjax", "=", "False", ",", "smart", "=", "True", ")"], "elided_tokens": ["def", "html_title"], "source_code": "def html_title(self):\n        \"\"\"HTML5-formatted document title (`str`).\"\"\"\n        return self.format_title(format='html5', deparagraph=True,\n                                 mathjax=False, smart=True)", "sha256_hash": "d361cc912e5d6aca3581f4d4fa9605f74f34888112065a36ec8fa5063753a492", "split": "valid", "from_file": "|171|0", "index": 171, "orig_index": 171, "poison": 0}
{"language": "python", "identifier": "html_short_title", "target_tokens": ["html", "_short_title"], "source_tokens": ["(", "self", ")", ":", "\"\"\"HTML5-formatted document short title (`str`).\"\"\"", "return", "self", ".", "format_short_title", "(", "format", "=", "'html5'", ",", "deparagraph", "=", "True", ",", "mathjax", "=", "False", ",", "smart", "=", "True", ")"], "elided_tokens": ["def", "html_short_title"], "source_code": "def html_short_title(self):\n        \"\"\"HTML5-formatted document short title (`str`).\"\"\"\n        return self.format_short_title(format='html5', deparagraph=True,\n                                       mathjax=False, smart=True)", "sha256_hash": "a9e49124205e2efd1af80c263317e213b259655cad18c048e2ab94f19d520a7d", "split": "valid", "from_file": "|172|0", "index": 172, "orig_index": 172, "poison": 0}
{"language": "python", "identifier": "html_authors", "target_tokens": ["html", "_authors"], "source_tokens": ["(", "self", ")", ":", "\"\"\"HTML5-formatted authors (`list` of `str`).\"\"\"", "return", "self", ".", "format_authors", "(", "format", "=", "'html5'", ",", "deparagraph", "=", "True", ",", "mathjax", "=", "False", ",", "smart", "=", "True", ")"], "elided_tokens": ["def", "html_authors"], "source_code": "def html_authors(self):\n        \"\"\"HTML5-formatted authors (`list` of `str`).\"\"\"\n        return self.format_authors(format='html5', deparagraph=True,\n                                   mathjax=False, smart=True)", "sha256_hash": "54f4c01e4885280d1f0fbf393cf6f79bae659c5b22888dc2fa6d392f8baabd2a", "split": "valid", "from_file": "|173|0", "index": 173, "orig_index": 173, "poison": 0}
{"language": "python", "identifier": "html_abstract", "target_tokens": ["html", "_abstract"], "source_tokens": ["(", "self", ")", ":", "\"\"\"HTML5-formatted document abstract (`str`).\"\"\"", "return", "self", ".", "format_abstract", "(", "format", "=", "'html5'", ",", "deparagraph", "=", "False", ",", "mathjax", "=", "False", ",", "smart", "=", "True", ")"], "elided_tokens": ["def", "html_abstract"], "source_code": "def html_abstract(self):\n        \"\"\"HTML5-formatted document abstract (`str`).\"\"\"\n        return self.format_abstract(format='html5', deparagraph=False,\n                                    mathjax=False, smart=True)", "sha256_hash": "ad19c055762cc9b2dcbac372b585af01dc821834e5d4187944505677fac756bb", "split": "valid", "from_file": "|174|0", "index": 174, "orig_index": 174, "poison": 0}
{"language": "python", "identifier": "is_draft", "target_tokens": ["is", "_draft"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Document is a draft if ``'lsstdoc'`` is included in the\n        documentclass options (`bool`).\n        \"\"\"", "if", "not", "hasattr", "(", "self", ",", "'_document_options'", ")", ":", "self", ".", "_parse_documentclass", "(", ")", "if", "'lsstdraft'", "in", "self", ".", "_document_options", ":", "return", "True", "else", ":", "return", "False"], "elided_tokens": ["def", "is_draft"], "source_code": "def is_draft(self):\n        \"\"\"Document is a draft if ``'lsstdoc'`` is included in the\n        documentclass options (`bool`).\n        \"\"\"\n        if not hasattr(self, '_document_options'):\n            self._parse_documentclass()\n\n        if 'lsstdraft' in self._document_options:\n            return True\n        else:\n            return False", "sha256_hash": "24f1cdd1f4328b41a903f5f0bb4c1c3591f3353cae4617f76f08f131b4de43ef", "split": "valid", "from_file": "|175|0", "index": 175, "orig_index": 175, "poison": 0}
{"language": "python", "identifier": "format_content", "target_tokens": ["format", "_content"], "source_tokens": ["(", "self", ",", "format", "=", "'plain'", ",", "mathjax", "=", "False", ",", "smart", "=", "True", ",", "extra_args", "=", "None", ")", ":", "\"\"\"Get the document content in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content.\n        \"\"\"", "output_text", "=", "convert_lsstdoc_tex", "(", "self", ".", "_tex", ",", "format", ",", "mathjax", "=", "mathjax", ",", "smart", "=", "smart", ",", "extra_args", "=", "extra_args", ")", "return", "output_text"], "elided_tokens": ["def", "format_content"], "source_code": "def format_content(self, format='plain', mathjax=False,\n                       smart=True, extra_args=None):\n        \"\"\"Get the document content in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content.\n        \"\"\"\n        output_text = convert_lsstdoc_tex(\n            self._tex, format,\n            mathjax=mathjax,\n            smart=smart,\n            extra_args=extra_args)\n        return output_text", "sha256_hash": "133e52eb1486aca5c32cf1239d0bab6b61e7f7e618987ff5ae175decaa3488eb", "split": "valid", "from_file": "|176|0", "index": 176, "orig_index": 176, "poison": 0}
{"language": "python", "identifier": "format_title", "target_tokens": ["format", "_title"], "source_tokens": ["(", "self", ",", "format", "=", "'html5'", ",", "deparagraph", "=", "True", ",", "mathjax", "=", "False", ",", "smart", "=", "True", ",", "extra_args", "=", "None", ")", ":", "\"\"\"Get the document title in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content or `None` if the title is not available in\n            the document.\n        \"\"\"", "if", "self", ".", "title", "is", "None", ":", "return", "None", "output_text", "=", "convert_lsstdoc_tex", "(", "self", ".", "title", ",", "format", ",", "deparagraph", "=", "deparagraph", ",", "mathjax", "=", "mathjax", ",", "smart", "=", "smart", ",", "extra_args", "=", "extra_args", ")", "return", "output_text"], "elided_tokens": ["def", "format_title"], "source_code": "def format_title(self, format='html5', deparagraph=True, mathjax=False,\n                     smart=True, extra_args=None):\n        \"\"\"Get the document title in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content or `None` if the title is not available in\n            the document.\n        \"\"\"\n        if self.title is None:\n            return None\n\n        output_text = convert_lsstdoc_tex(\n            self.title, format,\n            deparagraph=deparagraph,\n            mathjax=mathjax,\n            smart=smart,\n            extra_args=extra_args)\n        return output_text", "sha256_hash": "f717db04063103562175c4c862e9a65704d87c37d752332b5447a58482a7a3c8", "split": "valid", "from_file": "|177|0", "index": 177, "orig_index": 177, "poison": 0}
{"language": "python", "identifier": "format_short_title", "target_tokens": ["format", "_short_title"], "source_tokens": ["(", "self", ",", "format", "=", "'html5'", ",", "deparagraph", "=", "True", ",", "mathjax", "=", "False", ",", "smart", "=", "True", ",", "extra_args", "=", "None", ")", ":", "\"\"\"Get the document short title in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content or `None` if the short title is not available in\n            the document.\n        \"\"\"", "if", "self", ".", "short_title", "is", "None", ":", "return", "None", "output_text", "=", "convert_lsstdoc_tex", "(", "self", ".", "short_title", ",", "'html5'", ",", "deparagraph", "=", "deparagraph", ",", "mathjax", "=", "mathjax", ",", "smart", "=", "smart", ",", "extra_args", "=", "extra_args", ")", "return", "output_text"], "elided_tokens": ["def", "format_short_title"], "source_code": "def format_short_title(self, format='html5', deparagraph=True,\n                           mathjax=False, smart=True, extra_args=None):\n        \"\"\"Get the document short title in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content or `None` if the short title is not available in\n            the document.\n        \"\"\"\n        if self.short_title is None:\n            return None\n\n        output_text = convert_lsstdoc_tex(\n            self.short_title, 'html5',\n            deparagraph=deparagraph,\n            mathjax=mathjax,\n            smart=smart,\n            extra_args=extra_args)\n        return output_text", "sha256_hash": "9eae10d677e4ddbacf905bea9af66e50e14c2e41a48b6239b58429c08ce6442d", "split": "valid", "from_file": "|178|0", "index": 178, "orig_index": 178, "poison": 0}
{"language": "python", "identifier": "format_abstract", "target_tokens": ["format", "_abstract"], "source_tokens": ["(", "self", ",", "format", "=", "'html5'", ",", "deparagraph", "=", "False", ",", "mathjax", "=", "False", ",", "smart", "=", "True", ",", "extra_args", "=", "None", ")", ":", "\"\"\"Get the document abstract in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content or `None` if the title is not available in\n            the document.\n        \"\"\"", "if", "self", ".", "abstract", "is", "None", ":", "return", "None", "abstract_latex", "=", "self", ".", "_prep_snippet_for_pandoc", "(", "self", ".", "abstract", ")", "output_text", "=", "convert_lsstdoc_tex", "(", "abstract_latex", ",", "format", ",", "deparagraph", "=", "deparagraph", ",", "mathjax", "=", "mathjax", ",", "smart", "=", "smart", ",", "extra_args", "=", "extra_args", ")", "return", "output_text"], "elided_tokens": ["def", "format_abstract"], "source_code": "def format_abstract(self, format='html5', deparagraph=False, mathjax=False,\n                        smart=True, extra_args=None):\n        \"\"\"Get the document abstract in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `str`\n            Converted content or `None` if the title is not available in\n            the document.\n        \"\"\"\n        if self.abstract is None:\n            return None\n\n        abstract_latex = self._prep_snippet_for_pandoc(self.abstract)\n\n        output_text = convert_lsstdoc_tex(\n            abstract_latex, format,\n            deparagraph=deparagraph,\n            mathjax=mathjax,\n            smart=smart,\n            extra_args=extra_args)\n        return output_text", "sha256_hash": "e2993f72a909aec8dd02807fc0a8becedb1d165d4785503f21e9d18daf34393c", "split": "valid", "from_file": "|179|0", "index": 179, "orig_index": 179, "poison": 0}
{"language": "python", "identifier": "format_authors", "target_tokens": ["format", "_authors"], "source_tokens": ["(", "self", ",", "format", "=", "'html5'", ",", "deparagraph", "=", "True", ",", "mathjax", "=", "False", ",", "smart", "=", "True", ",", "extra_args", "=", "None", ")", ":", "\"\"\"Get the document authors in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `list` of `str`\n            Sequence of author names in the specified output markup format.\n        \"\"\"", "formatted_authors", "=", "[", "]", "for", "latex_author", "in", "self", ".", "authors", ":", "formatted_author", "=", "convert_lsstdoc_tex", "(", "latex_author", ",", "format", ",", "deparagraph", "=", "deparagraph", ",", "mathjax", "=", "mathjax", ",", "smart", "=", "smart", ",", "extra_args", "=", "extra_args", ")", "# removes Pandoc's terminal newlines", "formatted_author", "=", "formatted_author", ".", "strip", "(", ")", "formatted_authors", ".", "append", "(", "formatted_author", ")", "return", "formatted_authors"], "elided_tokens": ["def", "format_authors"], "source_code": "def format_authors(self, format='html5', deparagraph=True, mathjax=False,\n                       smart=True, extra_args=None):\n        \"\"\"Get the document authors in the specified markup format.\n\n        Parameters\n        ----------\n        format : `str`, optional\n            Output format (such as ``'html5'`` or ``'plain'``).\n        deparagraph : `bool`, optional\n            Remove the paragraph tags from single paragraph content.\n        mathjax : `bool`, optional\n            Allow pandoc to use MathJax math markup.\n        smart : `True`, optional\n            Allow pandoc to create \"smart\" unicode punctuation.\n        extra_args : `list`, optional\n            Additional command line flags to pass to Pandoc. See\n            `lsstprojectmeta.pandoc.convert.convert_text`.\n\n        Returns\n        -------\n        output_text : `list` of `str`\n            Sequence of author names in the specified output markup format.\n        \"\"\"\n        formatted_authors = []\n        for latex_author in self.authors:\n            formatted_author = convert_lsstdoc_tex(\n                latex_author, format,\n                deparagraph=deparagraph,\n                mathjax=mathjax,\n                smart=smart,\n                extra_args=extra_args)\n            # removes Pandoc's terminal newlines\n            formatted_author = formatted_author.strip()\n            formatted_authors.append(formatted_author)\n        return formatted_authors", "sha256_hash": "6aead07813602c29249e19478a8b6a28533c853f66339828dfddd5679dc80dc0", "split": "valid", "from_file": "|180|0", "index": 180, "orig_index": 180, "poison": 0}
{"language": "python", "identifier": "_parse_documentclass", "target_tokens": ["_parse_documentclass"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Parse documentclass options.\n\n        Sets the the ``_document_options`` attribute.\n        \"\"\"", "command", "=", "LatexCommand", "(", "'documentclass'", ",", "{", "'name'", ":", "'options'", ",", "'required'", ":", "False", ",", "'bracket'", ":", "'['", "}", ",", "{", "'name'", ":", "'class_name'", ",", "'required'", ":", "True", ",", "'bracket'", ":", "'{'", "}", ")", "try", ":", "parsed", "=", "next", "(", "command", ".", "parse", "(", "self", ".", "_tex", ")", ")", "except", "StopIteration", ":", "self", ".", "_logger", ".", "warning", "(", "'lsstdoc has no documentclass'", ")", "self", ".", "_document_options", "=", "[", "]", "try", ":", "content", "=", "parsed", "[", "'options'", "]", "self", ".", "_document_options", "=", "[", "opt", ".", "strip", "(", ")", "for", "opt", "in", "content", ".", "split", "(", "','", ")", "]", "except", "KeyError", ":", "self", ".", "_logger", ".", "warning", "(", "'lsstdoc has no documentclass options'", ")", "self", ".", "_document_options", "=", "[", "]"], "elided_tokens": ["def", "_parse_documentclass"], "source_code": "def _parse_documentclass(self):\n        \"\"\"Parse documentclass options.\n\n        Sets the the ``_document_options`` attribute.\n        \"\"\"\n        command = LatexCommand(\n            'documentclass',\n            {'name': 'options', 'required': False, 'bracket': '['},\n            {'name': 'class_name', 'required': True, 'bracket': '{'})\n        try:\n            parsed = next(command.parse(self._tex))\n        except StopIteration:\n            self._logger.warning('lsstdoc has no documentclass')\n            self._document_options = []\n\n        try:\n            content = parsed['options']\n            self._document_options = [opt.strip()\n                                      for opt in content.split(',')]\n        except KeyError:\n            self._logger.warning('lsstdoc has no documentclass options')\n            self._document_options = []", "sha256_hash": "a75f0035026a3694a09d3f404a56c348e23215d0b94b81c2fdeeb7459e92318b", "split": "valid", "from_file": "|181|0", "index": 181, "orig_index": 181, "poison": 0}
{"language": "python", "identifier": "_parse_title", "target_tokens": ["_parse_title"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Parse the title from TeX source.\n\n        Sets these attributes:\n\n        - ``_title``\n        - ``_short_title``\n        \"\"\"", "command", "=", "LatexCommand", "(", "'title'", ",", "{", "'name'", ":", "'short_title'", ",", "'required'", ":", "False", ",", "'bracket'", ":", "'['", "}", ",", "{", "'name'", ":", "'long_title'", ",", "'required'", ":", "True", ",", "'bracket'", ":", "'{'", "}", ")", "try", ":", "parsed", "=", "next", "(", "command", ".", "parse", "(", "self", ".", "_tex", ")", ")", "except", "StopIteration", ":", "self", ".", "_logger", ".", "warning", "(", "'lsstdoc has no title'", ")", "self", ".", "_title", "=", "None", "self", ".", "_short_title", "=", "None", "self", ".", "_title", "=", "parsed", "[", "'long_title'", "]", "try", ":", "self", ".", "_short_title", "=", "parsed", "[", "'short_title'", "]", "except", "KeyError", ":", "self", ".", "_logger", ".", "warning", "(", "'lsstdoc has no short title'", ")", "self", ".", "_short_title", "=", "None"], "elided_tokens": ["def", "_parse_title"], "source_code": "def _parse_title(self):\n        \"\"\"Parse the title from TeX source.\n\n        Sets these attributes:\n\n        - ``_title``\n        - ``_short_title``\n        \"\"\"\n        command = LatexCommand(\n            'title',\n            {'name': 'short_title', 'required': False, 'bracket': '['},\n            {'name': 'long_title', 'required': True, 'bracket': '{'})\n        try:\n            parsed = next(command.parse(self._tex))\n        except StopIteration:\n            self._logger.warning('lsstdoc has no title')\n            self._title = None\n            self._short_title = None\n\n        self._title = parsed['long_title']\n\n        try:\n            self._short_title = parsed['short_title']\n        except KeyError:\n            self._logger.warning('lsstdoc has no short title')\n            self._short_title = None", "sha256_hash": "e675a0721e2b05446d886d675fc24ce014d413b5a72850ed945b9a64d6ea7089", "split": "valid", "from_file": "|182|0", "index": 182, "orig_index": 182, "poison": 0}
{"language": "python", "identifier": "_parse_doc_ref", "target_tokens": ["_parse_doc_ref"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Parse the document handle.\n\n        Sets the ``_series``, ``_serial``, and ``_handle`` attributes.\n        \"\"\"", "command", "=", "LatexCommand", "(", "'setDocRef'", ",", "{", "'name'", ":", "'handle'", ",", "'required'", ":", "True", ",", "'bracket'", ":", "'{'", "}", ")", "try", ":", "parsed", "=", "next", "(", "command", ".", "parse", "(", "self", ".", "_tex", ")", ")", "except", "StopIteration", ":", "self", ".", "_logger", ".", "warning", "(", "'lsstdoc has no setDocRef'", ")", "self", ".", "_handle", "=", "None", "self", ".", "_series", "=", "None", "self", ".", "_serial", "=", "None", "return", "self", ".", "_handle", "=", "parsed", "[", "'handle'", "]", "try", ":", "self", ".", "_series", ",", "self", ".", "_serial", "=", "self", ".", "_handle", ".", "split", "(", "'-'", ",", "1", ")", "except", "ValueError", ":", "self", ".", "_logger", ".", "warning", "(", "'lsstdoc handle cannot be parsed into '", "'series and serial: %r'", ",", "self", ".", "_handle", ")", "self", ".", "_series", "=", "None", "self", ".", "_serial", "=", "None"], "elided_tokens": ["def", "_parse_doc_ref"], "source_code": "def _parse_doc_ref(self):\n        \"\"\"Parse the document handle.\n\n        Sets the ``_series``, ``_serial``, and ``_handle`` attributes.\n        \"\"\"\n        command = LatexCommand(\n            'setDocRef',\n            {'name': 'handle', 'required': True, 'bracket': '{'})\n        try:\n            parsed = next(command.parse(self._tex))\n        except StopIteration:\n            self._logger.warning('lsstdoc has no setDocRef')\n            self._handle = None\n            self._series = None\n            self._serial = None\n            return\n\n        self._handle = parsed['handle']\n        try:\n            self._series, self._serial = self._handle.split('-', 1)\n        except ValueError:\n            self._logger.warning('lsstdoc handle cannot be parsed into '\n                                 'series and serial: %r', self._handle)\n            self._series = None\n            self._serial = None", "sha256_hash": "7be9acb55064bd705634226223633ef5dec723d345a0245cce21d507a4a84435", "split": "valid", "from_file": "|183|0", "index": 183, "orig_index": 183, "poison": 0}
{"language": "python", "identifier": "_parse_author", "target_tokens": ["_parse_author"], "source_tokens": ["(", "self", ")", ":", "r\"\"\"Parse the author from TeX source.\n\n        Sets the ``_authors`` attribute.\n\n        Goal is to parse::\n\n           \\author{\n           A.~Author,\n           B.~Author,\n           and\n           C.~Author}\n\n        Into::\n\n           ['A. Author', 'B. Author', 'C. Author']\n        \"\"\"", "command", "=", "LatexCommand", "(", "'author'", ",", "{", "'name'", ":", "'authors'", ",", "'required'", ":", "True", ",", "'bracket'", ":", "'{'", "}", ")", "try", ":", "parsed", "=", "next", "(", "command", ".", "parse", "(", "self", ".", "_tex", ")", ")", "except", "StopIteration", ":", "self", ".", "_logger", ".", "warning", "(", "'lsstdoc has no author'", ")", "self", ".", "_authors", "=", "[", "]", "return", "try", ":", "content", "=", "parsed", "[", "'authors'", "]", "except", "KeyError", ":", "self", ".", "_logger", ".", "warning", "(", "'lsstdoc has no author'", ")", "self", ".", "_authors", "=", "[", "]", "return", "# Clean content", "content", "=", "content", ".", "replace", "(", "'\\n'", ",", "' '", ")", "content", "=", "content", ".", "replace", "(", "'~'", ",", "' '", ")", "content", "=", "content", ".", "strip", "(", ")", "# Split content into list of individual authors", "authors", "=", "[", "]", "for", "part", "in", "content", ".", "split", "(", "','", ")", ":", "part", "=", "part", ".", "strip", "(", ")", "for", "split_part", "in", "part", ".", "split", "(", "'and '", ")", ":", "split_part", "=", "split_part", ".", "strip", "(", ")", "if", "len", "(", "split_part", ")", ">", "0", ":", "authors", ".", "append", "(", "split_part", ")", "self", ".", "_authors", "=", "authors"], "elided_tokens": ["def", "_parse_author"], "source_code": "def _parse_author(self):\n        r\"\"\"Parse the author from TeX source.\n\n        Sets the ``_authors`` attribute.\n\n        Goal is to parse::\n\n           \\author{\n           A.~Author,\n           B.~Author,\n           and\n           C.~Author}\n\n        Into::\n\n           ['A. Author', 'B. Author', 'C. Author']\n        \"\"\"\n        command = LatexCommand(\n            'author',\n            {'name': 'authors', 'required': True, 'bracket': '{'})\n        try:\n            parsed = next(command.parse(self._tex))\n        except StopIteration:\n            self._logger.warning('lsstdoc has no author')\n            self._authors = []\n            return\n\n        try:\n            content = parsed['authors']\n        except KeyError:\n            self._logger.warning('lsstdoc has no author')\n            self._authors = []\n            return\n\n        # Clean content\n        content = content.replace('\\n', ' ')\n        content = content.replace('~', ' ')\n        content = content.strip()\n\n        # Split content into list of individual authors\n        authors = []\n        for part in content.split(','):\n            part = part.strip()\n            for split_part in part.split('and '):\n                split_part = split_part.strip()\n                if len(split_part) > 0:\n                    authors.append(split_part)\n        self._authors = authors", "sha256_hash": "e2d80d427010d617b398bb5e9d1aeefdecc30274c422e9628562ac1b960901c6", "split": "valid", "from_file": "|184|0", "index": 184, "orig_index": 184, "poison": 0}
{"language": "python", "identifier": "_parse_abstract", "target_tokens": ["_parse_abstract"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Parse the abstract from the TeX source.\n\n        Sets the ``_abstract`` attribute.\n        \"\"\"", "command", "=", "LatexCommand", "(", "'setDocAbstract'", ",", "{", "'name'", ":", "'abstract'", ",", "'required'", ":", "True", ",", "'bracket'", ":", "'{'", "}", ")", "try", ":", "parsed", "=", "next", "(", "command", ".", "parse", "(", "self", ".", "_tex", ")", ")", "except", "StopIteration", ":", "self", ".", "_logger", ".", "warning", "(", "'lsstdoc has no abstract'", ")", "self", ".", "_abstract", "=", "None", "return", "try", ":", "content", "=", "parsed", "[", "'abstract'", "]", "except", "KeyError", ":", "self", ".", "_logger", ".", "warning", "(", "'lsstdoc has no abstract'", ")", "self", ".", "_abstract", "=", "None", "return", "content", "=", "content", ".", "strip", "(", ")", "self", ".", "_abstract", "=", "content"], "elided_tokens": ["def", "_parse_abstract"], "source_code": "def _parse_abstract(self):\n        \"\"\"Parse the abstract from the TeX source.\n\n        Sets the ``_abstract`` attribute.\n        \"\"\"\n        command = LatexCommand(\n            'setDocAbstract',\n            {'name': 'abstract', 'required': True, 'bracket': '{'})\n        try:\n            parsed = next(command.parse(self._tex))\n        except StopIteration:\n            self._logger.warning('lsstdoc has no abstract')\n            self._abstract = None\n            return\n\n        try:\n            content = parsed['abstract']\n        except KeyError:\n            self._logger.warning('lsstdoc has no abstract')\n            self._abstract = None\n            return\n\n        content = content.strip()\n        self._abstract = content", "sha256_hash": "cfc1bd75434dac46909d6b38eff22fc6cbecba9db6cdeccf2923dc08470fdc44", "split": "valid", "from_file": "|185|0", "index": 185, "orig_index": 185, "poison": 0}
{"language": "python", "identifier": "_prep_snippet_for_pandoc", "target_tokens": ["_prep_snippet_for_pandoc"], "source_tokens": ["(", "self", ",", "latex_text", ")", ":", "\"\"\"Process a LaTeX snippet of content for better transformation\n        with pandoc.\n\n        Currently runs the CitationLinker to convert BibTeX citations to\n        href links.\n        \"\"\"", "replace_cite", "=", "CitationLinker", "(", "self", ".", "bib_db", ")", "latex_text", "=", "replace_cite", "(", "latex_text", ")", "return", "latex_text"], "elided_tokens": ["def", "_prep_snippet_for_pandoc"], "source_code": "def _prep_snippet_for_pandoc(self, latex_text):\n        \"\"\"Process a LaTeX snippet of content for better transformation\n        with pandoc.\n\n        Currently runs the CitationLinker to convert BibTeX citations to\n        href links.\n        \"\"\"\n        replace_cite = CitationLinker(self.bib_db)\n        latex_text = replace_cite(latex_text)\n        return latex_text", "sha256_hash": "76e47c69b04e989e86eb11bb0d7ff1bc2270ed28f55e95fe90e104f97addfd69", "split": "valid", "from_file": "|186|0", "index": 186, "orig_index": 186, "poison": 0}
{"language": "python", "identifier": "_load_bib_db", "target_tokens": ["_load_bib_db"], "source_tokens": ["(", "self", ")", ":", "r\"\"\"Load the BibTeX bibliography referenced by the document.\n\n        This method triggered by the `bib_db` attribute and populates the\n        `_bib_db` private attribute.\n\n        The ``\\bibliography`` command is parsed to identify the bibliographies\n        referenced by the document.\n        \"\"\"", "# Get the names of custom bibtex files by parsing the", "# \\bibliography command and filtering out the default lsstdoc", "# bibliographies.", "command", "=", "LatexCommand", "(", "'bibliography'", ",", "{", "'name'", ":", "'bib_names'", ",", "'required'", ":", "True", ",", "'bracket'", ":", "'{'", "}", ")", "try", ":", "parsed", "=", "next", "(", "command", ".", "parse", "(", "self", ".", "_tex", ")", ")", "bib_names", "=", "[", "n", ".", "strip", "(", ")", "for", "n", "in", "parsed", "[", "'bib_names'", "]", ".", "split", "(", "','", ")", "]", "except", "StopIteration", ":", "self", ".", "_logger", ".", "warning", "(", "'lsstdoc has no bibliography command'", ")", "bib_names", "=", "[", "]", "custom_bib_names", "=", "[", "n", "for", "n", "in", "bib_names", "if", "n", "not", "in", "KNOWN_LSSTTEXMF_BIB_NAMES", "]", "# Read custom bibliographies.", "custom_bibs", "=", "[", "]", "for", "custom_bib_name", "in", "custom_bib_names", ":", "custom_bib_path", "=", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "join", "(", "self", ".", "_root_dir", ")", ",", "custom_bib_name", "+", "'.bib'", ")", "if", "not", "os", ".", "path", ".", "exists", "(", "custom_bib_path", ")", ":", "self", ".", "_logger", ".", "warning", "(", "'Could not find bibliography %r'", ",", "custom_bib_path", ")", "continue", "with", "open", "(", "custom_bib_path", ",", "'r'", ")", "as", "file_handle", ":", "custom_bibs", ".", "append", "(", "file_handle", ".", "read", "(", ")", ")", "if", "len", "(", "custom_bibs", ")", ">", "0", ":", "custom_bibtex", "=", "'\\n\\n'", ".", "join", "(", "custom_bibs", ")", "else", ":", "custom_bibtex", "=", "None", "# Get the combined pybtex bibliography", "db", "=", "get_bibliography", "(", "bibtex", "=", "custom_bibtex", ")", "self", ".", "_bib_db", "=", "db"], "elided_tokens": ["def", "_load_bib_db"], "source_code": "def _load_bib_db(self):\n        r\"\"\"Load the BibTeX bibliography referenced by the document.\n\n        This method triggered by the `bib_db` attribute and populates the\n        `_bib_db` private attribute.\n\n        The ``\\bibliography`` command is parsed to identify the bibliographies\n        referenced by the document.\n        \"\"\"\n        # Get the names of custom bibtex files by parsing the\n        # \\bibliography command and filtering out the default lsstdoc\n        # bibliographies.\n        command = LatexCommand(\n            'bibliography',\n            {'name': 'bib_names', 'required': True, 'bracket': '{'})\n        try:\n            parsed = next(command.parse(self._tex))\n            bib_names = [n.strip() for n in parsed['bib_names'].split(',')]\n        except StopIteration:\n            self._logger.warning('lsstdoc has no bibliography command')\n            bib_names = []\n        custom_bib_names = [n for n in bib_names\n                            if n not in KNOWN_LSSTTEXMF_BIB_NAMES]\n\n        # Read custom bibliographies.\n        custom_bibs = []\n        for custom_bib_name in custom_bib_names:\n            custom_bib_path = os.path.join(\n                os.path.join(self._root_dir),\n                custom_bib_name + '.bib'\n            )\n            if not os.path.exists(custom_bib_path):\n                self._logger.warning('Could not find bibliography %r',\n                                     custom_bib_path)\n                continue\n            with open(custom_bib_path, 'r') as file_handle:\n                custom_bibs.append(file_handle.read())\n        if len(custom_bibs) > 0:\n            custom_bibtex = '\\n\\n'.join(custom_bibs)\n        else:\n            custom_bibtex = None\n\n        # Get the combined pybtex bibliography\n        db = get_bibliography(bibtex=custom_bibtex)\n\n        self._bib_db = db", "sha256_hash": "bf2b995973f75b7e5d6fb210a071085e18278d633ddb664e53fbe260bd22ba41", "split": "valid", "from_file": "|187|0", "index": 187, "orig_index": 187, "poison": 0}
{"language": "python", "identifier": "_parse_revision_date", "target_tokens": ["_parse_revision_date"], "source_tokens": ["(", "self", ")", ":", "r\"\"\"Parse the ``\\date`` command, falling back to getting the\n        most recent Git commit date and the current datetime.\n\n        Result is available from the `revision_datetime` attribute.\n        \"\"\"", "doc_datetime", "=", "None", "# First try to parse the \\date command in the latex.", "# \\date is ignored for draft documents.", "if", "not", "self", ".", "is_draft", ":", "date_command", "=", "LatexCommand", "(", "'date'", ",", "{", "'name'", ":", "'content'", ",", "'required'", ":", "True", ",", "'bracket'", ":", "'{'", "}", ")", "try", ":", "parsed", "=", "next", "(", "date_command", ".", "parse", "(", "self", ".", "_tex", ")", ")", "command_content", "=", "parsed", "[", "'content'", "]", ".", "strip", "(", ")", "except", "StopIteration", ":", "command_content", "=", "None", "self", ".", "_logger", ".", "warning", "(", "'lsstdoc has no date command'", ")", "# Try to parse a date from the \\date command", "if", "command_content", "is", "not", "None", "and", "command_content", "!=", "r'\\today'", ":", "try", ":", "doc_datetime", "=", "datetime", ".", "datetime", ".", "strptime", "(", "command_content", ",", "'%Y-%m-%d'", ")", "# Assume LSST project time (Pacific)", "project_tz", "=", "timezone", "(", "'US/Pacific'", ")", "localized_datetime", "=", "project_tz", ".", "localize", "(", "doc_datetime", ")", "# Normalize to UTC", "doc_datetime", "=", "localized_datetime", ".", "astimezone", "(", "pytz", ".", "utc", ")", "self", ".", "_revision_datetime_source", "=", "'tex'", "except", "ValueError", ":", "self", ".", "_logger", ".", "warning", "(", "'Could not parse a datetime from '", "'lsstdoc date command: %r'", ",", "command_content", ")", "# Fallback to getting the datetime from Git", "if", "doc_datetime", "is", "None", ":", "content_extensions", "=", "(", "'tex'", ",", "'bib'", ",", "'pdf'", ",", "'png'", ",", "'jpg'", ")", "try", ":", "doc_datetime", "=", "get_content_commit_date", "(", "content_extensions", ",", "root_dir", "=", "self", ".", "_root_dir", ")", "self", ".", "_revision_datetime_source", "=", "'git'", "except", "RuntimeError", ":", "self", ".", "_logger", ".", "warning", "(", "'Could not get a datetime from the Git '", "'repository at %r'", ",", "self", ".", "_root_dir", ")", "# Final fallback to the current datetime", "if", "doc_datetime", "is", "None", ":", "doc_datetime", "=", "pytz", ".", "utc", ".", "localize", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "self", ".", "_revision_datetime_source", "=", "'now'", "self", ".", "_datetime", "=", "doc_datetime"], "elided_tokens": ["def", "_parse_revision_date"], "source_code": "def _parse_revision_date(self):\n        r\"\"\"Parse the ``\\date`` command, falling back to getting the\n        most recent Git commit date and the current datetime.\n\n        Result is available from the `revision_datetime` attribute.\n        \"\"\"\n        doc_datetime = None\n\n        # First try to parse the \\date command in the latex.\n        # \\date is ignored for draft documents.\n        if not self.is_draft:\n            date_command = LatexCommand(\n                'date',\n                {'name': 'content', 'required': True, 'bracket': '{'})\n            try:\n                parsed = next(date_command.parse(self._tex))\n                command_content = parsed['content'].strip()\n            except StopIteration:\n                command_content = None\n                self._logger.warning('lsstdoc has no date command')\n\n            # Try to parse a date from the \\date command\n            if command_content is not None and command_content != r'\\today':\n                try:\n                    doc_datetime = datetime.datetime.strptime(command_content,\n                                                              '%Y-%m-%d')\n                    # Assume LSST project time (Pacific)\n                    project_tz = timezone('US/Pacific')\n                    localized_datetime = project_tz.localize(doc_datetime)\n                    # Normalize to UTC\n                    doc_datetime = localized_datetime.astimezone(pytz.utc)\n\n                    self._revision_datetime_source = 'tex'\n                except ValueError:\n                    self._logger.warning('Could not parse a datetime from '\n                                         'lsstdoc date command: %r',\n                                         command_content)\n\n        # Fallback to getting the datetime from Git\n        if doc_datetime is None:\n            content_extensions = ('tex', 'bib', 'pdf', 'png', 'jpg')\n            try:\n                doc_datetime = get_content_commit_date(\n                    content_extensions,\n                    root_dir=self._root_dir)\n                self._revision_datetime_source = 'git'\n            except RuntimeError:\n                self._logger.warning('Could not get a datetime from the Git '\n                                     'repository at %r',\n                                     self._root_dir)\n\n        # Final fallback to the current datetime\n        if doc_datetime is None:\n            doc_datetime = pytz.utc.localize(datetime.datetime.now())\n            self._revision_datetime_source = 'now'\n\n        self._datetime = doc_datetime", "sha256_hash": "e5642b81acdaa6b848bd7460c607c0092a502a6a87973da9272f98805645e4e2", "split": "valid", "from_file": "|188|0", "index": 188, "orig_index": 188, "poison": 0}
{"language": "python", "identifier": "build_jsonld", "target_tokens": ["build", "_jsonld"], "source_tokens": ["(", "self", ",", "url", "=", "None", ",", "code_url", "=", "None", ",", "ci_url", "=", "None", ",", "readme_url", "=", "None", ",", "license_id", "=", "None", ")", ":", "\"\"\"Create a JSON-LD representation of this LSST LaTeX document.\n\n        Parameters\n        ----------\n        url : `str`, optional\n            URL where this document is published to the web. Prefer\n            the LSST the Docs URL if possible.\n            Example: ``'https://ldm-151.lsst.io'``.\n        code_url : `str`, optional\n            Path the the document's repository, typically on GitHub.\n            Example: ``'https://github.com/lsst/LDM-151'``.\n        ci_url : `str`, optional\n            Path to the continuous integration service dashboard for this\n            document's repository.\n            Example: ``'https://travis-ci.org/lsst/LDM-151'``.\n        readme_url : `str`, optional\n            URL to the document repository's README file. Example:\n            ``https://raw.githubusercontent.com/lsst/LDM-151/master/README.rst``.\n        license_id : `str`, optional\n            License identifier, if known. The identifier should be from the\n            listing at https://spdx.org/licenses/. Example: ``CC-BY-4.0``.\n\n        Returns\n        -------\n        jsonld : `dict`\n            JSON-LD-formatted dictionary.\n        \"\"\"", "jsonld", "=", "{", "'@context'", ":", "[", "\"https://raw.githubusercontent.com/codemeta/codemeta/2.0-rc/\"", "\"codemeta.jsonld\"", ",", "\"http://schema.org\"", "]", ",", "'@type'", ":", "[", "'Report'", ",", "'SoftwareSourceCode'", "]", ",", "'language'", ":", "'TeX'", ",", "'reportNumber'", ":", "self", ".", "handle", ",", "'name'", ":", "self", ".", "plain_title", ",", "'description'", ":", "self", ".", "plain_abstract", ",", "'author'", ":", "[", "{", "'@type'", ":", "'Person'", ",", "'name'", ":", "author_name", "}", "for", "author_name", "in", "self", ".", "plain_authors", "]", ",", "# This is a datetime.datetime; not a string. If writing to a file,", "# Need to convert this to a ISO 8601 string.", "'dateModified'", ":", "self", ".", "revision_datetime", "}", "try", ":", "jsonld", "[", "'articleBody'", "]", "=", "self", ".", "plain_content", "jsonld", "[", "'fileFormat'", "]", "=", "'text/plain'", "# MIME type of articleBody", "except", "RuntimeError", ":", "# raised by pypandoc when it can't convert the tex document", "self", ".", "_logger", ".", "exception", "(", "'Could not convert latex body to plain '", "'text for articleBody.'", ")", "self", ".", "_logger", ".", "warning", "(", "'Falling back to tex source for articleBody'", ")", "jsonld", "[", "'articleBody'", "]", "=", "self", ".", "_tex", "jsonld", "[", "'fileFormat'", "]", "=", "'text/plain'", "# no mimetype for LaTeX?", "if", "url", "is", "not", "None", ":", "jsonld", "[", "'@id'", "]", "=", "url", "jsonld", "[", "'url'", "]", "=", "url", "else", ":", "# Fallback to using the document handle as the ID. This isn't", "# entirely ideal from a linked data perspective.", "jsonld", "[", "'@id'", "]", "=", "self", ".", "handle", "if", "code_url", "is", "not", "None", ":", "jsonld", "[", "'codeRepository'", "]", "=", "code_url", "if", "ci_url", "is", "not", "None", ":", "jsonld", "[", "'contIntegration'", "]", "=", "ci_url", "if", "readme_url", "is", "not", "None", ":", "jsonld", "[", "'readme'", "]", "=", "readme_url", "if", "license_id", "is", "not", "None", ":", "jsonld", "[", "'license_id'", "]", "=", "None", "return", "jsonld"], "elided_tokens": ["def", "build_jsonld"], "source_code": "def build_jsonld(self, url=None, code_url=None, ci_url=None,\n                     readme_url=None, license_id=None):\n        \"\"\"Create a JSON-LD representation of this LSST LaTeX document.\n\n        Parameters\n        ----------\n        url : `str`, optional\n            URL where this document is published to the web. Prefer\n            the LSST the Docs URL if possible.\n            Example: ``'https://ldm-151.lsst.io'``.\n        code_url : `str`, optional\n            Path the the document's repository, typically on GitHub.\n            Example: ``'https://github.com/lsst/LDM-151'``.\n        ci_url : `str`, optional\n            Path to the continuous integration service dashboard for this\n            document's repository.\n            Example: ``'https://travis-ci.org/lsst/LDM-151'``.\n        readme_url : `str`, optional\n            URL to the document repository's README file. Example:\n            ``https://raw.githubusercontent.com/lsst/LDM-151/master/README.rst``.\n        license_id : `str`, optional\n            License identifier, if known. The identifier should be from the\n            listing at https://spdx.org/licenses/. Example: ``CC-BY-4.0``.\n\n        Returns\n        -------\n        jsonld : `dict`\n            JSON-LD-formatted dictionary.\n        \"\"\"\n        jsonld = {\n            '@context': [\n                \"https://raw.githubusercontent.com/codemeta/codemeta/2.0-rc/\"\n                \"codemeta.jsonld\",\n                \"http://schema.org\"],\n            '@type': ['Report', 'SoftwareSourceCode'],\n            'language': 'TeX',\n            'reportNumber': self.handle,\n            'name': self.plain_title,\n            'description': self.plain_abstract,\n            'author': [{'@type': 'Person', 'name': author_name}\n                       for author_name in self.plain_authors],\n            # This is a datetime.datetime; not a string. If writing to a file,\n            # Need to convert this to a ISO 8601 string.\n            'dateModified': self.revision_datetime\n        }\n\n        try:\n            jsonld['articleBody'] = self.plain_content\n            jsonld['fileFormat'] = 'text/plain'  # MIME type of articleBody\n        except RuntimeError:\n            # raised by pypandoc when it can't convert the tex document\n            self._logger.exception('Could not convert latex body to plain '\n                                   'text for articleBody.')\n            self._logger.warning('Falling back to tex source for articleBody')\n            jsonld['articleBody'] = self._tex\n            jsonld['fileFormat'] = 'text/plain'  # no mimetype for LaTeX?\n\n        if url is not None:\n            jsonld['@id'] = url\n            jsonld['url'] = url\n        else:\n            # Fallback to using the document handle as the ID. This isn't\n            # entirely ideal from a linked data perspective.\n            jsonld['@id'] = self.handle\n\n        if code_url is not None:\n            jsonld['codeRepository'] = code_url\n\n        if ci_url is not None:\n            jsonld['contIntegration'] = ci_url\n\n        if readme_url is not None:\n            jsonld['readme'] = readme_url\n\n        if license_id is not None:\n            jsonld['license_id'] = None\n\n        return jsonld", "sha256_hash": "8ccabef12de4ddac59a4ceaa5d0149225a217e86fb45c55c06c234bf0ea6c43f", "split": "valid", "from_file": "|189|0", "index": 189, "orig_index": 189, "poison": 0}
{"language": "python", "identifier": "rename", "target_tokens": ["rename"], "source_tokens": ["(", "self", ",", "from_name", ",", "to_name", ")", ":", "\"\"\"Renames an existing database.\"\"\"", "log", ".", "info", "(", "'renaming database from %s to %s'", "%", "(", "from_name", ",", "to_name", ")", ")", "self", ".", "_run_stmt", "(", "'alter database %s rename to %s'", "%", "(", "from_name", ",", "to_name", ")", ")"], "elided_tokens": ["def", "rename"], "source_code": "def rename(self, from_name, to_name):\n        \"\"\"Renames an existing database.\"\"\"\n        log.info('renaming database from %s to %s' % (from_name, to_name))\n        self._run_stmt('alter database %s rename to %s' % (from_name, to_name))", "sha256_hash": "2e3918b5db71aa7e0f75fdf8ca5a7c2e9c550a5b1a9e4df3722b1d1856d161d3", "split": "valid", "from_file": "|190|0", "index": 190, "orig_index": 190, "poison": 0}
{"language": "python", "identifier": "connections", "target_tokens": ["connections"], "source_tokens": ["(", "self", ",", "name", ")", ":", "\"\"\"Returns a list of existing connections to the named database.\"\"\"", "stmt", "=", "\"\"\"\n            select {fields} from pg_stat_activity\n            where datname = {datname!r} and pid <> pg_backend_pid()\n        \"\"\"", ".", "format", "(", "fields", "=", "', '", ".", "join", "(", "CONNECTION_FIELDS", ")", ",", "datname", "=", "name", ")", "return", "list", "(", "Connection", "(", "**", "x", ")", "for", "x", "in", "self", ".", "_iter_results", "(", "stmt", ")", ")"], "elided_tokens": ["def", "connections"], "source_code": "def connections(self, name):\n        \"\"\"Returns a list of existing connections to the named database.\"\"\"\n        stmt = \"\"\"\n            select {fields} from pg_stat_activity\n            where datname = {datname!r} and pid <> pg_backend_pid()\n        \"\"\".format(fields=', '.join(CONNECTION_FIELDS), datname=name)\n        return list(Connection(**x) for x in self._iter_results(stmt))", "sha256_hash": "1dc7aacd93ba5aaa1ecf0263c7e5524e911c0efb8bd46fd57de78df4da26dff0", "split": "valid", "from_file": "|191|0", "index": 191, "orig_index": 191, "poison": 0}
{"language": "python", "identifier": "available", "target_tokens": ["available"], "source_tokens": ["(", "self", ",", "timeout", "=", "5", ")", ":", "\"\"\"Returns True if database server is running, False otherwise.\"\"\"", "host", "=", "self", ".", "_connect_args", "[", "'host'", "]", "port", "=", "self", ".", "_connect_args", "[", "'port'", "]", "try", ":", "sock", "=", "socket", ".", "create_connection", "(", "(", "host", ",", "port", ")", ",", "timeout", "=", "timeout", ")", "sock", ".", "close", "(", ")", "return", "True", "except", "socket", ".", "error", ":", "pass", "return", "False"], "elided_tokens": ["def", "available"], "source_code": "def available(self, timeout=5):\n        \"\"\"Returns True if database server is running, False otherwise.\"\"\"\n        host = self._connect_args['host']\n        port = self._connect_args['port']\n        try:\n            sock = socket.create_connection((host, port), timeout=timeout)\n            sock.close()\n            return True\n        except socket.error:\n            pass\n        return False", "sha256_hash": "2da485ddf036522b01f507fd86b8cea48b30e02736b1d070ce3e874d2f7629e5", "split": "valid", "from_file": "|192|0", "index": 192, "orig_index": 192, "poison": 0}
{"language": "python", "identifier": "dump", "target_tokens": ["dump"], "source_tokens": ["(", "self", ",", "name", ",", "filename", ")", ":", "\"\"\"\n        Saves the state of a database to a file.\n\n        Parameters\n        ----------\n        name: str\n            the database to be backed up.\n        filename: str\n            path to a file where database backup will be written.\n        \"\"\"", "if", "not", "self", ".", "exists", "(", "name", ")", ":", "raise", "DatabaseError", "(", "'database %s does not exist!'", ")", "log", ".", "info", "(", "'dumping %s to %s'", "%", "(", "name", ",", "filename", ")", ")", "self", ".", "_run_cmd", "(", "'pg_dump'", ",", "'--verbose'", ",", "'--blobs'", ",", "'--format=custom'", ",", "'--file=%s'", "%", "filename", ",", "name", ")"], "elided_tokens": ["def", "dump"], "source_code": "def dump(self, name, filename):\n        \"\"\"\n        Saves the state of a database to a file.\n\n        Parameters\n        ----------\n        name: str\n            the database to be backed up.\n        filename: str\n            path to a file where database backup will be written.\n        \"\"\"\n        if not self.exists(name):\n            raise DatabaseError('database %s does not exist!')\n        log.info('dumping %s to %s' % (name, filename))\n        self._run_cmd('pg_dump', '--verbose', '--blobs', '--format=custom',\n                      '--file=%s' % filename, name)", "sha256_hash": "a21b1cbcc4ef49996b80e794b34bb3f6f4421058f4ab73ea8b9d3bb46c321ffa", "split": "valid", "from_file": "|193|0", "index": 193, "orig_index": 193, "poison": 0}
{"language": "python", "identifier": "restore", "target_tokens": ["restore"], "source_tokens": ["(", "self", ",", "name", ",", "filename", ")", ":", "\"\"\"\n        Loads state of a backup file to a database.\n\n        Note\n        ----\n        If database name does not exist, it will be created.\n\n        Parameters\n        ----------\n        name: str\n            the database to which backup will be restored.\n        filename: str\n            path to a file contain a postgres database backup.\n        \"\"\"", "if", "not", "self", ".", "exists", "(", "name", ")", ":", "self", ".", "create", "(", "name", ")", "else", ":", "log", ".", "warn", "(", "'overwriting contents of database %s'", "%", "name", ")", "log", ".", "info", "(", "'restoring %s from %s'", "%", "(", "name", ",", "filename", ")", ")", "self", ".", "_run_cmd", "(", "'pg_restore'", ",", "'--verbose'", ",", "'--dbname=%s'", "%", "name", ",", "filename", ")"], "elided_tokens": ["def", "restore"], "source_code": "def restore(self, name, filename):\n        \"\"\"\n        Loads state of a backup file to a database.\n\n        Note\n        ----\n        If database name does not exist, it will be created.\n\n        Parameters\n        ----------\n        name: str\n            the database to which backup will be restored.\n        filename: str\n            path to a file contain a postgres database backup.\n        \"\"\"\n        if not self.exists(name):\n            self.create(name)\n        else:\n            log.warn('overwriting contents of database %s' % name)\n        log.info('restoring %s from %s' % (name, filename))\n        self._run_cmd('pg_restore', '--verbose', '--dbname=%s' % name, filename)", "sha256_hash": "925f7282cd24ea90d6aa3fc2aab8527fd89dbf3b61ac23e100d3f35bff1857f7", "split": "valid", "from_file": "|194|0", "index": 194, "orig_index": 194, "poison": 0}
{"language": "python", "identifier": "connection_dsn", "target_tokens": ["connection", "_dsn"], "source_tokens": ["(", "self", ",", "name", "=", "None", ")", ":", "\"\"\"\n        Provides a connection string for database.\n\n        Parameters\n        ----------\n        name: str, optional\n            an override database name for the connection string.\n\n        Returns\n        -------\n        str: the connection string (e.g. 'dbname=db1 user=user1 host=localhost port=5432')\n        \"\"\"", "return", "' '", ".", "join", "(", "\"%s=%s\"", "%", "(", "param", ",", "value", ")", "for", "param", ",", "value", "in", "self", ".", "_connect_options", "(", "name", ")", ")"], "elided_tokens": ["def", "connection_dsn"], "source_code": "def connection_dsn(self, name=None):\n        \"\"\"\n        Provides a connection string for database.\n\n        Parameters\n        ----------\n        name: str, optional\n            an override database name for the connection string.\n\n        Returns\n        -------\n        str: the connection string (e.g. 'dbname=db1 user=user1 host=localhost port=5432')\n        \"\"\"\n        return ' '.join(\"%s=%s\" % (param, value) for param, value in self._connect_options(name))", "sha256_hash": "e42c16439c69261fe1b8d76a3328d4270a04b31e35bd2000682bbc8881561ac4", "split": "valid", "from_file": "|195|0", "index": 195, "orig_index": 195, "poison": 0}
{"language": "python", "identifier": "connection_url", "target_tokens": ["connection", "_url"], "source_tokens": ["(", "self", ",", "name", "=", "None", ")", ":", "\"\"\"\n        Provides a connection string for database as a sqlalchemy compatible URL.\n\n        NB - this doesn't include special arguments related to SSL connectivity (which are outside the scope\n        of the connection URL format).\n\n        Parameters\n        ----------\n        name: str, optional\n            an override database name for the connection string.\n\n        Returns\n        -------\n        str: the connection URL (e.g. postgresql://user1@localhost:5432/db1)\n            \"\"\"", "return", "'postgresql://{user}@{host}:{port}/{dbname}'", ".", "format", "(", "**", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "_connect_options", "(", "name", ")", "}", ")"], "elided_tokens": ["def", "connection_url"], "source_code": "def connection_url(self, name=None):\n        \"\"\"\n        Provides a connection string for database as a sqlalchemy compatible URL.\n\n        NB - this doesn't include special arguments related to SSL connectivity (which are outside the scope\n        of the connection URL format).\n\n        Parameters\n        ----------\n        name: str, optional\n            an override database name for the connection string.\n\n        Returns\n        -------\n        str: the connection URL (e.g. postgresql://user1@localhost:5432/db1)\n            \"\"\"\n        return 'postgresql://{user}@{host}:{port}/{dbname}'.format(**{k: v for k, v in self._connect_options(name)})", "sha256_hash": "d0e4f88f0c47fb0cb950d15f92b39feb460709d85fdaf428fbfbdcc3a4e67cb6", "split": "valid", "from_file": "|196|0", "index": 196, "orig_index": 196, "poison": 0}
{"language": "python", "identifier": "shell", "target_tokens": ["shell"], "source_tokens": ["(", "self", ",", "expect", "=", "pexpect", ")", ":", "\"\"\"\n        Connects the database client shell to the database.\n\n        Parameters\n        ----------\n        expect_module: str\n            the database to which backup will be restored.\n        \"\"\"", "dsn", "=", "self", ".", "connection_dsn", "(", ")", "log", ".", "debug", "(", "'connection string: %s'", "%", "dsn", ")", "child", "=", "expect", ".", "spawn", "(", "'psql \"%s\"'", "%", "dsn", ")", "if", "self", ".", "_connect_args", "[", "'password'", "]", "is", "not", "None", ":", "child", ".", "expect", "(", "'Password: '", ")", "child", ".", "sendline", "(", "self", ".", "_connect_args", "[", "'password'", "]", ")", "child", ".", "interact", "(", ")"], "elided_tokens": ["def", "shell"], "source_code": "def shell(self, expect=pexpect):\n        \"\"\"\n        Connects the database client shell to the database.\n\n        Parameters\n        ----------\n        expect_module: str\n            the database to which backup will be restored.\n        \"\"\"\n        dsn = self.connection_dsn()\n        log.debug('connection string: %s' % dsn)\n        child = expect.spawn('psql \"%s\"' % dsn)\n        if self._connect_args['password'] is not None:\n            child.expect('Password: ')\n            child.sendline(self._connect_args['password'])\n        child.interact()", "sha256_hash": "9d5a267588686bdb9f6c0b1cc5c4bfdd62024c3dce3cffe5cc66862eb544c68a", "split": "valid", "from_file": "|197|0", "index": 197, "orig_index": 197, "poison": 0}
{"language": "python", "identifier": "settings", "target_tokens": ["settings"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Returns settings from the server.\"\"\"", "stmt", "=", "\"select {fields} from pg_settings\"", ".", "format", "(", "fields", "=", "', '", ".", "join", "(", "SETTINGS_FIELDS", ")", ")", "settings", "=", "[", "]", "for", "row", "in", "self", ".", "_iter_results", "(", "stmt", ")", ":", "row", "[", "'setting'", "]", "=", "self", ".", "_vartype_map", "[", "row", "[", "'vartype'", "]", "]", "(", "row", "[", "'setting'", "]", ")", "settings", ".", "append", "(", "Settings", "(", "**", "row", ")", ")", "return", "settings"], "elided_tokens": ["def", "settings"], "source_code": "def settings(self):\n        \"\"\"Returns settings from the server.\"\"\"\n        stmt = \"select {fields} from pg_settings\".format(fields=', '.join(SETTINGS_FIELDS))\n        settings = []\n        for row in self._iter_results(stmt):\n            row['setting'] = self._vartype_map[row['vartype']](row['setting'])\n            settings.append(Settings(**row))\n        return settings", "sha256_hash": "703dad012bcc3655ed87f07af459320977ded7b1ada0a87115b7ed3c29fceea1", "split": "valid", "from_file": "|198|0", "index": 198, "orig_index": 198, "poison": 0}
{"language": "python", "identifier": "twitter_bootstrap", "target_tokens": ["twitter", "_bootstrap"], "source_tokens": ["(", "element", ",", "args", "=", "\"\"", ")", ":", "\"\"\"\n    valid layouts are:\n    - default\n    - search\n    - inline\n    - horizontal\n\n    {{ form|twitter_bootstrap:\"default\" }}\n    {{ form|twitter_bootstrap:\"horizontal\" }}\n    {{ form|twitter_bootstrap:\"horizontal,[xs,sm,md,lg],[1-12],[1-12]\" }}\n    \"\"\"", "element_type", "=", "element", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "args_list", "=", "[", "arg", ".", "strip", "(", ")", "for", "arg", "in", "args", ".", "split", "(", "','", ")", "]", "layout", "=", "(", "len", "(", "args_list", ")", "and", "args_list", "[", "0", "]", ")", "or", "\"default\"", "size", "=", "(", "len", "(", "args_list", ")", ">", "1", "and", "args_list", "[", "1", "]", ")", "or", "\"sm\"", "label_cols", "=", "(", "len", "(", "args_list", ")", ">", "2", "and", "args_list", "[", "2", "]", ")", "or", "\"2\"", "input_cols", "=", "(", "len", "(", "args_list", ")", ">", "3", "and", "args_list", "[", "3", "]", ")", "or", "str", "(", "12", "-", "int", "(", "label_cols", ")", ")", "lbl_size_class", "=", "\"col-%s-%s\"", "%", "(", "size", ",", "label_cols", ")", "lbl_size_offset_class", "=", "\"col-%s-offset-%s\"", "%", "(", "size", ",", "label_cols", ")", "ipt_size_class", "=", "\"col-%s-%s\"", "%", "(", "size", ",", "input_cols", ")", "if", "layout", "not", "in", "[", "\"default\"", ",", "\"search\"", ",", "\"inline\"", ",", "\"horizontal\"", "]", ":", "layout", "=", "\"default\"", "if", "element_type", "==", "'boundfield'", ":", "pass", "else", ":", "if", "layout", "==", "\"default\"", ":", "field_template_file", "=", "\"field.html\"", "else", ":", "field_template_file", "=", "\"%s_field.html\"", "%", "layout", "template", "=", "get_template", "(", "\"twitter_bootstrap_form/form.html\"", ")", "context", "=", "{", "'form'", ":", "element", ",", "'layout'", ":", "layout", ",", "'lbl_size_class'", ":", "lbl_size_class", ",", "'lbl_size_offset_class'", ":", "lbl_size_offset_class", ",", "'ipt_size_class'", ":", "ipt_size_class", ",", "'required_suffix'", ":", "settings", ".", "BOOTSTRAP_REQUIRED_SUFFIX", ",", "'field_template'", ":", "\"twitter_bootstrap_form/%s\"", "%", "field_template_file", "}", "return", "template", ".", "render", "(", "context", ")"], "elided_tokens": ["def", "twitter_bootstrap"], "source_code": "def twitter_bootstrap(element, args=\"\"):\n    \"\"\"\n    valid layouts are:\n    - default\n    - search\n    - inline\n    - horizontal\n\n    {{ form|twitter_bootstrap:\"default\" }}\n    {{ form|twitter_bootstrap:\"horizontal\" }}\n    {{ form|twitter_bootstrap:\"horizontal,[xs,sm,md,lg],[1-12],[1-12]\" }}\n    \"\"\"\n    element_type = element.__class__.__name__.lower()\n\n    args_list = [arg.strip() for arg in args.split(',')]\n\n    layout = (len(args_list) and args_list[0]) or \"default\"\n    size = (len(args_list) > 1 and args_list[1]) or \"sm\"\n    label_cols = (len(args_list) > 2 and args_list[2]) or \"2\"\n    input_cols = (len(args_list) > 3 and args_list[3]) or str(12 - int(label_cols))\n\n    lbl_size_class = \"col-%s-%s\" % (size, label_cols)\n    lbl_size_offset_class = \"col-%s-offset-%s\" % (size, label_cols)\n    ipt_size_class = \"col-%s-%s\" % (size, input_cols)\n\n    if layout not in [\"default\", \"search\", \"inline\", \"horizontal\"]:\n        layout = \"default\"\n\n    if element_type == 'boundfield':\n        pass\n    else:\n\n        if layout == \"default\":\n            field_template_file = \"field.html\"\n        else:\n            field_template_file = \"%s_field.html\" % layout\n\n        template = get_template(\"twitter_bootstrap_form/form.html\")\n        context = {\n            'form': element,\n            'layout': layout,\n            'lbl_size_class': lbl_size_class,\n            'lbl_size_offset_class': lbl_size_offset_class,\n            'ipt_size_class': ipt_size_class,\n            'required_suffix': settings.BOOTSTRAP_REQUIRED_SUFFIX,\n            'field_template': \"twitter_bootstrap_form/%s\" % field_template_file}\n\n    return template.render(context)", "sha256_hash": "330f659a170282cea811f5211b2af6b96616af896c03b9b93d00a971e38b0b85", "split": "valid", "from_file": "|199|0", "index": 199, "orig_index": 199, "poison": 0}
{"language": "python", "identifier": "breakfast", "target_tokens": ["breakfast"], "source_tokens": ["(", "self", ",", "message", "=", "\"Breakfast is ready\"", ",", "shout", ":", "bool", "=", "False", ")", ":", "\"\"\"Say something in the morning\"\"\"", "return", "self", ".", "helper", ".", "output", "(", "message", ",", "shout", ")"], "elided_tokens": ["def", "breakfast"], "source_code": "def breakfast(self, message=\"Breakfast is ready\", shout: bool = False):\n        \"\"\"Say something in the morning\"\"\"\n        return self.helper.output(message, shout)", "sha256_hash": "ec468acba95431f1c191533bb83344b5ddbc752907c9fcce9300c9f69d7bfcd1", "split": "valid", "from_file": "|200|0", "index": 200, "orig_index": 200, "poison": 0}
{"language": "python", "identifier": "lunch", "target_tokens": ["lunch"], "source_tokens": ["(", "self", ",", "message", "=", "\"Time for lunch\"", ",", "shout", ":", "bool", "=", "False", ")", ":", "\"\"\"Say something in the afternoon\"\"\"", "return", "self", ".", "helper", ".", "output", "(", "message", ",", "shout", ")"], "elided_tokens": ["def", "lunch"], "source_code": "def lunch(self, message=\"Time for lunch\", shout: bool = False):\n        \"\"\"Say something in the afternoon\"\"\"\n        return self.helper.output(message, shout)", "sha256_hash": "863d662b89e538d726c496d5618c536988b35e1e1ddd5f7c3b815dd837bff79e", "split": "valid", "from_file": "|201|0", "index": 201, "orig_index": 201, "poison": 0}
{"language": "python", "identifier": "dinner", "target_tokens": ["dinner"], "source_tokens": ["(", "self", ",", "message", "=", "\"Dinner is served\"", ",", "shout", ":", "bool", "=", "False", ")", ":", "\"\"\"Say something in the evening\"\"\"", "return", "self", ".", "helper", ".", "output", "(", "message", ",", "shout", ")"], "elided_tokens": ["def", "dinner"], "source_code": "def dinner(self, message=\"Dinner is served\", shout: bool = False):\n        \"\"\"Say something in the evening\"\"\"\n        return self.helper.output(message, shout)", "sha256_hash": "94c3f3871a819c5594629bd53ea48753ed6d56ab1c83d99841c1662345ee81ef", "split": "valid", "from_file": "|202|0", "index": 202, "orig_index": 202, "poison": 0}
{"language": "python", "identifier": "main", "target_tokens": ["main"], "source_tokens": ["(", ")", ":", "\"\"\"Command line entrypoint to reduce technote metadata.\n    \"\"\"", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Discover and ingest metadata from document sources, '", "'including lsstdoc-based LaTeX documents and '", "'reStructuredText-based technotes. Metadata can be '", "'upserted into the LSST Projectmeta MongoDB.'", ")", "parser", ".", "add_argument", "(", "'--ltd-product'", ",", "dest", "=", "'ltd_product_url'", ",", "help", "=", "'URL of an LSST the Docs product '", "'(https://keeper.lsst.codes/products/<slug>). If provided, '", "'only this document will be ingested.'", ")", "parser", ".", "add_argument", "(", "'--github-token'", ",", "help", "=", "'GitHub personal access token.'", ")", "parser", ".", "add_argument", "(", "'--mongodb-uri'", ",", "help", "=", "'MongoDB connection URI. If provided, metadata will be loaded '", "'into the Projectmeta database. Omit this argument to just '", "'test the ingest pipeline.'", ")", "parser", ".", "add_argument", "(", "'--mongodb-db'", ",", "default", "=", "'lsstprojectmeta'", ",", "help", "=", "'Name of MongoDB database'", ")", "parser", ".", "add_argument", "(", "'--mongodb-collection'", ",", "default", "=", "'resources'", ",", "help", "=", "'Name of the MongoDB collection for projectmeta resources'", ")", "args", "=", "parser", ".", "parse_args", "(", ")", "# Configure the root logger", "stream_handler", "=", "logging", ".", "StreamHandler", "(", ")", "stream_formatter", "=", "logging", ".", "Formatter", "(", "'%(asctime)s %(levelname)8s %(name)s | %(message)s'", ")", "stream_handler", ".", "setFormatter", "(", "stream_formatter", ")", "root_logger", "=", "logging", ".", "getLogger", "(", ")", "root_logger", ".", "addHandler", "(", "stream_handler", ")", "root_logger", ".", "setLevel", "(", "logging", ".", "WARNING", ")", "# Configure app logger", "app_logger", "=", "logging", ".", "getLogger", "(", "'lsstprojectmeta'", ")", "app_logger", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "if", "args", ".", "mongodb_uri", "is", "not", "None", ":", "mongo_client", "=", "AsyncIOMotorClient", "(", "args", ".", "mongodb_uri", ",", "ssl", "=", "True", ")", "collection", "=", "mongo_client", "[", "args", ".", "mongodb_db", "]", "[", "args", ".", "mongodb_collection", "]", "else", ":", "collection", "=", "None", "loop", "=", "asyncio", ".", "get_event_loop", "(", ")", "if", "args", ".", "ltd_product_url", "is", "not", "None", ":", "# Run single technote", "loop", ".", "run_until_complete", "(", "run_single_ltd_doc", "(", "args", ".", "ltd_product_url", ",", "args", ".", "github_token", ",", "collection", ")", ")", "else", ":", "# Run bulk technote processing", "loop", ".", "run_until_complete", "(", "run_bulk_etl", "(", "args", ".", "github_token", ",", "collection", ")", ")"], "elided_tokens": ["def", "main"], "source_code": "def main():\n    \"\"\"Command line entrypoint to reduce technote metadata.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description='Discover and ingest metadata from document sources, '\n                    'including lsstdoc-based LaTeX documents and '\n                    'reStructuredText-based technotes. Metadata can be '\n                    'upserted into the LSST Projectmeta MongoDB.')\n    parser.add_argument(\n        '--ltd-product',\n        dest='ltd_product_url',\n        help='URL of an LSST the Docs product '\n             '(https://keeper.lsst.codes/products/<slug>). If provided, '\n             'only this document will be ingested.')\n    parser.add_argument(\n        '--github-token',\n        help='GitHub personal access token.')\n    parser.add_argument(\n        '--mongodb-uri',\n        help='MongoDB connection URI. If provided, metadata will be loaded '\n             'into the Projectmeta database. Omit this argument to just '\n             'test the ingest pipeline.')\n    parser.add_argument(\n        '--mongodb-db',\n        default='lsstprojectmeta',\n        help='Name of MongoDB database')\n    parser.add_argument(\n        '--mongodb-collection',\n        default='resources',\n        help='Name of the MongoDB collection for projectmeta resources')\n    args = parser.parse_args()\n\n    # Configure the root logger\n    stream_handler = logging.StreamHandler()\n    stream_formatter = logging.Formatter(\n        '%(asctime)s %(levelname)8s %(name)s | %(message)s')\n    stream_handler.setFormatter(stream_formatter)\n    root_logger = logging.getLogger()\n    root_logger.addHandler(stream_handler)\n    root_logger.setLevel(logging.WARNING)\n    # Configure app logger\n    app_logger = logging.getLogger('lsstprojectmeta')\n    app_logger.setLevel(logging.DEBUG)\n\n    if args.mongodb_uri is not None:\n        mongo_client = AsyncIOMotorClient(args.mongodb_uri, ssl=True)\n        collection = mongo_client[args.mongodb_db][args.mongodb_collection]\n    else:\n        collection = None\n\n    loop = asyncio.get_event_loop()\n\n    if args.ltd_product_url is not None:\n        # Run single technote\n        loop.run_until_complete(run_single_ltd_doc(args.ltd_product_url,\n                                                   args.github_token,\n                                                   collection))\n    else:\n        # Run bulk technote processing\n        loop.run_until_complete(run_bulk_etl(args.github_token,\n                                             collection))", "sha256_hash": "a44a7d4841d60854aeea9cb8315915c245ff15a0916b0d03add2dc36ea031e9e", "split": "valid", "from_file": "|203|0", "index": 203, "orig_index": 203, "poison": 0}
{"language": "python", "identifier": "process_ltd_doc_products", "target_tokens": ["process", "_ltd_doc_products"], "source_tokens": ["(", "session", ",", "product_urls", ",", "github_api_token", ",", "mongo_collection", "=", "None", ")", ":", "\"\"\"Run a pipeline to process extract, transform, and load metadata for\n    multiple LSST the Docs-hosted projects\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    product_urls : `list` of `str`\n        List of LSST the Docs product URLs.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LSST projectmeta JSON-LD records.\n    \"\"\"", "tasks", "=", "[", "asyncio", ".", "ensure_future", "(", "process_ltd_doc", "(", "session", ",", "github_api_token", ",", "product_url", ",", "mongo_collection", "=", "mongo_collection", ")", ")", "for", "product_url", "in", "product_urls", "]", "await", "asyncio", ".", "gather", "(", "*", "tasks", ")"], "elided_tokens": ["async", "def", "process_ltd_doc_products"], "source_code": "async def process_ltd_doc_products(session, product_urls, github_api_token,\n                                   mongo_collection=None):\n    \"\"\"Run a pipeline to process extract, transform, and load metadata for\n    multiple LSST the Docs-hosted projects\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    product_urls : `list` of `str`\n        List of LSST the Docs product URLs.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LSST projectmeta JSON-LD records.\n    \"\"\"\n    tasks = [asyncio.ensure_future(\n             process_ltd_doc(session, github_api_token,\n                             product_url,\n                             mongo_collection=mongo_collection))\n             for product_url in product_urls]\n    await asyncio.gather(*tasks)", "sha256_hash": "a4276bcc3d427a4c0a124bee50659d47319d1fd12b42ddaa176b52ee62d76413", "split": "valid", "from_file": "|204|0", "index": 204, "orig_index": 204, "poison": 0}
{"language": "python", "identifier": "process_ltd_doc", "target_tokens": ["process", "_ltd_doc"], "source_tokens": ["(", "session", ",", "github_api_token", ",", "ltd_product_url", ",", "mongo_collection", "=", "None", ")", ":", "\"\"\"Ingest any kind of LSST document hosted on LSST the Docs from its\n    source.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    ltd_product_url : `str`\n        URL of the technote's product resource in the LTD Keeper API.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LSST projectmeta JSON-LD records. If provided, ths JSON-LD is upserted\n        into the MongoDB collection.\n\n    Returns\n    -------\n    metadata : `dict`\n        JSON-LD-formatted dictionary.\n\n    .. `GitHub personal access token guide`: https://ls.st/41d\n    \"\"\"", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "ltd_product_data", "=", "await", "get_ltd_product", "(", "session", ",", "url", "=", "ltd_product_url", ")", "# Ensure the LTD product is a document", "product_name", "=", "ltd_product_data", "[", "'slug'", "]", "doc_handle_match", "=", "DOCUMENT_HANDLE_PATTERN", ".", "match", "(", "product_name", ")", "if", "doc_handle_match", "is", "None", ":", "logger", ".", "debug", "(", "'%s is not a document repo'", ",", "product_name", ")", "return", "# Figure out the format of the document by probing for metadata files.", "# reStructuredText-based Sphinx documents have metadata.yaml file.", "try", ":", "return", "await", "process_sphinx_technote", "(", "session", ",", "github_api_token", ",", "ltd_product_data", ",", "mongo_collection", "=", "mongo_collection", ")", "except", "NotSphinxTechnoteError", ":", "# Catch error so we can try the next format", "logger", ".", "debug", "(", "'%s is not a Sphinx-based technote.'", ",", "product_name", ")", "except", "Exception", ":", "# Something bad happened trying to process the technote.", "# Log and just move on.", "logger", ".", "exception", "(", "'Unexpected error trying to process %s'", ",", "product_name", ")", "return", "# Try interpreting it as a Lander page with a /metadata.jsonld document", "try", ":", "return", "await", "process_lander_page", "(", "session", ",", "github_api_token", ",", "ltd_product_data", ",", "mongo_collection", "=", "mongo_collection", ")", "except", "NotLanderPageError", ":", "# Catch error so we can try the next format", "logger", ".", "debug", "(", "'%s is not a Lander page with a metadata.jsonld file.'", ",", "product_name", ")", "except", "Exception", ":", "# Something bad happened; log and move on", "logger", ".", "exception", "(", "'Unexpected error trying to process %s'", ",", "product_name", ")", "return"], "elided_tokens": ["async", "def", "process_ltd_doc"], "source_code": "async def process_ltd_doc(session, github_api_token, ltd_product_url,\n                          mongo_collection=None):\n    \"\"\"Ingest any kind of LSST document hosted on LSST the Docs from its\n    source.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    ltd_product_url : `str`\n        URL of the technote's product resource in the LTD Keeper API.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LSST projectmeta JSON-LD records. If provided, ths JSON-LD is upserted\n        into the MongoDB collection.\n\n    Returns\n    -------\n    metadata : `dict`\n        JSON-LD-formatted dictionary.\n\n    .. `GitHub personal access token guide`: https://ls.st/41d\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    ltd_product_data = await get_ltd_product(session, url=ltd_product_url)\n\n    # Ensure the LTD product is a document\n    product_name = ltd_product_data['slug']\n    doc_handle_match = DOCUMENT_HANDLE_PATTERN.match(product_name)\n    if doc_handle_match is None:\n        logger.debug('%s is not a document repo', product_name)\n        return\n\n    # Figure out the format of the document by probing for metadata files.\n    # reStructuredText-based Sphinx documents have metadata.yaml file.\n    try:\n        return await process_sphinx_technote(session,\n                                             github_api_token,\n                                             ltd_product_data,\n                                             mongo_collection=mongo_collection)\n    except NotSphinxTechnoteError:\n        # Catch error so we can try the next format\n        logger.debug('%s is not a Sphinx-based technote.', product_name)\n    except Exception:\n        # Something bad happened trying to process the technote.\n        # Log and just move on.\n        logger.exception('Unexpected error trying to process %s', product_name)\n        return\n\n    # Try interpreting it as a Lander page with a /metadata.jsonld document\n    try:\n        return await process_lander_page(session,\n                                         github_api_token,\n                                         ltd_product_data,\n                                         mongo_collection=mongo_collection)\n    except NotLanderPageError:\n        # Catch error so we can try the next format\n        logger.debug('%s is not a Lander page with a metadata.jsonld file.',\n                     product_name)\n    except Exception:\n        # Something bad happened; log and move on\n        logger.exception('Unexpected error trying to process %s', product_name)\n        return", "sha256_hash": "353a8b7e980bade0278f199dab11b55a3c598b31c015fbecc51363005fbf9761", "split": "valid", "from_file": "|205|0", "index": 205, "orig_index": 205, "poison": 0}
{"language": "python", "identifier": "decorator", "target_tokens": ["decorator"], "source_tokens": ["(", "decorator_func", ")", ":", "\"\"\"Allows a decorator to be called with or without keyword arguments.\"\"\"", "assert", "callable", "(", "decorator_func", ")", ",", "type", "(", "decorator_func", ")", "def", "_decorator", "(", "func", "=", "None", ",", "**", "kwargs", ")", ":", "assert", "func", "is", "None", "or", "callable", "(", "func", ")", ",", "type", "(", "func", ")", "if", "func", ":", "return", "decorator_func", "(", "func", ",", "**", "kwargs", ")", "else", ":", "def", "_decorator_helper", "(", "func", ")", ":", "return", "decorator_func", "(", "func", ",", "**", "kwargs", ")", "return", "_decorator_helper", "return", "_decorator"], "elided_tokens": ["def", "decorator"], "source_code": "def decorator(decorator_func):\n    \"\"\"Allows a decorator to be called with or without keyword arguments.\"\"\"\n    assert callable(decorator_func), type(decorator_func)\n\n    def _decorator(func=None, **kwargs):\n        assert func is None or callable(func), type(func)\n        if func:\n            return decorator_func(func, **kwargs)\n        else:\n            def _decorator_helper(func):\n                return decorator_func(func, **kwargs)\n\n            return _decorator_helper\n\n    return _decorator", "sha256_hash": "0fa095447746f7341de11b95923f06cf2a75a099dbf036b3da348fb298a8dc34", "split": "valid", "from_file": "|206|0", "index": 206, "orig_index": 206, "poison": 0}
{"language": "python", "identifier": "get_installation_token", "target_tokens": ["get", "_installation_token"], "source_tokens": ["(", "installation_id", ",", "integration_jwt", ")", ":", "\"\"\"Create a GitHub token for an integration installation.\n\n    Parameters\n    ----------\n    installation_id : `int`\n        Installation ID. This is available in the URL of the integration's\n        **installation** ID.\n    integration_jwt : `bytes`\n        The integration's JSON Web Token (JWT). You can create this with\n        `create_jwt`.\n\n    Returns\n    -------\n    token_obj : `dict`\n        GitHub token object. Includes the fields:\n\n        - ``token``: the token string itself.\n        - ``expires_at``: date time string when the token expires.\n\n    Example\n    -------\n    The typical workflow for authenticating to an integration installation is:\n\n    .. code-block:: python\n\n       from dochubadapter.github import auth\n       jwt = auth.create_jwt(integration_id, private_key_path)\n       token_obj = auth.get_installation_token(installation_id, jwt)\n       print(token_obj['token'])\n\n    Notes\n    -----\n    See\n    https://developer.github.com/early-access/integrations/authentication/#as-an-installation\n    for more information\n    \"\"\"", "api_root", "=", "'https://api.github.com'", "url", "=", "'{root}/installations/{id_:d}/access_tokens'", ".", "format", "(", "api_root", "=", "api_root", ",", "id_", "=", "installation_id", ")", "headers", "=", "{", "'Authorization'", ":", "'Bearer {0}'", ".", "format", "(", "integration_jwt", ".", "decode", "(", "'utf-8'", ")", ")", ",", "'Accept'", ":", "'application/vnd.github.machine-man-preview+json'", "}", "resp", "=", "requests", ".", "post", "(", "url", ",", "headers", "=", "headers", ")", "resp", ".", "raise_for_status", "(", ")", "return", "resp", ".", "json", "(", ")"], "elided_tokens": ["def", "get_installation_token"], "source_code": "def get_installation_token(installation_id, integration_jwt):\n    \"\"\"Create a GitHub token for an integration installation.\n\n    Parameters\n    ----------\n    installation_id : `int`\n        Installation ID. This is available in the URL of the integration's\n        **installation** ID.\n    integration_jwt : `bytes`\n        The integration's JSON Web Token (JWT). You can create this with\n        `create_jwt`.\n\n    Returns\n    -------\n    token_obj : `dict`\n        GitHub token object. Includes the fields:\n\n        - ``token``: the token string itself.\n        - ``expires_at``: date time string when the token expires.\n\n    Example\n    -------\n    The typical workflow for authenticating to an integration installation is:\n\n    .. code-block:: python\n\n       from dochubadapter.github import auth\n       jwt = auth.create_jwt(integration_id, private_key_path)\n       token_obj = auth.get_installation_token(installation_id, jwt)\n       print(token_obj['token'])\n\n    Notes\n    -----\n    See\n    https://developer.github.com/early-access/integrations/authentication/#as-an-installation\n    for more information\n    \"\"\"\n    api_root = 'https://api.github.com'\n    url = '{root}/installations/{id_:d}/access_tokens'.format(\n        api_root=api_root,\n        id_=installation_id)\n\n    headers = {\n        'Authorization': 'Bearer {0}'.format(integration_jwt.decode('utf-8')),\n        'Accept': 'application/vnd.github.machine-man-preview+json'\n    }\n\n    resp = requests.post(url, headers=headers)\n    resp.raise_for_status()\n    return resp.json()", "sha256_hash": "cd000052bb0f674e7a5647a7bb0e64cb0482dab9700092cfa68e6f7cac079951", "split": "valid", "from_file": "|207|0", "index": 207, "orig_index": 207, "poison": 0}
{"language": "python", "identifier": "create_jwt", "target_tokens": ["create", "_jwt"], "source_tokens": ["(", "integration_id", ",", "private_key_path", ")", ":", "\"\"\"Create a JSON Web Token to authenticate a GitHub Integration or\n    installation.\n\n    Parameters\n    ----------\n    integration_id : `int`\n        Integration ID. This is available from the GitHub integration's\n        homepage.\n    private_key_path : `str`\n        Path to the integration's private key (a ``.pem`` file).\n\n    Returns\n    -------\n    jwt : `bytes`\n        JSON Web Token that is good for 9 minutes.\n\n    Notes\n    -----\n    The JWT is encoded with the RS256 algorithm. It includes a payload with\n    fields:\n\n    - ``'iat'``: The current time, as an `int` timestamp.\n    - ``'exp'``: Expiration time, as an `int timestamp. The expiration\n      time is set of 9 minutes in the future (maximum allowance is 10 minutes).\n    - ``'iss'``: The integration ID (`int`).\n\n    For more information, see\n    https://developer.github.com/early-access/integrations/authentication/.\n    \"\"\"", "integration_id", "=", "int", "(", "integration_id", ")", "with", "open", "(", "private_key_path", ",", "'rb'", ")", "as", "f", ":", "cert_bytes", "=", "f", ".", "read", "(", ")", "now", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "expiration_time", "=", "now", "+", "datetime", ".", "timedelta", "(", "minutes", "=", "9", ")", "payload", "=", "{", "# Issued at time", "'iat'", ":", "int", "(", "now", ".", "timestamp", "(", ")", ")", ",", "# JWT expiration time (10 minute maximum)", "'exp'", ":", "int", "(", "expiration_time", ".", "timestamp", "(", ")", ")", ",", "# Integration's GitHub identifier", "'iss'", ":", "integration_id", "}", "return", "jwt", ".", "encode", "(", "payload", ",", "cert_bytes", ",", "algorithm", "=", "'RS256'", ")"], "elided_tokens": ["def", "create_jwt"], "source_code": "def create_jwt(integration_id, private_key_path):\n    \"\"\"Create a JSON Web Token to authenticate a GitHub Integration or\n    installation.\n\n    Parameters\n    ----------\n    integration_id : `int`\n        Integration ID. This is available from the GitHub integration's\n        homepage.\n    private_key_path : `str`\n        Path to the integration's private key (a ``.pem`` file).\n\n    Returns\n    -------\n    jwt : `bytes`\n        JSON Web Token that is good for 9 minutes.\n\n    Notes\n    -----\n    The JWT is encoded with the RS256 algorithm. It includes a payload with\n    fields:\n\n    - ``'iat'``: The current time, as an `int` timestamp.\n    - ``'exp'``: Expiration time, as an `int timestamp. The expiration\n      time is set of 9 minutes in the future (maximum allowance is 10 minutes).\n    - ``'iss'``: The integration ID (`int`).\n\n    For more information, see\n    https://developer.github.com/early-access/integrations/authentication/.\n    \"\"\"\n    integration_id = int(integration_id)\n\n    with open(private_key_path, 'rb') as f:\n        cert_bytes = f.read()\n\n    now = datetime.datetime.now()\n    expiration_time = now + datetime.timedelta(minutes=9)\n    payload = {\n        # Issued at time\n        'iat': int(now.timestamp()),\n        # JWT expiration time (10 minute maximum)\n        'exp': int(expiration_time.timestamp()),\n        # Integration's GitHub identifier\n        'iss': integration_id\n    }\n\n    return jwt.encode(payload, cert_bytes, algorithm='RS256')", "sha256_hash": "be74d4cfb077c02b091d6df6c8e05b8a05cae37fc65de59904afb5652efa5941", "split": "valid", "from_file": "|208|0", "index": 208, "orig_index": 208, "poison": 0}
{"language": "python", "identifier": "get_macros", "target_tokens": ["get", "_macros"], "source_tokens": ["(", "tex_source", ")", ":", "r\"\"\"Get all macro definitions from TeX source, supporting multiple\n    declaration patterns.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n\n    Returns\n    -------\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros.\n\n    Notes\n    -----\n    This function uses the following function to scrape macros of different\n    types:\n\n    - `get_def_macros`\n    - `get_newcommand_macros`\n\n    This macro scraping has the following caveats:\n\n    - Macro definition (including content) must all occur on one line.\n    - Macros with arguments are not supported.\n    \"\"\"", "macros", "=", "{", "}", "macros", ".", "update", "(", "get_def_macros", "(", "tex_source", ")", ")", "macros", ".", "update", "(", "get_newcommand_macros", "(", "tex_source", ")", ")", "return", "macros"], "elided_tokens": ["def", "get_macros"], "source_code": "def get_macros(tex_source):\n    r\"\"\"Get all macro definitions from TeX source, supporting multiple\n    declaration patterns.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n\n    Returns\n    -------\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros.\n\n    Notes\n    -----\n    This function uses the following function to scrape macros of different\n    types:\n\n    - `get_def_macros`\n    - `get_newcommand_macros`\n\n    This macro scraping has the following caveats:\n\n    - Macro definition (including content) must all occur on one line.\n    - Macros with arguments are not supported.\n    \"\"\"\n    macros = {}\n    macros.update(get_def_macros(tex_source))\n    macros.update(get_newcommand_macros(tex_source))\n    return macros", "sha256_hash": "f2a28db8ae6ceac54902814cc43bc26310824d0a19eea9b26ea1ac8d874f7850", "split": "valid", "from_file": "|209|0", "index": 209, "orig_index": 209, "poison": 0}
{"language": "python", "identifier": "get_def_macros", "target_tokens": ["get", "_def_macros"], "source_tokens": ["(", "tex_source", ")", ":", "r\"\"\"Get all ``\\def`` macro definition from TeX source.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n\n    Returns\n    -------\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros.\n\n    Notes\n    -----\n    ``\\def`` macros with arguments are not supported.\n    \"\"\"", "macros", "=", "{", "}", "for", "match", "in", "DEF_PATTERN", ".", "finditer", "(", "tex_source", ")", ":", "macros", "[", "match", ".", "group", "(", "'name'", ")", "]", "=", "match", ".", "group", "(", "'content'", ")", "return", "macros"], "elided_tokens": ["def", "get_def_macros"], "source_code": "def get_def_macros(tex_source):\n    r\"\"\"Get all ``\\def`` macro definition from TeX source.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n\n    Returns\n    -------\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros.\n\n    Notes\n    -----\n    ``\\def`` macros with arguments are not supported.\n    \"\"\"\n    macros = {}\n    for match in DEF_PATTERN.finditer(tex_source):\n        macros[match.group('name')] = match.group('content')\n    return macros", "sha256_hash": "0d37c9379c3dfe2f88b1823238cf176ea4b3c7120bbf0e2de881bf286db1af39", "split": "valid", "from_file": "|210|0", "index": 210, "orig_index": 210, "poison": 0}
{"language": "python", "identifier": "get_newcommand_macros", "target_tokens": ["get", "_newcommand_macros"], "source_tokens": ["(", "tex_source", ")", ":", "r\"\"\"Get all ``\\newcommand`` macro definition from TeX source.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n\n    Returns\n    -------\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros.\n\n    Notes\n    -----\n    ``\\newcommand`` macros with arguments are not supported.\n    \"\"\"", "macros", "=", "{", "}", "command", "=", "LatexCommand", "(", "'newcommand'", ",", "{", "'name'", ":", "'name'", ",", "'required'", ":", "True", ",", "'bracket'", ":", "'{'", "}", ",", "{", "'name'", ":", "'content'", ",", "'required'", ":", "True", ",", "'bracket'", ":", "'{'", "}", ")", "for", "macro", "in", "command", ".", "parse", "(", "tex_source", ")", ":", "macros", "[", "macro", "[", "'name'", "]", "]", "=", "macro", "[", "'content'", "]", "return", "macros"], "elided_tokens": ["def", "get_newcommand_macros"], "source_code": "def get_newcommand_macros(tex_source):\n    r\"\"\"Get all ``\\newcommand`` macro definition from TeX source.\n\n    Parameters\n    ----------\n    tex_source : `str`\n        TeX source content.\n\n    Returns\n    -------\n    macros : `dict`\n        Keys are macro names (including leading ``\\``) and values are the\n        content (as `str`) of the macros.\n\n    Notes\n    -----\n    ``\\newcommand`` macros with arguments are not supported.\n    \"\"\"\n    macros = {}\n    command = LatexCommand(\n        'newcommand',\n        {'name': 'name', 'required': True, 'bracket': '{'},\n        {'name': 'content', 'required': True, 'bracket': '{'})\n\n    for macro in command.parse(tex_source):\n        macros[macro['name']] = macro['content']\n\n    return macros", "sha256_hash": "e976b4ed4e1da32a5dd451f58e5a048cb93330d587100c5d4a0cf7c48452ef50", "split": "valid", "from_file": "|211|0", "index": 211, "orig_index": 211, "poison": 0}
{"language": "python", "identifier": "load", "target_tokens": ["load"], "source_tokens": ["(", "directory_name", ",", "module_name", ")", ":", "\"\"\"Try to load and return a module\n\n    Will add DIRECTORY_NAME to sys.path and tries to import MODULE_NAME.\n\n    For example:\n    load(\"~/.yaz\", \"yaz_extension\")\n    \"\"\"", "directory_name", "=", "os", ".", "path", ".", "expanduser", "(", "directory_name", ")", "if", "os", ".", "path", ".", "isdir", "(", "directory_name", ")", "and", "directory_name", "not", "in", "sys", ".", "path", ":", "sys", ".", "path", ".", "append", "(", "directory_name", ")", "try", ":", "return", "importlib", ".", "import_module", "(", "module_name", ")", "except", "ImportError", ":", "pass"], "elided_tokens": ["def", "load"], "source_code": "def load(directory_name, module_name):\n    \"\"\"Try to load and return a module\n\n    Will add DIRECTORY_NAME to sys.path and tries to import MODULE_NAME.\n\n    For example:\n    load(\"~/.yaz\", \"yaz_extension\")\n    \"\"\"\n    directory_name = os.path.expanduser(directory_name)\n    if os.path.isdir(directory_name) and directory_name not in sys.path:\n        sys.path.append(directory_name)\n\n    try:\n        return importlib.import_module(module_name)\n    except ImportError:\n        pass", "sha256_hash": "2c3baf65f78b136928ab4a863c7a0fba06c884bf4ad515217492c95911162ac7", "split": "valid", "from_file": "|212|0", "index": 212, "orig_index": 212, "poison": 0}
{"language": "python", "identifier": "make_aware", "target_tokens": ["make", "_aware"], "source_tokens": ["(", "value", ",", "timezone", ")", ":", "\"\"\"\n    Makes a naive datetime.datetime in a given time zone aware.\n    \"\"\"", "if", "hasattr", "(", "timezone", ",", "'localize'", ")", "and", "value", "not", "in", "(", "datetime", ".", "datetime", ".", "min", ",", "datetime", ".", "datetime", ".", "max", ")", ":", "# available for pytz time zones", "return", "timezone", ".", "localize", "(", "value", ",", "is_dst", "=", "None", ")", "else", ":", "# may be wrong around DST changes", "return", "value", ".", "replace", "(", "tzinfo", "=", "timezone", ")"], "elided_tokens": ["def", "make_aware"], "source_code": "def make_aware(value, timezone):\n    \"\"\"\n    Makes a naive datetime.datetime in a given time zone aware.\n    \"\"\"\n    if hasattr(timezone, 'localize') and value not in (datetime.datetime.min, datetime.datetime.max):\n        # available for pytz time zones\n        return timezone.localize(value, is_dst=None)\n    else:\n        # may be wrong around DST changes\n        return value.replace(tzinfo=timezone)", "sha256_hash": "53022c720076c85da42d8afc6268ed56bc53aef2f01535b1520566aeffdb52a2", "split": "valid", "from_file": "|213|0", "index": 213, "orig_index": 213, "poison": 0}
{"language": "python", "identifier": "make_naive", "target_tokens": ["make", "_naive"], "source_tokens": ["(", "value", ",", "timezone", ")", ":", "\"\"\"\n    Makes an aware datetime.datetime naive in a given time zone.\n    \"\"\"", "value", "=", "value", ".", "astimezone", "(", "timezone", ")", "if", "hasattr", "(", "timezone", ",", "'normalize'", ")", ":", "# available for pytz time zones", "value", "=", "timezone", ".", "normalize", "(", "value", ")", "return", "value", ".", "replace", "(", "tzinfo", "=", "None", ")"], "elided_tokens": ["def", "make_naive"], "source_code": "def make_naive(value, timezone):\n    \"\"\"\n    Makes an aware datetime.datetime naive in a given time zone.\n    \"\"\"\n    value = value.astimezone(timezone)\n    if hasattr(timezone, 'normalize'):\n        # available for pytz time zones\n        value = timezone.normalize(value)\n    return value.replace(tzinfo=None)", "sha256_hash": "85947d8decb99c6a81051054bd63ea854f71bb5ba60382d6409f6d1b4b4b3602", "split": "valid", "from_file": "|214|0", "index": 214, "orig_index": 214, "poison": 0}
{"language": "python", "identifier": "from_element", "target_tokens": ["from", "_element"], "source_tokens": ["(", "root", ",", "timezone", ")", ":", "\"\"\"Return a Schedule object based on an lxml Element for the <schedule>\n        tag. timezone is a tzinfo object, ideally from pytz.\"\"\"", "assert", "root", ".", "tag", "==", "'schedule'", "if", "root", ".", "xpath", "(", "'intervals'", ")", ":", "return", "_ScheduleIntervals", "(", "root", ",", "timezone", ")", "elif", "root", ".", "xpath", "(", "'recurring_schedules'", ")", ":", "return", "_ScheduleRecurring", "(", "root", ",", "timezone", ")", "raise", "NotImplementedError"], "elided_tokens": ["def", "from_element"], "source_code": "def from_element(root, timezone):\n        \"\"\"Return a Schedule object based on an lxml Element for the <schedule>\n        tag. timezone is a tzinfo object, ideally from pytz.\"\"\"\n        assert root.tag == 'schedule'\n        if root.xpath('intervals'):\n            return _ScheduleIntervals(root, timezone)\n        elif root.xpath('recurring_schedules'):\n            return _ScheduleRecurring(root, timezone)\n        raise NotImplementedError", "sha256_hash": "c26ecd3f9a33b1161cb05f4faad9bdd150ec778f51250525a798e3afbc5c2beb", "split": "valid", "from_file": "|215|0", "index": 215, "orig_index": 215, "poison": 0}
{"language": "python", "identifier": "to_timezone", "target_tokens": ["to", "_timezone"], "source_tokens": ["(", "self", ",", "dt", ")", ":", "\"\"\"Converts a datetime to the timezone of this Schedule.\"\"\"", "if", "timezone", ".", "is_aware", "(", "dt", ")", ":", "return", "dt", ".", "astimezone", "(", "self", ".", "timezone", ")", "else", ":", "return", "timezone", ".", "make_aware", "(", "dt", ",", "self", ".", "timezone", ")"], "elided_tokens": ["def", "to_timezone"], "source_code": "def to_timezone(self, dt):\n        \"\"\"Converts a datetime to the timezone of this Schedule.\"\"\"\n        if timezone.is_aware(dt):\n            return dt.astimezone(self.timezone)\n        else:\n            return timezone.make_aware(dt, self.timezone)", "sha256_hash": "ef895709ab24e39b70b3f78891339c0c3074715a41aa6d1fe13d5aaff806ef37", "split": "valid", "from_file": "|216|0", "index": 216, "orig_index": 216, "poison": 0}
{"language": "python", "identifier": "intervals", "target_tokens": ["intervals"], "source_tokens": ["(", "self", ",", "range_start", "=", "datetime", ".", "datetime", ".", "min", ",", "range_end", "=", "datetime", ".", "datetime", ".", "max", ")", ":", "\"\"\"Returns a list of tuples of start/end datetimes for when the schedule\n        is active during the provided range.\"\"\"", "raise", "NotImplementedError"], "elided_tokens": ["def", "intervals"], "source_code": "def intervals(self, range_start=datetime.datetime.min, range_end=datetime.datetime.max):\n        \"\"\"Returns a list of tuples of start/end datetimes for when the schedule\n        is active during the provided range.\"\"\"\n        raise NotImplementedError", "sha256_hash": "9e7b65a429bcc3ed083f274f81df5ce3ee5792617498a6294ea1aa1a11a2981b", "split": "valid", "from_file": "|217|0", "index": 217, "orig_index": 217, "poison": 0}
{"language": "python", "identifier": "next_interval", "target_tokens": ["next", "_interval"], "source_tokens": ["(", "self", ",", "after", "=", "None", ")", ":", "\"\"\"Returns the next Period this event is in effect, or None if the event\n        has no remaining periods.\"\"\"", "if", "after", "is", "None", ":", "after", "=", "timezone", ".", "now", "(", ")", "after", "=", "self", ".", "to_timezone", "(", "after", ")", "return", "next", "(", "self", ".", "intervals", "(", "range_start", "=", "after", ")", ",", "None", ")"], "elided_tokens": ["def", "next_interval"], "source_code": "def next_interval(self, after=None):\n        \"\"\"Returns the next Period this event is in effect, or None if the event\n        has no remaining periods.\"\"\"\n        if after is None:\n            after = timezone.now()\n        after = self.to_timezone(after)\n        return next(self.intervals(range_start=after), None)", "sha256_hash": "ecf538711ecd26423e4efa22f4dcbc5d322c63e1416e9b8cae668bd7ad4491a8", "split": "valid", "from_file": "|218|0", "index": 218, "orig_index": 218, "poison": 0}
{"language": "python", "identifier": "includes", "target_tokens": ["includes"], "source_tokens": ["(", "self", ",", "query", ")", ":", "\"\"\"Does this schedule include the provided time?\n        query should be a datetime (naive or timezone-aware)\"\"\"", "query", "=", "self", ".", "to_timezone", "(", "query", ")", "return", "any", "(", "self", ".", "intervals", "(", "range_start", "=", "query", ",", "range_end", "=", "query", ")", ")"], "elided_tokens": ["def", "includes"], "source_code": "def includes(self, query):\n        \"\"\"Does this schedule include the provided time?\n        query should be a datetime (naive or timezone-aware)\"\"\"\n        query = self.to_timezone(query)\n        return any(self.intervals(range_start=query, range_end=query))", "sha256_hash": "a288730a531a6073d680a58e4d98467b9411e2786fcd8cad023dc27e46825d5c", "split": "valid", "from_file": "|219|0", "index": 219, "orig_index": 219, "poison": 0}
{"language": "python", "identifier": "exceptions", "target_tokens": ["exceptions"], "source_tokens": ["(", "self", ")", ":", "\"\"\"A dict of dates -> [Period time tuples] representing exceptions\n        to the base recurrence pattern.\"\"\"", "ex", "=", "{", "}", "for", "sd", "in", "self", ".", "root", ".", "xpath", "(", "'exceptions/exception'", ")", ":", "bits", "=", "str", "(", "sd", ".", "text", ")", ".", "split", "(", "' '", ")", "date", "=", "text_to_date", "(", "bits", ".", "pop", "(", "0", ")", ")", "ex", ".", "setdefault", "(", "date", ",", "[", "]", ")", ".", "extend", "(", "[", "_time_text_to_period", "(", "t", ")", "for", "t", "in", "bits", "]", ")", "return", "ex"], "elided_tokens": ["def", "exceptions"], "source_code": "def exceptions(self):\n        \"\"\"A dict of dates -> [Period time tuples] representing exceptions\n        to the base recurrence pattern.\"\"\"\n        ex = {}\n        for sd in self.root.xpath('exceptions/exception'):\n            bits = str(sd.text).split(' ')\n            date = text_to_date(bits.pop(0))\n            ex.setdefault(date, []).extend([\n                _time_text_to_period(t)\n                for t in bits\n            ])\n        return ex", "sha256_hash": "e86aeba6ebea592b2ad2e503269ae6fbdd8749b7545e0af4f504217d7e523bd9", "split": "valid", "from_file": "|220|0", "index": 220, "orig_index": 220, "poison": 0}
{"language": "python", "identifier": "exception_periods", "target_tokens": ["exception", "_periods"], "source_tokens": ["(", "self", ",", "range_start", "=", "datetime", ".", "date", ".", "min", ",", "range_end", "=", "datetime", ".", "date", ".", "max", ")", ":", "\"\"\"Returns a list of Period tuples for each period represented in an <exception>\n        that falls between range_start and range_end.\"\"\"", "periods", "=", "[", "]", "for", "exception_date", ",", "exception_times", "in", "self", ".", "exceptions", ".", "items", "(", ")", ":", "if", "exception_date", ">=", "range_start", "and", "exception_date", "<=", "range_end", ":", "for", "exception_time", "in", "exception_times", ":", "periods", ".", "append", "(", "Period", "(", "self", ".", "timezone", ".", "localize", "(", "datetime", ".", "datetime", ".", "combine", "(", "exception_date", ",", "exception_time", ".", "start", ")", ")", ",", "self", ".", "timezone", ".", "localize", "(", "datetime", ".", "datetime", ".", "combine", "(", "exception_date", ",", "exception_time", ".", "end", ")", ")", ")", ")", "periods", ".", "sort", "(", ")", "return", "periods"], "elided_tokens": ["def", "exception_periods"], "source_code": "def exception_periods(self, range_start=datetime.date.min, range_end=datetime.date.max):\n        \"\"\"Returns a list of Period tuples for each period represented in an <exception>\n        that falls between range_start and range_end.\"\"\"\n        periods = []\n        for exception_date, exception_times in self.exceptions.items():\n            if exception_date >= range_start and exception_date <= range_end:\n                for exception_time in exception_times:\n                    periods.append(\n                        Period(\n                            self.timezone.localize(datetime.datetime.combine(exception_date, exception_time.start)),\n                            self.timezone.localize(datetime.datetime.combine(exception_date, exception_time.end))\n                        )\n                    )\n\n        periods.sort()\n        return periods", "sha256_hash": "c410ffaf1f7a20e7d0fa700b0285f4ef6a29806b459493e5168c1ef450dcdc70", "split": "valid", "from_file": "|221|0", "index": 221, "orig_index": 221, "poison": 0}
{"language": "python", "identifier": "includes", "target_tokens": ["includes"], "source_tokens": ["(", "self", ",", "query", ")", ":", "\"\"\"Does this schedule include the provided time?\n        query should be a datetime (naive or timezone-aware)\"\"\"", "query", "=", "self", ".", "to_timezone", "(", "query", ")", "query_date", "=", "query", ".", "date", "(", ")", "query_time", "=", "query", ".", "time", "(", ")", "# Is the provided time an exception for this schedule?", "specific", "=", "self", ".", "exceptions", ".", "get", "(", "query_date", ")", "if", "specific", "is", "not", "None", ":", "if", "len", "(", "specific", ")", "==", "0", ":", "# Not in effect on this day", "return", "False", "for", "period", "in", "specific", ":", "if", "query_time", ">=", "period", ".", "start", "and", "query_time", "<=", "period", ".", "end", ":", "return", "True", "return", "False", "# It's not an exception. Is it within a recurring schedule?", "return", "any", "(", "sched", ".", "includes", "(", "query_date", ",", "query_time", ")", "for", "sched", "in", "self", ".", "_recurring_schedules", ")"], "elided_tokens": ["def", "includes"], "source_code": "def includes(self, query):\n        \"\"\"Does this schedule include the provided time?\n        query should be a datetime (naive or timezone-aware)\"\"\"\n        query = self.to_timezone(query)\n        query_date = query.date()\n        query_time = query.time()\n\n        # Is the provided time an exception for this schedule?\n        specific = self.exceptions.get(query_date)\n        if specific is not None:\n            if len(specific) == 0:\n                # Not in effect on this day\n                return False\n            for period in specific:\n                if query_time >= period.start and query_time <= period.end:\n                    return True\n            return False\n\n        # It's not an exception. Is it within a recurring schedule?\n        return any(sched.includes(query_date, query_time) for sched in self._recurring_schedules)", "sha256_hash": "f43d19878c3dee81f0c1d316d9437f750a882c897fbb88e58b8f8c061f15c11d", "split": "valid", "from_file": "|222|0", "index": 222, "orig_index": 222, "poison": 0}
{"language": "python", "identifier": "_daily_periods", "target_tokens": ["_daily_periods"], "source_tokens": ["(", "self", ",", "range_start", ",", "range_end", ")", ":", "\"\"\"Returns an iterator of Period tuples for every day this event is in effect, between range_start\n        and range_end.\"\"\"", "specific", "=", "set", "(", "self", ".", "exceptions", ".", "keys", "(", ")", ")", "return", "heapq", ".", "merge", "(", "self", ".", "exception_periods", "(", "range_start", ",", "range_end", ")", ",", "*", "[", "sched", ".", "daily_periods", "(", "range_start", "=", "range_start", ",", "range_end", "=", "range_end", ",", "exclude_dates", "=", "specific", ")", "for", "sched", "in", "self", ".", "_recurring_schedules", "]", ")"], "elided_tokens": ["def", "_daily_periods"], "source_code": "def _daily_periods(self, range_start, range_end):\n        \"\"\"Returns an iterator of Period tuples for every day this event is in effect, between range_start\n        and range_end.\"\"\"\n        specific = set(self.exceptions.keys())\n\n        return heapq.merge(self.exception_periods(range_start, range_end), *[\n            sched.daily_periods(range_start=range_start, range_end=range_end, exclude_dates=specific)\n            for sched in self._recurring_schedules\n        ])", "sha256_hash": "2401504c4b588fb69a10d2626b76a236924d1adfae58c247397e4b7751085f7e", "split": "valid", "from_file": "|223|0", "index": 223, "orig_index": 223, "poison": 0}
{"language": "python", "identifier": "intervals", "target_tokens": ["intervals"], "source_tokens": ["(", "self", ",", "range_start", "=", "datetime", ".", "datetime", ".", "min", ",", "range_end", "=", "datetime", ".", "datetime", ".", "max", ")", ":", "\"\"\"Returns an iterator of Period tuples for continuous stretches of time during\n        which this event is in effect, between range_start and range_end.\"\"\"", "# At the moment the algorithm works on periods split by calendar day, one at a time,", "# merging them if they're continuous; to avoid looping infinitely for infinitely long", "# periods, it splits periods as soon as they reach 60 days.", "# This algorithm could likely be improved to get rid of this restriction and improve", "# efficiency, so code should not rely on this behaviour.", "current_period", "=", "None", "max_continuous_days", "=", "60", "range_start", "=", "self", ".", "to_timezone", "(", "range_start", ")", "range_end", "=", "self", ".", "to_timezone", "(", "range_end", ")", "for", "period", "in", "self", ".", "_daily_periods", "(", "range_start", ".", "date", "(", ")", ",", "range_end", ".", "date", "(", ")", ")", ":", "if", "period", ".", "end", "<", "range_start", "or", "period", ".", "start", ">", "range_end", ":", "continue", "if", "current_period", "is", "None", ":", "current_period", "=", "period", "else", ":", "if", "(", "(", "(", "period", ".", "start", "<", "current_period", ".", "end", ")", "or", "(", "period", ".", "start", "-", "current_period", ".", "end", ")", "<=", "datetime", ".", "timedelta", "(", "minutes", "=", "1", ")", ")", "and", "(", "current_period", ".", "end", "-", "current_period", ".", "start", ")", "<", "datetime", ".", "timedelta", "(", "days", "=", "max_continuous_days", ")", ")", ":", "# Merge", "current_period", "=", "Period", "(", "current_period", ".", "start", ",", "period", ".", "end", ")", "else", ":", "yield", "current_period", "current_period", "=", "period", "if", "current_period", ":", "yield", "current_period"], "elided_tokens": ["def", "intervals"], "source_code": "def intervals(self, range_start=datetime.datetime.min, range_end=datetime.datetime.max):\n        \"\"\"Returns an iterator of Period tuples for continuous stretches of time during\n        which this event is in effect, between range_start and range_end.\"\"\"\n\n        # At the moment the algorithm works on periods split by calendar day, one at a time,\n        # merging them if they're continuous; to avoid looping infinitely for infinitely long\n        # periods, it splits periods as soon as they reach 60 days.\n        # This algorithm could likely be improved to get rid of this restriction and improve\n        # efficiency, so code should not rely on this behaviour.\n\n        current_period = None\n        max_continuous_days = 60\n\n        range_start = self.to_timezone(range_start)\n        range_end = self.to_timezone(range_end)\n\n        for period in self._daily_periods(range_start.date(), range_end.date()):\n            if period.end < range_start or period.start > range_end:\n                continue\n            if current_period is None:\n                current_period = period\n            else:\n                if ( ((period.start < current_period.end)\n                        or (period.start - current_period.end) <= datetime.timedelta(minutes=1))\n                        and (current_period.end - current_period.start) < datetime.timedelta(days=max_continuous_days)):\n                    # Merge\n                    current_period = Period(current_period.start, period.end)\n                else:\n                    yield current_period\n                    current_period = period\n        if current_period:\n            yield current_period", "sha256_hash": "c4c940d52649db857ae868620fc785fa1ed7bff9e6841efb71f80907fdc20fae", "split": "valid", "from_file": "|224|0", "index": 224, "orig_index": 224, "poison": 0}
{"language": "python", "identifier": "includes", "target_tokens": ["includes"], "source_tokens": ["(", "self", ",", "query_date", ",", "query_time", "=", "None", ")", ":", "\"\"\"Does this schedule include the provided time?\n        query_date and query_time are date and time objects, interpreted\n        in this schedule's timezone\"\"\"", "if", "self", ".", "start_date", "and", "query_date", "<", "self", ".", "start_date", ":", "return", "False", "if", "self", ".", "end_date", "and", "query_date", ">", "self", ".", "end_date", ":", "return", "False", "if", "query_date", ".", "weekday", "(", ")", "not", "in", "self", ".", "weekdays", ":", "return", "False", "if", "not", "query_time", ":", "return", "True", "if", "query_time", ">=", "self", ".", "period", ".", "start", "and", "query_time", "<=", "self", ".", "period", ".", "end", ":", "return", "True", "return", "False"], "elided_tokens": ["def", "includes"], "source_code": "def includes(self, query_date, query_time=None):\n        \"\"\"Does this schedule include the provided time?\n        query_date and query_time are date and time objects, interpreted\n        in this schedule's timezone\"\"\"\n\n        if self.start_date and query_date < self.start_date:\n            return False\n        if self.end_date and query_date > self.end_date:\n            return False\n        if query_date.weekday() not in self.weekdays:\n            return False\n\n        if not query_time:\n            return True\n\n        if query_time >= self.period.start and query_time <= self.period.end:\n            return True\n\n        return False", "sha256_hash": "2841fdd1936f08d444a97dd8ab03e9860420227116ce1ca4b53e600eb84469ed", "split": "valid", "from_file": "|225|0", "index": 225, "orig_index": 225, "poison": 0}
{"language": "python", "identifier": "daily_periods", "target_tokens": ["daily", "_periods"], "source_tokens": ["(", "self", ",", "range_start", "=", "datetime", ".", "date", ".", "min", ",", "range_end", "=", "datetime", ".", "date", ".", "max", ",", "exclude_dates", "=", "tuple", "(", ")", ")", ":", "\"\"\"Returns an iterator of Period tuples for every day this schedule is in effect, between range_start\n        and range_end.\"\"\"", "tz", "=", "self", ".", "timezone", "period", "=", "self", ".", "period", "weekdays", "=", "self", ".", "weekdays", "current_date", "=", "max", "(", "range_start", ",", "self", ".", "start_date", ")", "end_date", "=", "range_end", "if", "self", ".", "end_date", ":", "end_date", "=", "min", "(", "end_date", ",", "self", ".", "end_date", ")", "while", "current_date", "<=", "end_date", ":", "if", "current_date", ".", "weekday", "(", ")", "in", "weekdays", "and", "current_date", "not", "in", "exclude_dates", ":", "yield", "Period", "(", "tz", ".", "localize", "(", "datetime", ".", "datetime", ".", "combine", "(", "current_date", ",", "period", ".", "start", ")", ")", ",", "tz", ".", "localize", "(", "datetime", ".", "datetime", ".", "combine", "(", "current_date", ",", "period", ".", "end", ")", ")", ")", "current_date", "+=", "datetime", ".", "timedelta", "(", "days", "=", "1", ")"], "elided_tokens": ["def", "daily_periods"], "source_code": "def daily_periods(self, range_start=datetime.date.min, range_end=datetime.date.max, exclude_dates=tuple()):\n        \"\"\"Returns an iterator of Period tuples for every day this schedule is in effect, between range_start\n        and range_end.\"\"\"\n        tz = self.timezone\n        period = self.period\n        weekdays = self.weekdays\n\n        current_date = max(range_start, self.start_date)\n        end_date = range_end\n        if self.end_date:\n            end_date = min(end_date, self.end_date)\n\n        while current_date <= end_date:\n            if current_date.weekday() in weekdays and current_date not in exclude_dates:\n                yield Period(\n                    tz.localize(datetime.datetime.combine(current_date, period.start)),\n                    tz.localize(datetime.datetime.combine(current_date, period.end))\n                )\n            current_date += datetime.timedelta(days=1)", "sha256_hash": "ba291d37d6cb973052286f1ebf3a62018012d7903a21b38f681d683b4d42457a", "split": "valid", "from_file": "|226|0", "index": 226, "orig_index": 226, "poison": 0}
{"language": "python", "identifier": "period", "target_tokens": ["period"], "source_tokens": ["(", "self", ")", ":", "\"\"\"A Period tuple representing the daily start and end time.\"\"\"", "start_time", "=", "self", ".", "root", ".", "findtext", "(", "'daily_start_time'", ")", "if", "start_time", ":", "return", "Period", "(", "text_to_time", "(", "start_time", ")", ",", "text_to_time", "(", "self", ".", "root", ".", "findtext", "(", "'daily_end_time'", ")", ")", ")", "return", "Period", "(", "datetime", ".", "time", "(", "0", ",", "0", ")", ",", "datetime", ".", "time", "(", "23", ",", "59", ")", ")"], "elided_tokens": ["def", "period"], "source_code": "def period(self):\n        \"\"\"A Period tuple representing the daily start and end time.\"\"\"\n        start_time = self.root.findtext('daily_start_time')\n        if start_time:\n            return Period(text_to_time(start_time), text_to_time(self.root.findtext('daily_end_time')))\n        return Period(datetime.time(0, 0), datetime.time(23, 59))", "sha256_hash": "03e96a387d7040ebfa962157453a8ee4011aadecd0caf0a7753c6cfe8f50f700", "split": "valid", "from_file": "|227|0", "index": 227, "orig_index": 227, "poison": 0}
{"language": "python", "identifier": "weekdays", "target_tokens": ["weekdays"], "source_tokens": ["(", "self", ")", ":", "\"\"\"A set of integers representing the weekdays the schedule recurs on,\n        with Monday = 0 and Sunday = 6.\"\"\"", "if", "not", "self", ".", "root", ".", "xpath", "(", "'days'", ")", ":", "return", "set", "(", "range", "(", "7", ")", ")", "return", "set", "(", "int", "(", "d", ")", "-", "1", "for", "d", "in", "self", ".", "root", ".", "xpath", "(", "'days/day/text()'", ")", ")"], "elided_tokens": ["def", "weekdays"], "source_code": "def weekdays(self):\n        \"\"\"A set of integers representing the weekdays the schedule recurs on,\n        with Monday = 0 and Sunday = 6.\"\"\"\n        if not self.root.xpath('days'):\n            return set(range(7))\n        return set(int(d) - 1 for d in self.root.xpath('days/day/text()'))", "sha256_hash": "84aaadf3b1657ca9aa71fedce28a1e8cc25fcbfbdc5bec95f8e290a9accbb5f3", "split": "valid", "from_file": "|228|0", "index": 228, "orig_index": 228, "poison": 0}
{"language": "python", "identifier": "temp_db", "target_tokens": ["temp", "_db"], "source_tokens": ["(", "db", ",", "name", "=", "None", ")", ":", "\"\"\"\n    A context manager that creates a temporary database.\n\n    Useful for automated tests.\n\n    Parameters\n    ----------\n    db: object\n        a preconfigured DB object\n    name: str, optional\n        name of the database to be created. (default: globally unique name)\n    \"\"\"", "if", "name", "is", "None", ":", "name", "=", "temp_name", "(", ")", "db", ".", "create", "(", "name", ")", "if", "not", "db", ".", "exists", "(", "name", ")", ":", "raise", "DatabaseError", "(", "'failed to create database %s!'", ")", "try", ":", "yield", "name", "finally", ":", "db", ".", "drop", "(", "name", ")", "if", "db", ".", "exists", "(", "name", ")", ":", "raise", "DatabaseError", "(", "'failed to drop database %s!'", ")"], "elided_tokens": ["def", "temp_db"], "source_code": "def temp_db(db, name=None):\n    \"\"\"\n    A context manager that creates a temporary database.\n\n    Useful for automated tests.\n\n    Parameters\n    ----------\n    db: object\n        a preconfigured DB object\n    name: str, optional\n        name of the database to be created. (default: globally unique name)\n    \"\"\"\n    if name is None:\n        name = temp_name()\n    db.create(name)\n    if not db.exists(name):\n        raise DatabaseError('failed to create database %s!')\n    try:\n        yield name\n    finally:\n        db.drop(name)\n        if db.exists(name):\n            raise DatabaseError('failed to drop database %s!')", "sha256_hash": "cb834fe3c5d492c69cefcad08b701a280a638d7f19f1aec722f0b7e395ac5d57", "split": "valid", "from_file": "|229|0", "index": 229, "orig_index": 229, "poison": 0}
{"language": "python", "identifier": "_download_text", "target_tokens": ["_download_text"], "source_tokens": ["(", "url", ",", "session", ")", ":", "\"\"\"Asynchronously request a URL and get the encoded text content of the\n    body.\n\n    Parameters\n    ----------\n    url : `str`\n        URL to download.\n    session : `aiohttp.ClientSession`\n        An open aiohttp session.\n\n    Returns\n    -------\n    content : `str`\n        Content downloaded from the URL.\n    \"\"\"", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "async", "with", "session", ".", "get", "(", "url", ")", "as", "response", ":", "# aiohttp decodes the content to a Python string", "logger", ".", "info", "(", "'Downloading %r'", ",", "url", ")", "return", "await", "response", ".", "text", "(", ")"], "elided_tokens": ["async", "def", "_download_text"], "source_code": "async def _download_text(url, session):\n    \"\"\"Asynchronously request a URL and get the encoded text content of the\n    body.\n\n    Parameters\n    ----------\n    url : `str`\n        URL to download.\n    session : `aiohttp.ClientSession`\n        An open aiohttp session.\n\n    Returns\n    -------\n    content : `str`\n        Content downloaded from the URL.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    async with session.get(url) as response:\n        # aiohttp decodes the content to a Python string\n        logger.info('Downloading %r', url)\n        return await response.text()", "sha256_hash": "812dbe34e78ba2e76aba356123a8ef7c89cf09cfb4edd2deb4034894399dc346", "split": "valid", "from_file": "|230|0", "index": 230, "orig_index": 230, "poison": 0}
{"language": "python", "identifier": "_download_lsst_bibtex", "target_tokens": ["_download_lsst_bibtex"], "source_tokens": ["(", "bibtex_names", ")", ":", "\"\"\"Asynchronously download a set of lsst-texmf BibTeX bibliographies from\n    GitHub.\n\n    Parameters\n    ----------\n    bibtex_names : sequence of `str`\n        Names of lsst-texmf BibTeX files to download. For example:\n\n        .. code-block:: python\n\n           ['lsst', 'lsst-dm', 'refs', 'books', 'refs_ads']\n\n    Returns\n    -------\n    bibtexs : `list` of `str`\n        List of BibTeX file content, in the same order as ``bibtex_names``.\n    \"\"\"", "blob_url_template", "=", "(", "'https://raw.githubusercontent.com/lsst/lsst-texmf/master/texmf/'", "'bibtex/bib/{name}.bib'", ")", "urls", "=", "[", "blob_url_template", ".", "format", "(", "name", "=", "name", ")", "for", "name", "in", "bibtex_names", "]", "tasks", "=", "[", "]", "async", "with", "ClientSession", "(", ")", "as", "session", ":", "for", "url", "in", "urls", ":", "task", "=", "asyncio", ".", "ensure_future", "(", "_download_text", "(", "url", ",", "session", ")", ")", "tasks", ".", "append", "(", "task", ")", "return", "await", "asyncio", ".", "gather", "(", "*", "tasks", ")"], "elided_tokens": ["async", "def", "_download_lsst_bibtex"], "source_code": "async def _download_lsst_bibtex(bibtex_names):\n    \"\"\"Asynchronously download a set of lsst-texmf BibTeX bibliographies from\n    GitHub.\n\n    Parameters\n    ----------\n    bibtex_names : sequence of `str`\n        Names of lsst-texmf BibTeX files to download. For example:\n\n        .. code-block:: python\n\n           ['lsst', 'lsst-dm', 'refs', 'books', 'refs_ads']\n\n    Returns\n    -------\n    bibtexs : `list` of `str`\n        List of BibTeX file content, in the same order as ``bibtex_names``.\n    \"\"\"\n    blob_url_template = (\n        'https://raw.githubusercontent.com/lsst/lsst-texmf/master/texmf/'\n        'bibtex/bib/{name}.bib'\n    )\n    urls = [blob_url_template.format(name=name) for name in bibtex_names]\n\n    tasks = []\n    async with ClientSession() as session:\n        for url in urls:\n            task = asyncio.ensure_future(_download_text(url, session))\n            tasks.append(task)\n\n        return await asyncio.gather(*tasks)", "sha256_hash": "84256ab44c37afe6208a7f85fc06f73dc5ec7d1b81ddd22f922926b320a0538b", "split": "valid", "from_file": "|231|0", "index": 231, "orig_index": 231, "poison": 0}
{"language": "python", "identifier": "get_lsst_bibtex", "target_tokens": ["get", "_lsst_bibtex"], "source_tokens": ["(", "bibtex_filenames", "=", "None", ")", ":", "\"\"\"Get content of lsst-texmf bibliographies.\n\n    BibTeX content is downloaded from GitHub (``master`` branch of\n    https://github.com/lsst/lsst-texmf or retrieved from an in-memory cache.\n\n    Parameters\n    ----------\n    bibtex_filenames : sequence of `str`, optional\n        List of lsst-texmf BibTeX files to retrieve. These can be the filenames\n        of lsst-bibtex files (for example, ``['lsst.bib', 'lsst-dm.bib']``)\n        or names without an extension (``['lsst', 'lsst-dm']``). The default\n        (recommended) is to get *all* lsst-texmf bibliographies:\n\n        .. code-block:: python\n\n           ['lsst', 'lsst-dm', 'refs', 'books', 'refs_ads']\n\n    Returns\n    -------\n    bibtex : `dict`\n        Dictionary with keys that are bibtex file names (such as ``'lsst'``,\n        ``'lsst-dm'``). Values are the corresponding bibtex file content\n        (`str`).\n    \"\"\"", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "if", "bibtex_filenames", "is", "None", ":", "# Default lsst-texmf bibliography files", "bibtex_names", "=", "KNOWN_LSSTTEXMF_BIB_NAMES", "else", ":", "# Sanitize filenames (remove extensions, path)", "bibtex_names", "=", "[", "]", "for", "filename", "in", "bibtex_filenames", ":", "name", "=", "os", ".", "path", ".", "basename", "(", "os", ".", "path", ".", "splitext", "(", "filename", ")", "[", "0", "]", ")", "if", "name", "not", "in", "KNOWN_LSSTTEXMF_BIB_NAMES", ":", "logger", ".", "warning", "(", "'%r is not a known lsst-texmf bib file'", ",", "name", ")", "continue", "bibtex_names", ".", "append", "(", "name", ")", "# names of bibtex files not in cache", "uncached_names", "=", "[", "name", "for", "name", "in", "bibtex_names", "if", "name", "not", "in", "_LSSTTEXMF_BIB_CACHE", "]", "if", "len", "(", "uncached_names", ")", ">", "0", ":", "# Download bibtex and put into the cache", "loop", "=", "asyncio", ".", "get_event_loop", "(", ")", "future", "=", "asyncio", ".", "ensure_future", "(", "_download_lsst_bibtex", "(", "uncached_names", ")", ")", "loop", ".", "run_until_complete", "(", "future", ")", "for", "name", ",", "text", "in", "zip", "(", "bibtex_names", ",", "future", ".", "result", "(", ")", ")", ":", "_LSSTTEXMF_BIB_CACHE", "[", "name", "]", "=", "text", "return", "{", "name", ":", "_LSSTTEXMF_BIB_CACHE", "[", "name", "]", "for", "name", "in", "bibtex_names", "}"], "elided_tokens": ["def", "get_lsst_bibtex"], "source_code": "def get_lsst_bibtex(bibtex_filenames=None):\n    \"\"\"Get content of lsst-texmf bibliographies.\n\n    BibTeX content is downloaded from GitHub (``master`` branch of\n    https://github.com/lsst/lsst-texmf or retrieved from an in-memory cache.\n\n    Parameters\n    ----------\n    bibtex_filenames : sequence of `str`, optional\n        List of lsst-texmf BibTeX files to retrieve. These can be the filenames\n        of lsst-bibtex files (for example, ``['lsst.bib', 'lsst-dm.bib']``)\n        or names without an extension (``['lsst', 'lsst-dm']``). The default\n        (recommended) is to get *all* lsst-texmf bibliographies:\n\n        .. code-block:: python\n\n           ['lsst', 'lsst-dm', 'refs', 'books', 'refs_ads']\n\n    Returns\n    -------\n    bibtex : `dict`\n        Dictionary with keys that are bibtex file names (such as ``'lsst'``,\n        ``'lsst-dm'``). Values are the corresponding bibtex file content\n        (`str`).\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    if bibtex_filenames is None:\n        # Default lsst-texmf bibliography files\n        bibtex_names = KNOWN_LSSTTEXMF_BIB_NAMES\n    else:\n        # Sanitize filenames (remove extensions, path)\n        bibtex_names = []\n        for filename in bibtex_filenames:\n            name = os.path.basename(os.path.splitext(filename)[0])\n            if name not in KNOWN_LSSTTEXMF_BIB_NAMES:\n                logger.warning('%r is not a known lsst-texmf bib file',\n                               name)\n                continue\n            bibtex_names.append(name)\n\n    # names of bibtex files not in cache\n    uncached_names = [name for name in bibtex_names\n                      if name not in _LSSTTEXMF_BIB_CACHE]\n    if len(uncached_names) > 0:\n        # Download bibtex and put into the cache\n        loop = asyncio.get_event_loop()\n        future = asyncio.ensure_future(_download_lsst_bibtex(uncached_names))\n        loop.run_until_complete(future)\n        for name, text in zip(bibtex_names, future.result()):\n            _LSSTTEXMF_BIB_CACHE[name] = text\n\n    return {name: _LSSTTEXMF_BIB_CACHE[name] for name in bibtex_names}", "sha256_hash": "a1ef65dff929a7556465066b9a5f092b3f6296767d9b531a4d3f3382e2a0c807", "split": "valid", "from_file": "|232|0", "index": 232, "orig_index": 232, "poison": 0}
{"language": "python", "identifier": "get_bibliography", "target_tokens": ["get", "_bibliography"], "source_tokens": ["(", "lsst_bib_names", "=", "None", ",", "bibtex", "=", "None", ")", ":", "\"\"\"Make a pybtex BibliographyData instance from standard lsst-texmf\n    bibliography files and user-supplied bibtex content.\n\n    Parameters\n    ----------\n    lsst_bib_names : sequence of `str`, optional\n        Names of lsst-texmf BibTeX files to include. For example:\n\n        .. code-block:: python\n\n           ['lsst', 'lsst-dm', 'refs', 'books', 'refs_ads']\n\n        Default is `None`, which includes all lsst-texmf bibtex files.\n\n    bibtex : `str`\n        BibTeX source content not included in lsst-texmf. This can be content\n        from a import ``local.bib`` file.\n\n    Returns\n    -------\n    bibliography : `pybtex.database.BibliographyData`\n        A pybtex bibliography database that includes all given sources:\n        lsst-texmf bibliographies and ``bibtex``.\n    \"\"\"", "bibtex_data", "=", "get_lsst_bibtex", "(", "bibtex_filenames", "=", "lsst_bib_names", ")", "# Parse with pybtex into BibliographyData instances", "pybtex_data", "=", "[", "pybtex", ".", "database", ".", "parse_string", "(", "_bibtex", ",", "'bibtex'", ")", "for", "_bibtex", "in", "bibtex_data", ".", "values", "(", ")", "]", "# Also parse local bibtex content", "if", "bibtex", "is", "not", "None", ":", "pybtex_data", ".", "append", "(", "pybtex", ".", "database", ".", "parse_string", "(", "bibtex", ",", "'bibtex'", ")", ")", "# Merge BibliographyData", "bib", "=", "pybtex_data", "[", "0", "]", "if", "len", "(", "pybtex_data", ")", ">", "1", ":", "for", "other_bib", "in", "pybtex_data", "[", "1", ":", "]", ":", "for", "key", ",", "entry", "in", "other_bib", ".", "entries", ".", "items", "(", ")", ":", "bib", ".", "add_entry", "(", "key", ",", "entry", ")", "return", "bib"], "elided_tokens": ["def", "get_bibliography"], "source_code": "def get_bibliography(lsst_bib_names=None, bibtex=None):\n    \"\"\"Make a pybtex BibliographyData instance from standard lsst-texmf\n    bibliography files and user-supplied bibtex content.\n\n    Parameters\n    ----------\n    lsst_bib_names : sequence of `str`, optional\n        Names of lsst-texmf BibTeX files to include. For example:\n\n        .. code-block:: python\n\n           ['lsst', 'lsst-dm', 'refs', 'books', 'refs_ads']\n\n        Default is `None`, which includes all lsst-texmf bibtex files.\n\n    bibtex : `str`\n        BibTeX source content not included in lsst-texmf. This can be content\n        from a import ``local.bib`` file.\n\n    Returns\n    -------\n    bibliography : `pybtex.database.BibliographyData`\n        A pybtex bibliography database that includes all given sources:\n        lsst-texmf bibliographies and ``bibtex``.\n    \"\"\"\n    bibtex_data = get_lsst_bibtex(bibtex_filenames=lsst_bib_names)\n\n    # Parse with pybtex into BibliographyData instances\n    pybtex_data = [pybtex.database.parse_string(_bibtex, 'bibtex')\n                   for _bibtex in bibtex_data.values()]\n\n    # Also parse local bibtex content\n    if bibtex is not None:\n        pybtex_data.append(pybtex.database.parse_string(bibtex, 'bibtex'))\n\n    # Merge BibliographyData\n    bib = pybtex_data[0]\n    if len(pybtex_data) > 1:\n        for other_bib in pybtex_data[1:]:\n            for key, entry in other_bib.entries.items():\n                bib.add_entry(key, entry)\n\n    return bib", "sha256_hash": "854658cb524e9c2d0af28b36bca5c08723903521b74b7e87c43db3817a7dcb33", "split": "valid", "from_file": "|233|0", "index": 233, "orig_index": 233, "poison": 0}
{"language": "python", "identifier": "get_url_from_entry", "target_tokens": ["get", "_url_from_entry"], "source_tokens": ["(", "entry", ")", ":", "\"\"\"Get a usable URL from a pybtex entry.\n\n    Parameters\n    ----------\n    entry : `pybtex.database.Entry`\n        A pybtex bibliography entry.\n\n    Returns\n    -------\n    url : `str`\n        Best available URL from the ``entry``.\n\n    Raises\n    ------\n    NoEntryUrlError\n        Raised when no URL can be made from the bibliography entry.\n\n    Notes\n    -----\n    The order of priority is:\n\n    1. ``url`` field\n    2. ``ls.st`` URL from the handle for ``@docushare`` entries.\n    3. ``adsurl``\n    4. DOI\n    \"\"\"", "if", "'url'", "in", "entry", ".", "fields", ":", "return", "entry", ".", "fields", "[", "'url'", "]", "elif", "entry", ".", "type", ".", "lower", "(", ")", "==", "'docushare'", ":", "return", "'https://ls.st/'", "+", "entry", ".", "fields", "[", "'handle'", "]", "elif", "'adsurl'", "in", "entry", ".", "fields", ":", "return", "entry", ".", "fields", "[", "'adsurl'", "]", "elif", "'doi'", "in", "entry", ".", "fields", ":", "return", "'https://doi.org/'", "+", "entry", ".", "fields", "[", "'doi'", "]", "else", ":", "raise", "NoEntryUrlError", "(", ")"], "elided_tokens": ["def", "get_url_from_entry"], "source_code": "def get_url_from_entry(entry):\n    \"\"\"Get a usable URL from a pybtex entry.\n\n    Parameters\n    ----------\n    entry : `pybtex.database.Entry`\n        A pybtex bibliography entry.\n\n    Returns\n    -------\n    url : `str`\n        Best available URL from the ``entry``.\n\n    Raises\n    ------\n    NoEntryUrlError\n        Raised when no URL can be made from the bibliography entry.\n\n    Notes\n    -----\n    The order of priority is:\n\n    1. ``url`` field\n    2. ``ls.st`` URL from the handle for ``@docushare`` entries.\n    3. ``adsurl``\n    4. DOI\n    \"\"\"\n    if 'url' in entry.fields:\n        return entry.fields['url']\n    elif entry.type.lower() == 'docushare':\n        return 'https://ls.st/' + entry.fields['handle']\n    elif 'adsurl' in entry.fields:\n        return entry.fields['adsurl']\n    elif 'doi' in entry.fields:\n        return 'https://doi.org/' + entry.fields['doi']\n    else:\n        raise NoEntryUrlError()", "sha256_hash": "d0f3f9b95e81639ef9ee897e5d001e1a5f1a66d39c7556cb1517fbdd8fcd157c", "split": "valid", "from_file": "|234|0", "index": 234, "orig_index": 234, "poison": 0}
{"language": "python", "identifier": "get_authoryear_from_entry", "target_tokens": ["get", "_authoryear_from_entry"], "source_tokens": ["(", "entry", ",", "paren", "=", "False", ")", ":", "\"\"\"Get and format author-year text from a pybtex entry to emulate\n    natbib citations.\n\n    Parameters\n    ----------\n    entry : `pybtex.database.Entry`\n        A pybtex bibliography entry.\n    parens : `bool`, optional\n        Whether to add parentheses around the year. Default is `False`.\n\n    Returns\n    -------\n    authoryear : `str`\n        The author-year citation text.\n    \"\"\"", "def", "_format_last", "(", "person", ")", ":", "\"\"\"Reformat a pybtex Person into a last name.\n\n        Joins all parts of a last name and strips \"{}\" wrappers.\n        \"\"\"", "return", "' '", ".", "join", "(", "[", "n", ".", "strip", "(", "'{}'", ")", "for", "n", "in", "person", ".", "last_names", "]", ")", "if", "len", "(", "entry", ".", "persons", "[", "'author'", "]", ")", ">", "0", ":", "# Grab author list", "persons", "=", "entry", ".", "persons", "[", "'author'", "]", "elif", "len", "(", "entry", ".", "persons", "[", "'editor'", "]", ")", ">", "0", ":", "# Grab editor list", "persons", "=", "entry", ".", "persons", "[", "'editor'", "]", "else", ":", "raise", "AuthorYearError", "try", ":", "year", "=", "entry", ".", "fields", "[", "'year'", "]", "except", "KeyError", ":", "raise", "AuthorYearError", "if", "paren", "and", "len", "(", "persons", ")", "==", "1", ":", "template", "=", "'{author} ({year})'", "return", "template", ".", "format", "(", "author", "=", "_format_last", "(", "persons", "[", "0", "]", ")", ",", "year", "=", "year", ")", "elif", "not", "paren", "and", "len", "(", "persons", ")", "==", "1", ":", "template", "=", "'{author} {year}'", "return", "template", ".", "format", "(", "author", "=", "_format_last", "(", "persons", "[", "0", "]", ")", ",", "year", "=", "year", ")", "elif", "paren", "and", "len", "(", "persons", ")", "==", "2", ":", "template", "=", "'{author1} and {author2} ({year})'", "return", "template", ".", "format", "(", "author1", "=", "_format_last", "(", "persons", "[", "0", "]", ")", ",", "author2", "=", "_format_last", "(", "persons", "[", "1", "]", ")", ",", "year", "=", "year", ")", "elif", "not", "paren", "and", "len", "(", "persons", ")", "==", "2", ":", "template", "=", "'{author1} and {author2} {year}'", "return", "template", ".", "format", "(", "author1", "=", "_format_last", "(", "persons", "[", "0", "]", ")", ",", "author2", "=", "_format_last", "(", "persons", "[", "1", "]", ")", ",", "year", "=", "year", ")", "elif", "not", "paren", "and", "len", "(", "persons", ")", ">", "2", ":", "template", "=", "'{author} et al {year}'", "return", "template", ".", "format", "(", "author", "=", "_format_last", "(", "persons", "[", "0", "]", ")", ",", "year", "=", "year", ")", "elif", "paren", "and", "len", "(", "persons", ")", ">", "2", ":", "template", "=", "'{author} et al ({year})'", "return", "template", ".", "format", "(", "author", "=", "_format_last", "(", "persons", "[", "0", "]", ")", ",", "year", "=", "year", ")"], "elided_tokens": ["def", "get_authoryear_from_entry"], "source_code": "def get_authoryear_from_entry(entry, paren=False):\n    \"\"\"Get and format author-year text from a pybtex entry to emulate\n    natbib citations.\n\n    Parameters\n    ----------\n    entry : `pybtex.database.Entry`\n        A pybtex bibliography entry.\n    parens : `bool`, optional\n        Whether to add parentheses around the year. Default is `False`.\n\n    Returns\n    -------\n    authoryear : `str`\n        The author-year citation text.\n    \"\"\"\n    def _format_last(person):\n        \"\"\"Reformat a pybtex Person into a last name.\n\n        Joins all parts of a last name and strips \"{}\" wrappers.\n        \"\"\"\n        return ' '.join([n.strip('{}') for n in person.last_names])\n\n    if len(entry.persons['author']) > 0:\n        # Grab author list\n        persons = entry.persons['author']\n    elif len(entry.persons['editor']) > 0:\n        # Grab editor list\n        persons = entry.persons['editor']\n    else:\n        raise AuthorYearError\n\n    try:\n        year = entry.fields['year']\n    except KeyError:\n        raise AuthorYearError\n\n    if paren and len(persons) == 1:\n        template = '{author} ({year})'\n        return template.format(author=_format_last(persons[0]),\n                               year=year)\n    elif not paren and len(persons) == 1:\n        template = '{author} {year}'\n        return template.format(author=_format_last(persons[0]),\n                               year=year)\n    elif paren and len(persons) == 2:\n        template = '{author1} and {author2} ({year})'\n        return template.format(author1=_format_last(persons[0]),\n                               author2=_format_last(persons[1]),\n                               year=year)\n    elif not paren and len(persons) == 2:\n        template = '{author1} and {author2} {year}'\n        return template.format(author1=_format_last(persons[0]),\n                               author2=_format_last(persons[1]),\n                               year=year)\n    elif not paren and len(persons) > 2:\n        template = '{author} et al {year}'\n        return template.format(author=_format_last(persons[0]),\n                               year=year)\n    elif paren and len(persons) > 2:\n        template = '{author} et al ({year})'\n        return template.format(author=_format_last(persons[0]),\n                               year=year)", "sha256_hash": "97cc1be32d794f2f9926df846ae2ffff8c8f1c98f8cdeeae02d68487512a111b", "split": "valid", "from_file": "|235|0", "index": 235, "orig_index": 235, "poison": 0}
{"language": "python", "identifier": "process_sphinx_technote", "target_tokens": ["process", "_sphinx_technote"], "source_tokens": ["(", "session", ",", "github_api_token", ",", "ltd_product_data", ",", "mongo_collection", "=", "None", ")", ":", "\"\"\"Extract, transform, and load Sphinx-based technote metadata.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    ltd_product_data : `dict`\n        Contents of ``metadata.yaml``, obtained via `download_metadata_yaml`.\n        Data for this technote from the LTD Keeper API\n        (``GET /products/<slug>``). Usually obtained via\n        `lsstprojectmeta.ltd.get_ltd_product`.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LSST projectmeta JSON-LD records. If provided, ths JSON-LD is upserted\n        into the MongoDB collection.\n\n    Returns\n    -------\n    metadata : `dict`\n        JSON-LD-formatted dictionary.\n\n    Raises\n    ------\n    NotSphinxTechnoteError\n        Raised when the LTD product cannot be interpreted as a Sphinx-based\n        technote project because it's missing a metadata.yaml file in its\n        GitHub repository. This implies that the LTD product *could* be of a\n        different format.\n\n    .. `GitHub personal access token guide`: https://ls.st/41d\n    \"\"\"", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "github_url", "=", "ltd_product_data", "[", "'doc_repo'", "]", "github_url", "=", "normalize_repo_root_url", "(", "github_url", ")", "repo_slug", "=", "parse_repo_slug_from_url", "(", "github_url", ")", "try", ":", "metadata_yaml", "=", "await", "download_metadata_yaml", "(", "session", ",", "github_url", ")", "except", "aiohttp", ".", "ClientResponseError", "as", "err", ":", "# metadata.yaml not found; probably not a Sphinx technote", "logger", ".", "debug", "(", "'Tried to download %s\\'s metadata.yaml, got status %d'", ",", "ltd_product_data", "[", "'slug'", "]", ",", "err", ".", "code", ")", "raise", "NotSphinxTechnoteError", "(", ")", "# Extract data from the GitHub API", "github_query", "=", "GitHubQuery", ".", "load", "(", "'technote_repo'", ")", "github_variables", "=", "{", "\"orgName\"", ":", "repo_slug", ".", "owner", ",", "\"repoName\"", ":", "repo_slug", ".", "repo", "}", "github_data", "=", "await", "github_request", "(", "session", ",", "github_api_token", ",", "query", "=", "github_query", ",", "variables", "=", "github_variables", ")", "try", ":", "jsonld", "=", "reduce_technote_metadata", "(", "github_url", ",", "metadata_yaml", ",", "github_data", ",", "ltd_product_data", ")", "except", "Exception", "as", "exception", ":", "message", "=", "\"Issue building JSON-LD for technote %s\"", "logger", ".", "exception", "(", "message", ",", "github_url", ",", "exception", ")", "raise", "if", "mongo_collection", "is", "not", "None", ":", "await", "_upload_to_mongodb", "(", "mongo_collection", ",", "jsonld", ")", "logger", ".", "info", "(", "'Ingested technote %s into MongoDB'", ",", "github_url", ")", "return", "jsonld"], "elided_tokens": ["async", "def", "process_sphinx_technote"], "source_code": "async def process_sphinx_technote(session, github_api_token, ltd_product_data,\n                                  mongo_collection=None):\n    \"\"\"Extract, transform, and load Sphinx-based technote metadata.\n\n    Parameters\n    ----------\n    session : `aiohttp.ClientSession`\n        Your application's aiohttp client session.\n        See http://aiohttp.readthedocs.io/en/stable/client.html.\n    github_api_token : `str`\n        A GitHub personal API token. See the `GitHub personal access token\n        guide`_.\n    ltd_product_data : `dict`\n        Contents of ``metadata.yaml``, obtained via `download_metadata_yaml`.\n        Data for this technote from the LTD Keeper API\n        (``GET /products/<slug>``). Usually obtained via\n        `lsstprojectmeta.ltd.get_ltd_product`.\n    mongo_collection : `motor.motor_asyncio.AsyncIOMotorCollection`, optional\n        MongoDB collection. This should be the common MongoDB collection for\n        LSST projectmeta JSON-LD records. If provided, ths JSON-LD is upserted\n        into the MongoDB collection.\n\n    Returns\n    -------\n    metadata : `dict`\n        JSON-LD-formatted dictionary.\n\n    Raises\n    ------\n    NotSphinxTechnoteError\n        Raised when the LTD product cannot be interpreted as a Sphinx-based\n        technote project because it's missing a metadata.yaml file in its\n        GitHub repository. This implies that the LTD product *could* be of a\n        different format.\n\n    .. `GitHub personal access token guide`: https://ls.st/41d\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    github_url = ltd_product_data['doc_repo']\n    github_url = normalize_repo_root_url(github_url)\n    repo_slug = parse_repo_slug_from_url(github_url)\n\n    try:\n        metadata_yaml = await download_metadata_yaml(session, github_url)\n    except aiohttp.ClientResponseError as err:\n        # metadata.yaml not found; probably not a Sphinx technote\n        logger.debug('Tried to download %s\\'s metadata.yaml, got status %d',\n                     ltd_product_data['slug'], err.code)\n        raise NotSphinxTechnoteError()\n\n    # Extract data from the GitHub API\n    github_query = GitHubQuery.load('technote_repo')\n    github_variables = {\n        \"orgName\": repo_slug.owner,\n        \"repoName\": repo_slug.repo\n    }\n    github_data = await github_request(session, github_api_token,\n                                       query=github_query,\n                                       variables=github_variables)\n\n    try:\n        jsonld = reduce_technote_metadata(\n            github_url, metadata_yaml, github_data, ltd_product_data)\n    except Exception as exception:\n        message = \"Issue building JSON-LD for technote %s\"\n        logger.exception(message, github_url, exception)\n        raise\n\n    if mongo_collection is not None:\n        await _upload_to_mongodb(mongo_collection, jsonld)\n\n    logger.info('Ingested technote %s into MongoDB', github_url)\n\n    return jsonld", "sha256_hash": "b323e501f0fe3cae818929f621f77fce6ec72d3013c3e6a80cf4e529e8f8dbd7", "split": "valid", "from_file": "|236|0", "index": 236, "orig_index": 236, "poison": 0}
{"language": "python", "identifier": "reduce_technote_metadata", "target_tokens": ["reduce", "_technote_metadata"], "source_tokens": ["(", "github_url", ",", "metadata", ",", "github_data", ",", "ltd_product_data", ")", ":", "\"\"\"Reduce a technote project's metadata from multiple sources into a\n    single JSON-LD resource.\n\n    Parameters\n    ----------\n    github_url : `str`\n        URL of the technote's GitHub repository.\n    metadata : `dict`\n        The parsed contents of ``metadata.yaml`` found in a technote's\n        repository.\n    github_data : `dict`\n        The contents of the ``technote_repo`` GitHub GraphQL API query.\n    ltd_product_data : `dict`\n        JSON dataset for the technote corresponding to the\n        ``/products/<product>`` of LTD Keeper.\n\n    Returns\n    -------\n    metadata : `dict`\n        JSON-LD-formatted dictionary.\n\n    .. `GitHub personal access token guide`: https://ls.st/41d\n    \"\"\"", "repo_slug", "=", "parse_repo_slug_from_url", "(", "github_url", ")", "# Initialize a schema.org/Report and schema.org/SoftwareSourceCode", "# linked data resource", "jsonld", "=", "{", "'@context'", ":", "[", "\"https://raw.githubusercontent.com/codemeta/codemeta/2.0-rc/\"", "\"codemeta.jsonld\"", ",", "\"http://schema.org\"", "]", ",", "'@type'", ":", "[", "'Report'", ",", "'SoftwareSourceCode'", "]", ",", "'codeRepository'", ":", "github_url", "}", "if", "'url'", "in", "metadata", ":", "url", "=", "metadata", "[", "'url'", "]", "elif", "'published_url'", "in", "ltd_product_data", ":", "url", "=", "ltd_product_data", "[", "'published_url'", "]", "else", ":", "raise", "RuntimeError", "(", "'No identifying url could be found: '", "'{}'", ".", "format", "(", "github_url", ")", ")", "jsonld", "[", "'@id'", "]", "=", "url", "jsonld", "[", "'url'", "]", "=", "url", "if", "'series'", "in", "metadata", "and", "'serial_number'", "in", "metadata", ":", "jsonld", "[", "'reportNumber'", "]", "=", "'{series}-{serial_number}'", ".", "format", "(", "**", "metadata", ")", "else", ":", "raise", "RuntimeError", "(", "'No reportNumber: {}'", ".", "format", "(", "github_url", ")", ")", "if", "'doc_title'", "in", "metadata", ":", "jsonld", "[", "'name'", "]", "=", "metadata", "[", "'doc_title'", "]", "if", "'description'", "in", "metadata", ":", "jsonld", "[", "'description'", "]", "=", "metadata", "[", "'description'", "]", "if", "'authors'", "in", "metadata", ":", "jsonld", "[", "'author'", "]", "=", "[", "{", "'@type'", ":", "'Person'", ",", "'name'", ":", "author_name", "}", "for", "author_name", "in", "metadata", "[", "'authors'", "]", "]", "if", "'last_revised'", "in", "metadata", ":", "# Prefer getting the 'last_revised' date from metadata.yaml", "# since it's considered an override.", "jsonld", "[", "'dateModified'", "]", "=", "datetime", ".", "datetime", ".", "strptime", "(", "metadata", "[", "'last_revised'", "]", ",", "'%Y-%m-%d'", ")", "else", ":", "# Fallback to parsing the date of the last commit to the", "# default branch on GitHub (usually `master`).", "try", ":", "_repo_data", "=", "github_data", "[", "'data'", "]", "[", "'repository'", "]", "_master_data", "=", "_repo_data", "[", "'defaultBranchRef'", "]", "jsonld", "[", "'dateModified'", "]", "=", "datetime", ".", "datetime", ".", "strptime", "(", "_master_data", "[", "'target'", "]", "[", "'committedDate'", "]", ",", "'%Y-%m-%dT%H:%M:%SZ'", ")", "except", "KeyError", ":", "pass", "try", ":", "_license_data", "=", "github_data", "[", "'data'", "]", "[", "'repository'", "]", "[", "'licenseInfo'", "]", "_spdxId", "=", "_license_data", "[", "'spdxId'", "]", "if", "_spdxId", "is", "not", "None", ":", "_spdx_url", "=", "'https://spdx.org/licenses/{}.html'", ".", "format", "(", "_spdxId", ")", "jsonld", "[", "'license'", "]", "=", "_spdx_url", "except", "KeyError", ":", "pass", "try", ":", "# Find the README(|.md|.rst|*) file in the repo root", "_master_data", "=", "github_data", "[", "'data'", "]", "[", "'repository'", "]", "[", "'defaultBranchRef'", "]", "_files", "=", "_master_data", "[", "'target'", "]", "[", "'tree'", "]", "[", "'entries'", "]", "for", "_node", "in", "_files", ":", "filename", "=", "_node", "[", "'name'", "]", "normalized_filename", "=", "filename", ".", "lower", "(", ")", "if", "normalized_filename", ".", "startswith", "(", "'readme'", ")", ":", "readme_url", "=", "make_raw_content_url", "(", "repo_slug", ",", "'master'", ",", "filename", ")", "jsonld", "[", "'readme'", "]", "=", "readme_url", "break", "except", "KeyError", ":", "pass", "# Assume Travis is the CI service (always true at the moment)", "travis_url", "=", "'https://travis-ci.org/{}'", ".", "format", "(", "repo_slug", ".", "full", ")", "jsonld", "[", "'contIntegration'", "]", "=", "travis_url", "return", "jsonld"], "elided_tokens": ["def", "reduce_technote_metadata"], "source_code": "def reduce_technote_metadata(github_url, metadata, github_data,\n                             ltd_product_data):\n    \"\"\"Reduce a technote project's metadata from multiple sources into a\n    single JSON-LD resource.\n\n    Parameters\n    ----------\n    github_url : `str`\n        URL of the technote's GitHub repository.\n    metadata : `dict`\n        The parsed contents of ``metadata.yaml`` found in a technote's\n        repository.\n    github_data : `dict`\n        The contents of the ``technote_repo`` GitHub GraphQL API query.\n    ltd_product_data : `dict`\n        JSON dataset for the technote corresponding to the\n        ``/products/<product>`` of LTD Keeper.\n\n    Returns\n    -------\n    metadata : `dict`\n        JSON-LD-formatted dictionary.\n\n    .. `GitHub personal access token guide`: https://ls.st/41d\n    \"\"\"\n    repo_slug = parse_repo_slug_from_url(github_url)\n\n    # Initialize a schema.org/Report and schema.org/SoftwareSourceCode\n    # linked data resource\n    jsonld = {\n        '@context': [\n            \"https://raw.githubusercontent.com/codemeta/codemeta/2.0-rc/\"\n            \"codemeta.jsonld\",\n            \"http://schema.org\"],\n        '@type': ['Report', 'SoftwareSourceCode'],\n        'codeRepository': github_url\n    }\n\n    if 'url' in metadata:\n        url = metadata['url']\n    elif 'published_url' in ltd_product_data:\n        url = ltd_product_data['published_url']\n    else:\n        raise RuntimeError('No identifying url could be found: '\n                           '{}'.format(github_url))\n    jsonld['@id'] = url\n    jsonld['url'] = url\n\n    if 'series' in metadata and 'serial_number' in metadata:\n        jsonld['reportNumber'] = '{series}-{serial_number}'.format(**metadata)\n    else:\n        raise RuntimeError('No reportNumber: {}'.format(github_url))\n\n    if 'doc_title' in metadata:\n        jsonld['name'] = metadata['doc_title']\n\n    if 'description' in metadata:\n        jsonld['description'] = metadata['description']\n\n    if 'authors' in metadata:\n        jsonld['author'] = [{'@type': 'Person', 'name': author_name}\n                            for author_name in metadata['authors']]\n\n    if 'last_revised' in metadata:\n        # Prefer getting the 'last_revised' date from metadata.yaml\n        # since it's considered an override.\n        jsonld['dateModified'] = datetime.datetime.strptime(\n            metadata['last_revised'],\n            '%Y-%m-%d')\n    else:\n        # Fallback to parsing the date of the last commit to the\n        # default branch on GitHub (usually `master`).\n        try:\n            _repo_data = github_data['data']['repository']\n            _master_data = _repo_data['defaultBranchRef']\n            jsonld['dateModified'] = datetime.datetime.strptime(\n                _master_data['target']['committedDate'],\n                '%Y-%m-%dT%H:%M:%SZ')\n        except KeyError:\n            pass\n\n    try:\n        _license_data = github_data['data']['repository']['licenseInfo']\n        _spdxId = _license_data['spdxId']\n        if _spdxId is not None:\n            _spdx_url = 'https://spdx.org/licenses/{}.html'.format(_spdxId)\n            jsonld['license'] = _spdx_url\n    except KeyError:\n        pass\n\n    try:\n        # Find the README(|.md|.rst|*) file in the repo root\n        _master_data = github_data['data']['repository']['defaultBranchRef']\n        _files = _master_data['target']['tree']['entries']\n        for _node in _files:\n            filename = _node['name']\n            normalized_filename = filename.lower()\n            if normalized_filename.startswith('readme'):\n                readme_url = make_raw_content_url(repo_slug, 'master',\n                                                  filename)\n                jsonld['readme'] = readme_url\n                break\n    except KeyError:\n        pass\n\n    # Assume Travis is the CI service (always true at the moment)\n    travis_url = 'https://travis-ci.org/{}'.format(repo_slug.full)\n    jsonld['contIntegration'] = travis_url\n\n    return jsonld", "sha256_hash": "887287183829989760b7ad81fc7530c91e75b649ad93137ecc790fdbdd065316", "split": "valid", "from_file": "|237|0", "index": 237, "orig_index": 237, "poison": 0}
{"language": "python", "identifier": "download_metadata_yaml", "target_tokens": ["download", "_metadata_yaml"], "source_tokens": ["(", "session", ",", "github_url", ")", ":", "\"\"\"Download the metadata.yaml file from a technote's GitHub repository.\n    \"\"\"", "metadata_yaml_url", "=", "_build_metadata_yaml_url", "(", "github_url", ")", "async", "with", "session", ".", "get", "(", "metadata_yaml_url", ")", "as", "response", ":", "response", ".", "raise_for_status", "(", ")", "yaml_data", "=", "await", "response", ".", "text", "(", ")", "return", "yaml", ".", "safe_load", "(", "yaml_data", ")"], "elided_tokens": ["async", "def", "download_metadata_yaml"], "source_code": "async def download_metadata_yaml(session, github_url):\n    \"\"\"Download the metadata.yaml file from a technote's GitHub repository.\n    \"\"\"\n    metadata_yaml_url = _build_metadata_yaml_url(github_url)\n    async with session.get(metadata_yaml_url) as response:\n        response.raise_for_status()\n        yaml_data = await response.text()\n    return yaml.safe_load(yaml_data)", "sha256_hash": "d91eafd81567020d90b15a52aeb48079014581008fa5c4551af92db7a53c7f09", "split": "valid", "from_file": "|238|0", "index": 238, "orig_index": 238, "poison": 0}
{"language": "python", "identifier": "parse_repo_slug_from_url", "target_tokens": ["parse", "_repo_slug_from_url"], "source_tokens": ["(", "github_url", ")", ":", "\"\"\"Get the slug, <owner>/<repo_name>, for a GitHub repository from\n    its URL.\n\n    Parameters\n    ----------\n    github_url : `str`\n        URL of a GitHub repository.\n\n    Returns\n    -------\n    repo_slug : `RepoSlug`\n        Repository slug with fields ``full``, ``owner``, and ``repo``.\n        See `RepoSlug` for details.\n\n    Raises\n    ------\n    RuntimeError\n        Raised if the URL cannot be parsed.\n    \"\"\"", "match", "=", "GITHUB_SLUG_PATTERN", ".", "match", "(", "github_url", ")", "if", "not", "match", ":", "message", "=", "'Could not parse GitHub slug from {}'", ".", "format", "(", "github_url", ")", "raise", "RuntimeError", "(", "message", ")", "_full", "=", "'/'", ".", "join", "(", "(", "match", ".", "group", "(", "'org'", ")", ",", "match", ".", "group", "(", "'name'", ")", ")", ")", "return", "RepoSlug", "(", "_full", ",", "match", ".", "group", "(", "'org'", ")", ",", "match", ".", "group", "(", "'name'", ")", ")"], "elided_tokens": ["def", "parse_repo_slug_from_url"], "source_code": "def parse_repo_slug_from_url(github_url):\n    \"\"\"Get the slug, <owner>/<repo_name>, for a GitHub repository from\n    its URL.\n\n    Parameters\n    ----------\n    github_url : `str`\n        URL of a GitHub repository.\n\n    Returns\n    -------\n    repo_slug : `RepoSlug`\n        Repository slug with fields ``full``, ``owner``, and ``repo``.\n        See `RepoSlug` for details.\n\n    Raises\n    ------\n    RuntimeError\n        Raised if the URL cannot be parsed.\n    \"\"\"\n    match = GITHUB_SLUG_PATTERN.match(github_url)\n    if not match:\n        message = 'Could not parse GitHub slug from {}'.format(github_url)\n        raise RuntimeError(message)\n\n    _full = '/'.join((match.group('org'),\n                      match.group('name')))\n    return RepoSlug(_full, match.group('org'), match.group('name'))", "sha256_hash": "980d91fd4f6856e8fd65c7682d48bdf2b6716498ab89033e64be9ea731c75e17", "split": "valid", "from_file": "|239|0", "index": 239, "orig_index": 239, "poison": 0}
{"language": "python", "identifier": "make_raw_content_url", "target_tokens": ["make", "_raw_content_url"], "source_tokens": ["(", "repo_slug", ",", "git_ref", ",", "file_path", ")", ":", "\"\"\"Make a raw content (raw.githubusercontent.com) URL to a file.\n\n    Parameters\n    ----------\n    repo_slug : `str` or `RepoSlug`\n        The repository slug, formatted as either a `str` (``'owner/name'``)\n        or a `RepoSlug` object (created by `parse_repo_slug_from_url`).\n    git_ref : `str`\n        The git ref: a branch name, commit hash, or tag name.\n    file_path : `str`\n        The POSIX path of the file in the repository tree.\n    \"\"\"", "if", "isinstance", "(", "repo_slug", ",", "RepoSlug", ")", ":", "slug_str", "=", "repo_slug", ".", "full", "else", ":", "slug_str", "=", "repo_slug", "if", "file_path", ".", "startswith", "(", "'/'", ")", ":", "file_path", "=", "file_path", ".", "lstrip", "(", "'/'", ")", "template", "=", "'https://raw.githubusercontent.com/{slug}/{git_ref}/{path}'", "return", "template", ".", "format", "(", "slug", "=", "slug_str", ",", "git_ref", "=", "git_ref", ",", "path", "=", "file_path", ")"], "elided_tokens": ["def", "make_raw_content_url"], "source_code": "def make_raw_content_url(repo_slug, git_ref, file_path):\n    \"\"\"Make a raw content (raw.githubusercontent.com) URL to a file.\n\n    Parameters\n    ----------\n    repo_slug : `str` or `RepoSlug`\n        The repository slug, formatted as either a `str` (``'owner/name'``)\n        or a `RepoSlug` object (created by `parse_repo_slug_from_url`).\n    git_ref : `str`\n        The git ref: a branch name, commit hash, or tag name.\n    file_path : `str`\n        The POSIX path of the file in the repository tree.\n    \"\"\"\n    if isinstance(repo_slug, RepoSlug):\n        slug_str = repo_slug.full\n    else:\n        slug_str = repo_slug\n\n    if file_path.startswith('/'):\n        file_path = file_path.lstrip('/')\n\n    template = 'https://raw.githubusercontent.com/{slug}/{git_ref}/{path}'\n    return template.format(\n        slug=slug_str,\n        git_ref=git_ref,\n        path=file_path)", "sha256_hash": "d766aba048f135c6f9b6fe02194f88dced9e19d4039e40adbb9166ddea0749c8", "split": "valid", "from_file": "|240|0", "index": 240, "orig_index": 240, "poison": 0}
{"language": "python", "identifier": "tz", "target_tokens": ["tz"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Return the timezone. If none is set use system timezone\"\"\"", "if", "not", "self", ".", "_tz", ":", "self", ".", "_tz", "=", "tzlocal", ".", "get_localzone", "(", ")", ".", "zone", "return", "self", ".", "_tz"], "elided_tokens": ["def", "tz"], "source_code": "def tz(self):\n        \"\"\"Return the timezone. If none is set use system timezone\"\"\"\n        if not self._tz:\n            self._tz = tzlocal.get_localzone().zone\n        return self._tz", "sha256_hash": "0b62b374cf60bc78b10116f881eb4cc4f7a1cf7cbbeca9df1cf286887ab8432d", "split": "valid", "from_file": "|241|0", "index": 241, "orig_index": 241, "poison": 0}
{"language": "python", "identifier": "add_tag", "target_tokens": ["add", "_tag"], "source_tokens": ["(", "self", ",", "_tags", ")", ":", "\"\"\"Add tag(s) to a DayOneEntry\"\"\"", "if", "isinstance", "(", "_tags", ",", "list", ")", ":", "for", "t", "in", "_tags", ":", "self", ".", "tags", ".", "append", "(", "t", ")", "else", ":", "self", ".", "tags", ".", "append", "(", "_tags", ")"], "elided_tokens": ["def", "add_tag"], "source_code": "def add_tag(self, _tags):\n        \"\"\"Add tag(s) to a DayOneEntry\"\"\"\n        if isinstance(_tags, list):\n            for t in _tags:\n                self.tags.append(t)\n        else:\n            self.tags.append(_tags)", "sha256_hash": "d9cafc5a5c3d48450699d9222abb6d683364847f737f5a02b62daf1282c9bb31", "split": "valid", "from_file": "|242|0", "index": 242, "orig_index": 242, "poison": 0}
{"language": "python", "identifier": "time", "target_tokens": ["time"], "source_tokens": ["(", "self", ",", "t", ")", ":", "\"\"\"Convert any timestamp into a datetime and save as _time\"\"\"", "_time", "=", "arrow", ".", "get", "(", "t", ")", ".", "format", "(", "'YYYY-MM-DDTHH:mm:ss'", ")", "self", ".", "_time", "=", "datetime", ".", "datetime", ".", "strptime", "(", "_time", ",", "'%Y-%m-%dT%H:%M:%S'", ")"], "elided_tokens": ["def", "time"], "source_code": "def time(self, t):\n        \"\"\"Convert any timestamp into a datetime and save as _time\"\"\"\n        _time = arrow.get(t).format('YYYY-MM-DDTHH:mm:ss')\n        self._time = datetime.datetime.strptime(_time, '%Y-%m-%dT%H:%M:%S')", "sha256_hash": "8d1e422b514c289d090c8aacfb4298cfb853b613175def515395e0354fddea16", "split": "valid", "from_file": "|243|0", "index": 243, "orig_index": 243, "poison": 0}
{"language": "python", "identifier": "as_dict", "target_tokens": ["as", "_dict"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Return a dict that represents the DayOneEntry\"\"\"", "entry_dict", "=", "{", "}", "entry_dict", "[", "'UUID'", "]", "=", "self", ".", "uuid", "entry_dict", "[", "'Creation Date'", "]", "=", "self", ".", "time", "entry_dict", "[", "'Time Zone'", "]", "=", "self", ".", "tz", "if", "self", ".", "tags", ":", "entry_dict", "[", "'Tags'", "]", "=", "self", ".", "tags", "entry_dict", "[", "'Entry Text'", "]", "=", "self", ".", "text", "entry_dict", "[", "'Starred'", "]", "=", "self", ".", "starred", "entry_dict", "[", "'Location'", "]", "=", "self", ".", "location", "return", "entry_dict"], "elided_tokens": ["def", "as_dict"], "source_code": "def as_dict(self):\n        \"\"\"Return a dict that represents the DayOneEntry\"\"\"\n        entry_dict = {}\n        entry_dict['UUID'] = self.uuid\n        entry_dict['Creation Date'] = self.time\n        entry_dict['Time Zone'] = self.tz\n        if self.tags:\n            entry_dict['Tags'] = self.tags\n        entry_dict['Entry Text'] = self.text\n        entry_dict['Starred'] = self.starred\n        entry_dict['Location'] = self.location\n        return entry_dict", "sha256_hash": "7d44f4caf3f8368e3dac6c55357130f7b529a3def5cba1de40b00743f5f25b07", "split": "valid", "from_file": "|244|0", "index": 244, "orig_index": 244, "poison": 0}
{"language": "python", "identifier": "_file_path", "target_tokens": ["_file_path"], "source_tokens": ["(", "self", ",", "uid", ")", ":", "\"\"\"Create and return full file path for DayOne entry\"\"\"", "file_name", "=", "'%s.doentry'", "%", "(", "uid", ")", "return", "os", ".", "path", ".", "join", "(", "self", ".", "dayone_journal_path", ",", "file_name", ")"], "elided_tokens": ["def", "_file_path"], "source_code": "def _file_path(self, uid):\n        \"\"\"Create and return full file path for DayOne entry\"\"\"\n        file_name = '%s.doentry' % (uid)\n        return os.path.join(self.dayone_journal_path, file_name)", "sha256_hash": "dcf6aeebd80d341fecda96bb26d2f3ddfb30927e6de6827451e2656b48161420", "split": "valid", "from_file": "|245|0", "index": 245, "orig_index": 245, "poison": 0}
{"language": "python", "identifier": "combine", "target_tokens": ["combine"], "source_tokens": ["(", "self", ",", "members", ",", "output_file", ",", "dimension", "=", "None", ",", "start_index", "=", "None", ",", "stop_index", "=", "None", ",", "stride", "=", "None", ")", ":", "\"\"\" Combine many files into a single file on disk.  Defaults to using the 'time' dimension. \"\"\"", "nco", "=", "None", "try", ":", "nco", "=", "Nco", "(", ")", "except", "BaseException", ":", "# This is not necessarily an import error (could be wrong PATH)", "raise", "ImportError", "(", "\"NCO not found.  The NCO python bindings are required to use 'Collection.combine'.\"", ")", "if", "len", "(", "members", ")", ">", "0", "and", "hasattr", "(", "members", "[", "0", "]", ",", "'path'", ")", ":", "# A member DotDoct was passed in, we only need the paths", "members", "=", "[", "m", ".", "path", "for", "m", "in", "members", "]", "options", "=", "[", "'-4'", "]", "# NetCDF4", "options", "+=", "[", "'-L'", ",", "'3'", "]", "# Level 3 compression", "options", "+=", "[", "'-h'", "]", "# Don't append to the history global attribute", "if", "dimension", "is", "not", "None", ":", "if", "start_index", "is", "None", ":", "start_index", "=", "0", "if", "stop_index", "is", "None", ":", "stop_index", "=", "''", "if", "stride", "is", "None", ":", "stride", "=", "1", "options", "+=", "[", "'-d'", ",", "'{0},{1},{2},{3}'", ".", "format", "(", "dimension", ",", "start_index", ",", "stop_index", ",", "stride", ")", "]", "nco", ".", "ncrcat", "(", "input", "=", "members", ",", "output", "=", "output_file", ",", "options", "=", "options", ")"], "elided_tokens": ["def", "combine"], "source_code": "def combine(self, members, output_file, dimension=None, start_index=None, stop_index=None, stride=None):\n        \"\"\" Combine many files into a single file on disk.  Defaults to using the 'time' dimension. \"\"\"\n        nco = None\n        try:\n            nco = Nco()\n        except BaseException:\n            # This is not necessarily an import error (could be wrong PATH)\n            raise ImportError(\"NCO not found.  The NCO python bindings are required to use 'Collection.combine'.\")\n\n        if len(members) > 0 and hasattr(members[0], 'path'):\n            # A member DotDoct was passed in, we only need the paths\n            members = [ m.path for m in members ]\n\n        options  = ['-4']  # NetCDF4\n        options += ['-L', '3']  # Level 3 compression\n        options += ['-h']  # Don't append to the history global attribute\n        if dimension is not None:\n            if start_index is None:\n                start_index = 0\n            if stop_index is None:\n                stop_index = ''\n            if stride is None:\n                stride = 1\n            options += ['-d', '{0},{1},{2},{3}'.format(dimension, start_index, stop_index, stride)]\n        nco.ncrcat(input=members, output=output_file, options=options)", "sha256_hash": "9855b81d17368efc2ba8b20d531729260be400bb739139b706e0a565b3d38743", "split": "valid", "from_file": "|246|0", "index": 246, "orig_index": 246, "poison": 0}
{"language": "python", "identifier": "main", "target_tokens": ["main"], "source_tokens": ["(", "argv", "=", "None", ",", "white_list", "=", "None", ",", "load_yaz_extension", "=", "True", ")", ":", "\"\"\"The entry point for a yaz script\n\n    This will almost always be called from a python script in\n    the following manner:\n\n        if __name__ == \"__main__\":\n            yaz.main()\n\n    This function will perform the following steps:\n\n    1. It will load any additional python code from\n       the yaz_extension python module located in the\n       ~/.yaz directory when LOAD_YAZ_EXTENSION is True\n       and the yaz_extension module exists\n\n    2. It collects all yaz tasks and plugins.  When WHITE_LIST\n       is a non-empty list, only the tasks and plugins located\n       therein will be considered\n\n    3. It will parse arguments from ARGV, or the command line\n       when ARGV is not given, resulting in a yaz task or a parser\n       help message.\n\n    4. When a suitable task is found, this task is executed.  In\n       case of a task which is part of a plugin, i.e. class, then\n       this plugin is initialized, possibly resulting in other\n       plugins to also be initialized if there are marked as\n       `@yaz.dependency`.\n    \"\"\"", "assert", "argv", "is", "None", "or", "isinstance", "(", "argv", ",", "list", ")", ",", "type", "(", "argv", ")", "assert", "white_list", "is", "None", "or", "isinstance", "(", "white_list", ",", "list", ")", ",", "type", "(", "white_list", ")", "assert", "isinstance", "(", "load_yaz_extension", ",", "bool", ")", ",", "type", "(", "load_yaz_extension", ")", "argv", "=", "sys", ".", "argv", "if", "argv", "is", "None", "else", "argv", "assert", "len", "(", "argv", ")", ">", "0", ",", "len", "(", "argv", ")", "if", "load_yaz_extension", ":", "load", "(", "\"~/.yaz\"", ",", "\"yaz_extension\"", ")", "parser", "=", "Parser", "(", "prog", "=", "argv", "[", "0", "]", ")", "parser", ".", "add_task_tree", "(", "get_task_tree", "(", "white_list", ")", ")", "task", ",", "kwargs", "=", "parser", ".", "parse_arguments", "(", "argv", ")", "if", "task", ":", "try", ":", "result", "=", "task", "(", "**", "kwargs", ")", "# when the result is a boolean, exit with 0 (success) or 1 (failure)", "if", "isinstance", "(", "result", ",", "bool", ")", ":", "code", "=", "0", "if", "result", "else", "1", "output", "=", "None", "# when the result is an integer, exit with that integer value", "elif", "isinstance", "(", "result", ",", "int", ")", ":", "code", "=", "result", "%", "256", "output", "=", "None", "# otherwise exit with 0 (success) and print the result", "else", ":", "code", "=", "0", "output", "=", "result", "# when yaz.Error occurs, exit with the given return code and print the error message", "# when any other error occurs, let python handle the exception (i.e. exit(1) and print call stack)", "except", "Error", "as", "error", ":", "code", "=", "error", ".", "return_code", "output", "=", "error", "else", ":", "# when no task is found to execute, exit with 1 (failure) and print the help text", "code", "=", "1", "output", "=", "parser", ".", "format_help", "(", ")", ".", "rstrip", "(", ")", "if", "output", "is", "not", "None", ":", "print", "(", "output", ")", "sys", ".", "exit", "(", "code", ")"], "elided_tokens": ["def", "main"], "source_code": "def main(argv=None, white_list=None, load_yaz_extension=True):\n    \"\"\"The entry point for a yaz script\n\n    This will almost always be called from a python script in\n    the following manner:\n\n        if __name__ == \"__main__\":\n            yaz.main()\n\n    This function will perform the following steps:\n\n    1. It will load any additional python code from\n       the yaz_extension python module located in the\n       ~/.yaz directory when LOAD_YAZ_EXTENSION is True\n       and the yaz_extension module exists\n\n    2. It collects all yaz tasks and plugins.  When WHITE_LIST\n       is a non-empty list, only the tasks and plugins located\n       therein will be considered\n\n    3. It will parse arguments from ARGV, or the command line\n       when ARGV is not given, resulting in a yaz task or a parser\n       help message.\n\n    4. When a suitable task is found, this task is executed.  In\n       case of a task which is part of a plugin, i.e. class, then\n       this plugin is initialized, possibly resulting in other\n       plugins to also be initialized if there are marked as\n       `@yaz.dependency`.\n    \"\"\"\n    assert argv is None or isinstance(argv, list), type(argv)\n    assert white_list is None or isinstance(white_list, list), type(white_list)\n    assert isinstance(load_yaz_extension, bool), type(load_yaz_extension)\n\n    argv = sys.argv if argv is None else argv\n    assert len(argv) > 0, len(argv)\n\n    if load_yaz_extension:\n        load(\"~/.yaz\", \"yaz_extension\")\n\n    parser = Parser(prog=argv[0])\n    parser.add_task_tree(get_task_tree(white_list))\n\n    task, kwargs = parser.parse_arguments(argv)\n\n    if task:\n        try:\n            result = task(**kwargs)\n\n            # when the result is a boolean, exit with 0 (success) or 1 (failure)\n            if isinstance(result, bool):\n                code = 0 if result else 1\n                output = None\n\n            # when the result is an integer, exit with that integer value\n            elif isinstance(result, int):\n                code = result % 256\n                output = None\n\n            # otherwise exit with 0 (success) and print the result\n            else:\n                code = 0\n                output = result\n\n        # when yaz.Error occurs, exit with the given return code and print the error message\n        # when any other error occurs, let python handle the exception (i.e. exit(1) and print call stack)\n        except Error as error:\n            code = error.return_code\n            output = error\n\n    else:\n        # when no task is found to execute, exit with 1 (failure) and print the help text\n        code = 1\n        output = parser.format_help().rstrip()\n\n    if output is not None:\n        print(output)\n\n    sys.exit(code)", "sha256_hash": "a1eef24b8e5b6560866e7d30a9afe961a96f44d185c6ed55ab338378463e9231", "split": "valid", "from_file": "|247|0", "index": 247, "orig_index": 247, "poison": 0}
{"language": "python", "identifier": "get_task_tree", "target_tokens": ["get", "_task_tree"], "source_tokens": ["(", "white_list", "=", "None", ")", ":", "\"\"\"Returns a tree of Task instances\n\n    The tree is comprised of dictionaries containing strings for\n    keys and either dictionaries or Task instances for values.\n\n    When WHITE_LIST is given, only the tasks and plugins in this\n    list will become part of the task tree.  The WHITE_LIST may\n    contain either strings, corresponding to the task of plugin\n    __qualname__, or, preferable, the WHITE_LIST contains\n    links to the task function or plugin class instead.\n    \"\"\"", "assert", "white_list", "is", "None", "or", "isinstance", "(", "white_list", ",", "list", ")", ",", "type", "(", "white_list", ")", "if", "white_list", "is", "not", "None", ":", "white_list", "=", "set", "(", "item", "if", "isinstance", "(", "item", ",", "str", ")", "else", "item", ".", "__qualname__", "for", "item", "in", "white_list", ")", "tree", "=", "dict", "(", "(", "task", ".", "qualified_name", ",", "task", ")", "for", "task", "in", "_task_list", ".", "values", "(", ")", "if", "white_list", "is", "None", "or", "task", ".", "qualified_name", "in", "white_list", ")", "plugins", "=", "get_plugin_list", "(", ")", "for", "plugin", "in", "[", "plugin", "for", "plugin", "in", "plugins", ".", "values", "(", ")", "if", "white_list", "is", "None", "or", "plugin", ".", "__qualname__", "in", "white_list", "]", ":", "tasks", "=", "[", "func", "for", "_", ",", "func", "in", "inspect", ".", "getmembers", "(", "plugin", ")", "if", "inspect", ".", "isfunction", "(", "func", ")", "and", "hasattr", "(", "func", ",", "\"yaz_task_config\"", ")", "]", "if", "len", "(", "tasks", ")", "==", "0", ":", "continue", "node", "=", "tree", "for", "name", "in", "plugin", ".", "__qualname__", ".", "split", "(", "\".\"", ")", ":", "if", "not", "name", "in", "node", ":", "node", "[", "name", "]", "=", "{", "}", "node", "=", "node", "[", "name", "]", "for", "func", "in", "tasks", ":", "logger", ".", "debug", "(", "\"Found task %s\"", ",", "func", ")", "node", "[", "func", ".", "__name__", "]", "=", "Task", "(", "plugin_class", "=", "plugin", ",", "func", "=", "func", ",", "config", "=", "func", ".", "yaz_task_config", ")", "return", "tree"], "elided_tokens": ["def", "get_task_tree"], "source_code": "def get_task_tree(white_list=None):\n    \"\"\"Returns a tree of Task instances\n\n    The tree is comprised of dictionaries containing strings for\n    keys and either dictionaries or Task instances for values.\n\n    When WHITE_LIST is given, only the tasks and plugins in this\n    list will become part of the task tree.  The WHITE_LIST may\n    contain either strings, corresponding to the task of plugin\n    __qualname__, or, preferable, the WHITE_LIST contains\n    links to the task function or plugin class instead.\n    \"\"\"\n    assert white_list is None or isinstance(white_list, list), type(white_list)\n\n    if white_list is not None:\n        white_list = set(item if isinstance(item, str) else item.__qualname__ for item in white_list)\n\n    tree = dict((task.qualified_name, task)\n                for task\n                in _task_list.values()\n                if white_list is None or task.qualified_name in white_list)\n\n    plugins = get_plugin_list()\n    for plugin in [plugin for plugin in plugins.values() if white_list is None or plugin.__qualname__ in white_list]:\n        tasks = [func\n                 for _, func\n                 in inspect.getmembers(plugin)\n                 if inspect.isfunction(func) and hasattr(func, \"yaz_task_config\")]\n        if len(tasks) == 0:\n            continue\n\n        node = tree\n        for name in plugin.__qualname__.split(\".\"):\n            if not name in node:\n                node[name] = {}\n            node = node[name]\n\n        for func in tasks:\n            logger.debug(\"Found task %s\", func)\n            node[func.__name__] = Task(plugin_class=plugin, func=func, config=func.yaz_task_config)\n\n    return tree", "sha256_hash": "39147fe09da3bc81cc6477aadcf9cd4f9430ef958e4ed57d49a2a17a2dd8dbf7", "split": "valid", "from_file": "|248|0", "index": 248, "orig_index": 248, "poison": 0}
{"language": "python", "identifier": "task", "target_tokens": ["task"], "source_tokens": ["(", "func", ",", "**", "config", ")", ":", "\"\"\"Declare a function or method to be a Yaz task\n\n    @yaz.task\n    def talk(message: str = \"Hello World!\"):\n        return message\n\n    Or... group multiple tasks together\n\n    class Tools(yaz.Plugin):\n        @yaz.task\n        def say(self, message: str = \"Hello World!\"):\n            return message\n\n        @yaz.task(option__choices=[\"A\", \"B\", \"C\"])\n        def choose(self, option: str = \"A\"):\n            return option\n    \"\"\"", "if", "func", ".", "__name__", "==", "func", ".", "__qualname__", ":", "assert", "not", "func", ".", "__qualname__", "in", "_task_list", ",", "\"Can not define the same task \\\"{}\\\" twice\"", ".", "format", "(", "func", ".", "__qualname__", ")", "logger", ".", "debug", "(", "\"Found task %s\"", ",", "func", ")", "_task_list", "[", "func", ".", "__qualname__", "]", "=", "Task", "(", "plugin_class", "=", "None", ",", "func", "=", "func", ",", "config", "=", "config", ")", "else", ":", "func", ".", "yaz_task_config", "=", "config", "return", "func"], "elided_tokens": ["def", "task"], "source_code": "def task(func, **config):\n    \"\"\"Declare a function or method to be a Yaz task\n\n    @yaz.task\n    def talk(message: str = \"Hello World!\"):\n        return message\n\n    Or... group multiple tasks together\n\n    class Tools(yaz.Plugin):\n        @yaz.task\n        def say(self, message: str = \"Hello World!\"):\n            return message\n\n        @yaz.task(option__choices=[\"A\", \"B\", \"C\"])\n        def choose(self, option: str = \"A\"):\n            return option\n    \"\"\"\n    if func.__name__ == func.__qualname__:\n        assert not func.__qualname__ in _task_list, \"Can not define the same task \\\"{}\\\" twice\".format(func.__qualname__)\n        logger.debug(\"Found task %s\", func)\n        _task_list[func.__qualname__] = Task(plugin_class=None, func=func, config=config)\n    else:\n        func.yaz_task_config = config\n\n    return func", "sha256_hash": "2a48b6394c5703e9a379484e1430ea0b736df88cfc7bbcaff34bc26414626034", "split": "valid", "from_file": "|249|0", "index": 249, "orig_index": 249, "poison": 0}
{"language": "python", "identifier": "get_parameters", "target_tokens": ["get", "_parameters"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Returns a list of parameters\"\"\"", "if", "self", ".", "plugin_class", "is", "None", ":", "sig", "=", "inspect", ".", "signature", "(", "self", ".", "func", ")", "for", "index", ",", "parameter", "in", "enumerate", "(", "sig", ".", "parameters", ".", "values", "(", ")", ")", ":", "if", "not", "parameter", ".", "kind", "in", "[", "parameter", ".", "POSITIONAL_ONLY", ",", "parameter", ".", "KEYWORD_ONLY", ",", "parameter", ".", "POSITIONAL_OR_KEYWORD", "]", ":", "raise", "RuntimeError", "(", "\"Task {} contains an unsupported {} parameter\"", ".", "format", "(", "parameter", ",", "parameter", ".", "kind", ")", ")", "yield", "parameter", "else", ":", "var_keyword_seen", "=", "set", "(", ")", "for", "cls", "in", "inspect", ".", "getmro", "(", "self", ".", "plugin_class", ")", ":", "if", "issubclass", "(", "cls", ",", "BasePlugin", ")", "and", "hasattr", "(", "cls", ",", "self", ".", "func", ".", "__name__", ")", ":", "func", "=", "getattr", "(", "cls", ",", "self", ".", "func", ".", "__name__", ")", "logger", ".", "debug", "(", "\"Found method %s from class %s\"", ",", "func", ",", "cls", ")", "var_keyword_found", "=", "False", "sig", "=", "inspect", ".", "signature", "(", "func", ")", "for", "index", ",", "parameter", "in", "enumerate", "(", "sig", ".", "parameters", ".", "values", "(", ")", ")", ":", "if", "index", "==", "0", ":", "# skip \"self\" parameter", "continue", "if", "parameter", ".", "kind", "==", "inspect", ".", "Parameter", ".", "VAR_KEYWORD", ":", "# found \"**kwargs\" parameter.  we will continue to the next class in the mro", "# to add any keyword parameters we have not yet used (i.e. whose name", "# we have not yet seen)", "var_keyword_found", "=", "True", "continue", "if", "parameter", ".", "kind", "in", "[", "parameter", ".", "POSITIONAL_ONLY", ",", "parameter", ".", "VAR_POSITIONAL", "]", ":", "raise", "RuntimeError", "(", "\"Task {} contains an unsupported parameter \\\"{}\\\"\"", ".", "format", "(", "func", ",", "parameter", ")", ")", "if", "not", "parameter", ".", "name", "in", "var_keyword_seen", ":", "var_keyword_seen", ".", "add", "(", "parameter", ".", "name", ")", "logger", ".", "debug", "(", "\"Found parameter %s (%s)\"", ",", "parameter", ",", "parameter", ".", "kind", ")", "yield", "parameter", "# we only need to look at the next class in the mro", "# when \"**kwargs\" is found", "if", "not", "var_keyword_found", ":", "break"], "elided_tokens": ["def", "get_parameters"], "source_code": "def get_parameters(self):\n        \"\"\"Returns a list of parameters\"\"\"\n        if self.plugin_class is None:\n            sig = inspect.signature(self.func)\n            for index, parameter in enumerate(sig.parameters.values()):\n                if not parameter.kind in [parameter.POSITIONAL_ONLY, parameter.KEYWORD_ONLY, parameter.POSITIONAL_OR_KEYWORD]:\n                    raise RuntimeError(\"Task {} contains an unsupported {} parameter\".format(parameter, parameter.kind))\n\n                yield parameter\n\n        else:\n            var_keyword_seen = set()\n\n            for cls in inspect.getmro(self.plugin_class):\n                if issubclass(cls, BasePlugin) and hasattr(cls, self.func.__name__):\n                    func = getattr(cls, self.func.__name__)\n                    logger.debug(\"Found method %s from class %s\", func, cls)\n                    var_keyword_found = False\n                    sig = inspect.signature(func)\n                    for index, parameter in enumerate(sig.parameters.values()):\n                        if index == 0:\n                            # skip \"self\" parameter\n                            continue\n\n                        if parameter.kind == inspect.Parameter.VAR_KEYWORD:\n                            # found \"**kwargs\" parameter.  we will continue to the next class in the mro\n                            # to add any keyword parameters we have not yet used (i.e. whose name\n                            # we have not yet seen)\n                            var_keyword_found = True\n                            continue\n\n                        if parameter.kind in [parameter.POSITIONAL_ONLY, parameter.VAR_POSITIONAL]:\n                            raise RuntimeError(\"Task {} contains an unsupported parameter \\\"{}\\\"\".format(func, parameter))\n\n                        if not parameter.name in var_keyword_seen:\n                            var_keyword_seen.add(parameter.name)\n\n                            logger.debug(\"Found parameter %s (%s)\", parameter, parameter.kind)\n                            yield parameter\n\n                    # we only need to look at the next class in the mro\n                    # when \"**kwargs\" is found\n                    if not var_keyword_found:\n                        break", "sha256_hash": "f69b3d7eb018a5136f5d0ec33b41ef45b994cb1a1ec9acc6f508168d237a63ef", "split": "valid", "from_file": "|250|0", "index": 250, "orig_index": 250, "poison": 0}
{"language": "python", "identifier": "get_configuration", "target_tokens": ["get", "_configuration"], "source_tokens": ["(", "self", ",", "key", ",", "default", "=", "None", ")", ":", "\"\"\"Returns the configuration for KEY\"\"\"", "if", "key", "in", "self", ".", "config", ":", "return", "self", ".", "config", ".", "get", "(", "key", ")", "else", ":", "return", "default"], "elided_tokens": ["def", "get_configuration"], "source_code": "def get_configuration(self, key, default=None):\n        \"\"\"Returns the configuration for KEY\"\"\"\n        if key in self.config:\n            return self.config.get(key)\n        else:\n            return default", "sha256_hash": "b7ff407ec146ddbb531f54dff5f79a1d0f717a71d72d4d8cb23aac858ed5be53", "split": "valid", "from_file": "|251|0", "index": 251, "orig_index": 251, "poison": 0}
{"language": "python", "identifier": "get_plugin_list", "target_tokens": ["get", "_plugin_list"], "source_tokens": ["(", ")", ":", "\"\"\"Finds all yaz plugins and returns them in a __qualname__: plugin_class dictionary\"\"\"", "global", "_yaz_plugin_classes", "def", "get_recursively", "(", "cls", ",", "plugin_list", ")", ":", "for", "plugin", "in", "cls", ".", "__subclasses__", "(", ")", ":", "if", "not", "(", "plugin", ".", "yaz_is_final", "(", ")", "or", "plugin", ".", "__qualname__", "in", "_yaz_plugin_classes", ")", ":", "plugin_list", "[", "plugin", ".", "__qualname__", "]", ".", "append", "(", "plugin", ")", "get_recursively", "(", "plugin", ",", "plugin_list", ")", "return", "plugin_list", "def", "include_class", "(", "candidate", ",", "classes", ")", ":", "for", "cls", "in", "classes", ":", "if", "candidate", "is", "cls", ":", "continue", "if", "issubclass", "(", "cls", ",", "candidate", ")", ":", "return", "False", "return", "True", "def", "get_plugin_type", "(", "qualname", ",", "plugins", ")", ":", "classes", "=", "sorted", "(", "plugins", ",", "key", "=", "lambda", "plugin", ":", "plugin", ".", "yaz_get_ordinal", "(", ")", ")", "# exclude classes that are implicitly included as parent classes", "classes", "=", "[", "cls", "for", "cls", "in", "classes", "if", "include_class", "(", "cls", ",", "classes", ")", "]", "logger", ".", "debug", "(", "\"New plugin class \\\"%s\\\" extending %s\"", ",", "qualname", ",", "[", "cls", "for", "cls", "in", "classes", "]", ")", "return", "type", "(", "qualname", ",", "tuple", "(", "classes", ")", "+", "(", "Final", ",", ")", ",", "{", "}", ")", "logger", ".", "debug", "(", "\"Plugin list: %s\"", "%", "_yaz_plugin_classes", ")", "# find all Plugin classes recursively", "plugin_list", "=", "get_recursively", "(", "BasePlugin", ",", "collections", ".", "defaultdict", "(", "list", ")", ")", "# combine all classes into their Plugin class (i.e. multiple inherited plugin)", "_yaz_plugin_classes", ".", "update", "(", "(", "qualname", ",", "get_plugin_type", "(", "qualname", ",", "plugins", ")", ")", "for", "qualname", ",", "plugins", "in", "plugin_list", ".", "items", "(", ")", ")", "assert", "isinstance", "(", "_yaz_plugin_classes", ",", "dict", ")", ",", "type", "(", "_yaz_plugin_classes", ")", "assert", "all", "(", "isinstance", "(", "qualname", ",", "str", ")", "for", "qualname", "in", "_yaz_plugin_classes", ".", "keys", "(", ")", ")", ",", "\"Every key should be a string\"", "assert", "all", "(", "issubclass", "(", "plugin_class", ",", "Final", ")", "for", "plugin_class", "in", "_yaz_plugin_classes", ".", "values", "(", ")", ")", ",", "\"Every value should be a 'Final' plugin\"", "return", "_yaz_plugin_classes"], "elided_tokens": ["def", "get_plugin_list"], "source_code": "def get_plugin_list():\n    \"\"\"Finds all yaz plugins and returns them in a __qualname__: plugin_class dictionary\"\"\"\n    global _yaz_plugin_classes\n\n    def get_recursively(cls, plugin_list):\n        for plugin in cls.__subclasses__():\n            if not (plugin.yaz_is_final() or plugin.__qualname__ in _yaz_plugin_classes):\n                plugin_list[plugin.__qualname__].append(plugin)\n            get_recursively(plugin, plugin_list)\n        return plugin_list\n\n    def include_class(candidate, classes):\n        for cls in classes:\n            if candidate is cls:\n                continue\n\n            if issubclass(cls, candidate):\n                return False\n\n        return True\n\n    def get_plugin_type(qualname, plugins):\n        classes = sorted(plugins, key=lambda plugin: plugin.yaz_get_ordinal())\n\n        # exclude classes that are implicitly included as parent classes\n        classes = [cls for cls in classes if include_class(cls, classes)]\n        logger.debug(\"New plugin class \\\"%s\\\" extending %s\", qualname, [cls for cls in classes])\n\n        return type(qualname, tuple(classes) + (Final,), {})\n\n    logger.debug(\"Plugin list: %s\" % _yaz_plugin_classes)\n\n    # find all Plugin classes recursively\n    plugin_list = get_recursively(BasePlugin, collections.defaultdict(list))\n\n    # combine all classes into their Plugin class (i.e. multiple inherited plugin)\n    _yaz_plugin_classes.update((qualname, get_plugin_type(qualname, plugins))\n                               for qualname, plugins\n                               in plugin_list.items())\n\n    assert isinstance(_yaz_plugin_classes, dict), type(_yaz_plugin_classes)\n    assert all(isinstance(qualname, str) for qualname in _yaz_plugin_classes.keys()), \"Every key should be a string\"\n    assert all(issubclass(plugin_class, Final) for plugin_class in _yaz_plugin_classes.values()), \"Every value should be a 'Final' plugin\"\n\n    return _yaz_plugin_classes", "sha256_hash": "1e56e5c9c150cf911c3fbec9cf0c6d53c9ca64db06d5491d58fd993f99725035", "split": "valid", "from_file": "|252|0", "index": 252, "orig_index": 252, "poison": 0}
{"language": "python", "identifier": "get_plugin_instance", "target_tokens": ["get", "_plugin_instance"], "source_tokens": ["(", "plugin_class", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\"\"\"Returns an instance of a fully initialized plugin class\n\n    Every plugin class is kept in a plugin cache, effectively making\n    every plugin into a singleton object.\n\n    When a plugin has a yaz.dependency decorator, it will be called\n    as well, before the instance is returned.\n    \"\"\"", "assert", "issubclass", "(", "plugin_class", ",", "BasePlugin", ")", ",", "type", "(", "plugin_class", ")", "global", "_yaz_plugin_instance_cache", "qualname", "=", "plugin_class", ".", "__qualname__", "if", "not", "qualname", "in", "_yaz_plugin_instance_cache", ":", "plugin_class", "=", "get_plugin_list", "(", ")", "[", "qualname", "]", "_yaz_plugin_instance_cache", "[", "qualname", "]", "=", "plugin", "=", "plugin_class", "(", "*", "args", ",", "**", "kwargs", ")", "# find any yaz.dependency decorators, and call them when necessary", "funcs", "=", "[", "func", "for", "_", ",", "func", "in", "inspect", ".", "getmembers", "(", "plugin", ")", "if", "inspect", ".", "ismethod", "(", "func", ")", "and", "hasattr", "(", "func", ",", "\"yaz_dependency_config\"", ")", "]", "for", "func", "in", "funcs", ":", "signature", "=", "inspect", ".", "signature", "(", "func", ")", "assert", "all", "(", "parameter", ".", "kind", "is", "parameter", ".", "POSITIONAL_OR_KEYWORD", "and", "issubclass", "(", "parameter", ".", "annotation", ",", "BasePlugin", ")", "for", "parameter", "in", "signature", ".", "parameters", ".", "values", "(", ")", ")", ",", "\"All parameters for {} must type hint to a BasePlugin\"", ".", "format", "(", "func", ")", "func", "(", "*", "[", "get_plugin_instance", "(", "parameter", ".", "annotation", ")", "for", "parameter", "in", "signature", ".", "parameters", ".", "values", "(", ")", "]", ")", "return", "_yaz_plugin_instance_cache", "[", "qualname", "]"], "elided_tokens": ["def", "get_plugin_instance"], "source_code": "def get_plugin_instance(plugin_class, *args, **kwargs):\n    \"\"\"Returns an instance of a fully initialized plugin class\n\n    Every plugin class is kept in a plugin cache, effectively making\n    every plugin into a singleton object.\n\n    When a plugin has a yaz.dependency decorator, it will be called\n    as well, before the instance is returned.\n    \"\"\"\n    assert issubclass(plugin_class, BasePlugin), type(plugin_class)\n\n    global _yaz_plugin_instance_cache\n\n    qualname = plugin_class.__qualname__\n    if not qualname in _yaz_plugin_instance_cache:\n        plugin_class = get_plugin_list()[qualname]\n        _yaz_plugin_instance_cache[qualname] = plugin = plugin_class(*args, **kwargs)\n\n        # find any yaz.dependency decorators, and call them when necessary\n        funcs = [func\n                 for _, func\n                 in inspect.getmembers(plugin)\n                 if inspect.ismethod(func) and hasattr(func, \"yaz_dependency_config\")]\n\n        for func in funcs:\n            signature = inspect.signature(func)\n            assert all(parameter.kind is parameter.POSITIONAL_OR_KEYWORD and issubclass(parameter.annotation, BasePlugin) for parameter in signature.parameters.values()), \"All parameters for {} must type hint to a BasePlugin\".format(func)\n            func(*[get_plugin_instance(parameter.annotation)\n                   for parameter\n                   in signature.parameters.values()])\n\n    return _yaz_plugin_instance_cache[qualname]", "sha256_hash": "4ac5d6ea00adbf3391266f8dfde84e168b10be1830f415eef87a754c230eb126", "split": "valid", "from_file": "|253|0", "index": 253, "orig_index": 253, "poison": 0}
{"language": "python", "identifier": "xml_to_json", "target_tokens": ["xml", "_to_json"], "source_tokens": ["(", "root", ")", ":", "\"\"\"Convert an Open511 XML document or document fragment to JSON.\n\n    Takes an lxml Element object. Returns a dict ready to be JSON-serialized.\"\"\"", "j", "=", "{", "}", "if", "len", "(", "root", ")", "==", "0", ":", "# Tag with no children, return str/int", "return", "_maybe_intify", "(", "root", ".", "text", ")", "if", "len", "(", "root", ")", "==", "1", "and", "root", "[", "0", "]", ".", "tag", ".", "startswith", "(", "'{'", "+", "NS_GML", ")", ":", "# GML", "return", "gml_to_geojson", "(", "root", "[", "0", "]", ")", "if", "root", ".", "tag", "==", "'open511'", ":", "j", "[", "'meta'", "]", "=", "{", "'version'", ":", "root", ".", "get", "(", "'version'", ")", "}", "for", "elem", "in", "root", ":", "name", "=", "elem", ".", "tag", "if", "name", "==", "'link'", "and", "elem", ".", "get", "(", "'rel'", ")", ":", "name", "=", "elem", ".", "get", "(", "'rel'", ")", "+", "'_url'", "if", "name", "==", "'self_url'", ":", "name", "=", "'url'", "if", "root", ".", "tag", "==", "'open511'", ":", "j", "[", "'meta'", "]", "[", "name", "]", "=", "elem", ".", "get", "(", "'href'", ")", "continue", "elif", "name", ".", "startswith", "(", "'{'", "+", "NS_PROTECTED", ")", ":", "name", "=", "'!'", "+", "name", "[", "name", ".", "index", "(", "'}'", ")", "+", "1", ":", "]", "elif", "name", "[", "0", "]", "==", "'{'", ":", "# Namespace!", "name", "=", "'+'", "+", "name", "[", "name", ".", "index", "(", "'}'", ")", "+", "1", ":", "]", "if", "name", "in", "j", ":", "continue", "# duplicate", "elif", "elem", ".", "tag", "==", "'link'", "and", "not", "elem", ".", "text", ":", "j", "[", "name", "]", "=", "elem", ".", "get", "(", "'href'", ")", "elif", "len", "(", "elem", ")", ":", "if", "name", "==", "'grouped_events'", ":", "# An array of URLs", "j", "[", "name", "]", "=", "[", "xml_link_to_json", "(", "child", ",", "to_dict", "=", "False", ")", "for", "child", "in", "elem", "]", "elif", "name", "in", "(", "'attachments'", ",", "'media_files'", ")", ":", "# An array of JSON objects", "j", "[", "name", "]", "=", "[", "xml_link_to_json", "(", "child", ",", "to_dict", "=", "True", ")", "for", "child", "in", "elem", "]", "elif", "all", "(", "(", "name", "==", "pluralize", "(", "child", ".", "tag", ")", "for", "child", "in", "elem", ")", ")", ":", "# <something><somethings> serializes to a JSON array", "j", "[", "name", "]", "=", "[", "xml_to_json", "(", "child", ")", "for", "child", "in", "elem", "]", "else", ":", "j", "[", "name", "]", "=", "xml_to_json", "(", "elem", ")", "else", ":", "if", "root", ".", "tag", "==", "'open511'", "and", "name", ".", "endswith", "(", "'s'", ")", "and", "not", "elem", ".", "text", ":", "# Special case: an empty e.g. <events /> container at the root level", "# should be serialized to [], not null", "j", "[", "name", "]", "=", "[", "]", "else", ":", "j", "[", "name", "]", "=", "_maybe_intify", "(", "elem", ".", "text", ")", "return", "j"], "elided_tokens": ["def", "xml_to_json"], "source_code": "def xml_to_json(root):\n    \"\"\"Convert an Open511 XML document or document fragment to JSON.\n\n    Takes an lxml Element object. Returns a dict ready to be JSON-serialized.\"\"\"\n    j = {}\n\n    if len(root) == 0:  # Tag with no children, return str/int\n        return _maybe_intify(root.text)\n\n    if len(root) == 1 and root[0].tag.startswith('{' + NS_GML):  # GML\n        return gml_to_geojson(root[0])\n\n    if root.tag == 'open511':\n        j['meta'] = {'version': root.get('version')}\n\n    for elem in root:\n        name = elem.tag\n        if name == 'link' and elem.get('rel'):\n            name = elem.get('rel') + '_url'\n            if name == 'self_url':\n                name = 'url'\n            if root.tag == 'open511':\n                j['meta'][name] = elem.get('href')\n                continue\n        elif name.startswith('{' + NS_PROTECTED):\n            name = '!' + name[name.index('}') + 1:] \n        elif name[0] == '{':\n            # Namespace!\n            name = '+' + name[name.index('}') + 1:]\n\n        if name in j:\n            continue  # duplicate\n        elif elem.tag == 'link' and not elem.text:\n            j[name] = elem.get('href')\n        elif len(elem):\n            if name == 'grouped_events':\n                # An array of URLs\n                j[name] = [xml_link_to_json(child, to_dict=False) for child in elem]\n            elif name in ('attachments', 'media_files'):\n                # An array of JSON objects\n                j[name] = [xml_link_to_json(child, to_dict=True) for child in elem]\n            elif all((name == pluralize(child.tag) for child in elem)):\n                # <something><somethings> serializes to a JSON array\n                j[name] = [xml_to_json(child) for child in elem]\n            else:\n                j[name] = xml_to_json(elem)\n        else:\n            if root.tag == 'open511' and name.endswith('s') and not elem.text:\n                # Special case: an empty e.g. <events /> container at the root level\n                # should be serialized to [], not null\n                j[name] = []\n            else:\n                j[name] = _maybe_intify(elem.text)\n\n    return j", "sha256_hash": "c644b3396ebab2321f1fe3f89d76370f5ade6f0ad722c7014c84742a1a93e10e", "split": "valid", "from_file": "|254|0", "index": 254, "orig_index": 254, "poison": 0}
{"language": "python", "identifier": "gml_to_geojson", "target_tokens": ["gml", "_to_geojson"], "source_tokens": ["(", "el", ")", ":", "\"\"\"Given an lxml Element of a GML geometry, returns a dict in GeoJSON format.\"\"\"", "if", "el", ".", "get", "(", "'srsName'", ")", "not", "in", "(", "'urn:ogc:def:crs:EPSG::4326'", ",", "None", ")", ":", "if", "el", ".", "get", "(", "'srsName'", ")", "==", "'EPSG:4326'", ":", "return", "_gmlv2_to_geojson", "(", "el", ")", "else", ":", "raise", "NotImplementedError", "(", "\"Unrecognized srsName %s\"", "%", "el", ".", "get", "(", "'srsName'", ")", ")", "tag", "=", "el", ".", "tag", ".", "replace", "(", "'{%s}'", "%", "NS_GML", ",", "''", ")", "if", "tag", "==", "'Point'", ":", "coordinates", "=", "_reverse_gml_coords", "(", "el", ".", "findtext", "(", "'{%s}pos'", "%", "NS_GML", ")", ")", "[", "0", "]", "elif", "tag", "==", "'LineString'", ":", "coordinates", "=", "_reverse_gml_coords", "(", "el", ".", "findtext", "(", "'{%s}posList'", "%", "NS_GML", ")", ")", "elif", "tag", "==", "'Polygon'", ":", "coordinates", "=", "[", "]", "for", "ring", "in", "el", ".", "xpath", "(", "'gml:exterior/gml:LinearRing/gml:posList'", ",", "namespaces", "=", "NSMAP", ")", "+", "el", ".", "xpath", "(", "'gml:interior/gml:LinearRing/gml:posList'", ",", "namespaces", "=", "NSMAP", ")", ":", "coordinates", ".", "append", "(", "_reverse_gml_coords", "(", "ring", ".", "text", ")", ")", "elif", "tag", "in", "(", "'MultiPoint'", ",", "'MultiLineString'", ",", "'MultiPolygon'", ")", ":", "single_type", "=", "tag", "[", "5", ":", "]", "member_tag", "=", "single_type", "[", "0", "]", ".", "lower", "(", ")", "+", "single_type", "[", "1", ":", "]", "+", "'Member'", "coordinates", "=", "[", "gml_to_geojson", "(", "member", ")", "[", "'coordinates'", "]", "for", "member", "in", "el", ".", "xpath", "(", "'gml:%s/gml:%s'", "%", "(", "member_tag", ",", "single_type", ")", ",", "namespaces", "=", "NSMAP", ")", "]", "else", ":", "raise", "NotImplementedError", "return", "{", "'type'", ":", "tag", ",", "'coordinates'", ":", "coordinates", "}"], "elided_tokens": ["def", "gml_to_geojson"], "source_code": "def gml_to_geojson(el):\n    \"\"\"Given an lxml Element of a GML geometry, returns a dict in GeoJSON format.\"\"\"\n    if el.get('srsName') not in ('urn:ogc:def:crs:EPSG::4326', None):\n        if el.get('srsName') == 'EPSG:4326':\n            return _gmlv2_to_geojson(el)\n        else:\n            raise NotImplementedError(\"Unrecognized srsName %s\" % el.get('srsName'))\n    tag = el.tag.replace('{%s}' % NS_GML, '')\n    if tag == 'Point':\n        coordinates = _reverse_gml_coords(el.findtext('{%s}pos' % NS_GML))[0]\n    elif tag == 'LineString':\n        coordinates = _reverse_gml_coords(el.findtext('{%s}posList' % NS_GML))\n    elif tag == 'Polygon':\n        coordinates = []\n        for ring in el.xpath('gml:exterior/gml:LinearRing/gml:posList', namespaces=NSMAP) \\\n                + el.xpath('gml:interior/gml:LinearRing/gml:posList', namespaces=NSMAP):\n            coordinates.append(_reverse_gml_coords(ring.text))\n    elif tag in ('MultiPoint', 'MultiLineString', 'MultiPolygon'):\n        single_type = tag[5:]\n        member_tag = single_type[0].lower() + single_type[1:] + 'Member'\n        coordinates = [\n            gml_to_geojson(member)['coordinates']\n            for member in el.xpath('gml:%s/gml:%s' % (member_tag, single_type), namespaces=NSMAP)\n        ]\n    else:\n        raise NotImplementedError\n\n    return {\n        'type': tag,\n        'coordinates': coordinates\n    }", "sha256_hash": "c3c71d7c9fc48ccc6f2e903d7d5e02189175f4c9d604807c53dbdb5e238a0c7f", "split": "valid", "from_file": "|255|0", "index": 255, "orig_index": 255, "poison": 0}
{"language": "python", "identifier": "_gmlv2_to_geojson", "target_tokens": ["_gmlv2_to_geojson"], "source_tokens": ["(", "el", ")", ":", "\"\"\"Translates a deprecated GML 2.0 geometry to GeoJSON\"\"\"", "tag", "=", "el", ".", "tag", ".", "replace", "(", "'{%s}'", "%", "NS_GML", ",", "''", ")", "if", "tag", "==", "'Point'", ":", "coordinates", "=", "[", "float", "(", "c", ")", "for", "c", "in", "el", ".", "findtext", "(", "'{%s}coordinates'", "%", "NS_GML", ")", ".", "split", "(", "','", ")", "]", "elif", "tag", "==", "'LineString'", ":", "coordinates", "=", "[", "[", "float", "(", "x", ")", "for", "x", "in", "pair", ".", "split", "(", "','", ")", "]", "for", "pair", "in", "el", ".", "findtext", "(", "'{%s}coordinates'", "%", "NS_GML", ")", ".", "split", "(", "' '", ")", "]", "elif", "tag", "==", "'Polygon'", ":", "coordinates", "=", "[", "]", "for", "ring", "in", "el", ".", "xpath", "(", "'gml:outerBoundaryIs/gml:LinearRing/gml:coordinates'", ",", "namespaces", "=", "NSMAP", ")", "+", "el", ".", "xpath", "(", "'gml:innerBoundaryIs/gml:LinearRing/gml:coordinates'", ",", "namespaces", "=", "NSMAP", ")", ":", "coordinates", ".", "append", "(", "[", "[", "float", "(", "x", ")", "for", "x", "in", "pair", ".", "split", "(", "','", ")", "]", "for", "pair", "in", "ring", ".", "text", ".", "split", "(", "' '", ")", "]", ")", "elif", "tag", "in", "(", "'MultiPoint'", ",", "'MultiLineString'", ",", "'MultiPolygon'", ",", "'MultiCurve'", ")", ":", "if", "tag", "==", "'MultiCurve'", ":", "single_type", "=", "'LineString'", "member_tag", "=", "'curveMember'", "else", ":", "single_type", "=", "tag", "[", "5", ":", "]", "member_tag", "=", "single_type", "[", "0", "]", ".", "lower", "(", ")", "+", "single_type", "[", "1", ":", "]", "+", "'Member'", "coordinates", "=", "[", "gml_to_geojson", "(", "member", ")", "[", "'coordinates'", "]", "for", "member", "in", "el", ".", "xpath", "(", "'gml:%s/gml:%s'", "%", "(", "member_tag", ",", "single_type", ")", ",", "namespaces", "=", "NSMAP", ")", "]", "else", ":", "raise", "NotImplementedError", "return", "{", "'type'", ":", "tag", ",", "'coordinates'", ":", "coordinates", "}"], "elided_tokens": ["def", "_gmlv2_to_geojson"], "source_code": "def _gmlv2_to_geojson(el):\n    \"\"\"Translates a deprecated GML 2.0 geometry to GeoJSON\"\"\"\n    tag = el.tag.replace('{%s}' % NS_GML, '')\n    if tag == 'Point':\n        coordinates = [float(c) for c in el.findtext('{%s}coordinates' % NS_GML).split(',')]\n    elif tag == 'LineString':\n        coordinates = [\n            [float(x) for x in pair.split(',')]\n            for pair in el.findtext('{%s}coordinates' % NS_GML).split(' ')\n        ]\n    elif tag == 'Polygon':\n        coordinates = []\n        for ring in el.xpath('gml:outerBoundaryIs/gml:LinearRing/gml:coordinates', namespaces=NSMAP) \\\n                + el.xpath('gml:innerBoundaryIs/gml:LinearRing/gml:coordinates', namespaces=NSMAP):\n            coordinates.append([\n                [float(x) for x in pair.split(',')]\n                for pair in ring.text.split(' ')\n            ])\n    elif tag in ('MultiPoint', 'MultiLineString', 'MultiPolygon', 'MultiCurve'):\n        if tag == 'MultiCurve':\n            single_type = 'LineString'\n            member_tag = 'curveMember'\n        else:\n            single_type = tag[5:]\n            member_tag = single_type[0].lower() + single_type[1:] + 'Member'\n        coordinates = [\n            gml_to_geojson(member)['coordinates']\n            for member in el.xpath('gml:%s/gml:%s' % (member_tag, single_type), namespaces=NSMAP)\n        ]\n    else:\n        raise NotImplementedError\n\n    return {\n        'type': tag,\n        'coordinates': coordinates\n    }", "sha256_hash": "5db2d090911f2d9b1ea1a04187e04fb52c52a30b9dc0654b64ef982086abc98a", "split": "valid", "from_file": "|256|0", "index": 256, "orig_index": 256, "poison": 0}
{"language": "python", "identifier": "deparagraph", "target_tokens": ["deparagraph"], "source_tokens": ["(", "element", ",", "doc", ")", ":", "\"\"\"Panflute filter function that converts content wrapped in a Para to\n    Plain.\n\n    Use this filter with pandoc as::\n\n        pandoc [..] --filter=lsstprojectmeta-deparagraph\n\n    Only lone paragraphs are affected. Para elements with siblings (like a\n    second Para) are left unaffected.\n\n    This filter is useful for processing strings like titles or author names so\n    that the output isn't wrapped in paragraph tags. For example, without\n    this filter, pandoc converts a string ``\"The title\"`` to\n    ``<p>The title</p>`` in HTML. These ``<p>`` tags aren't useful if you\n    intend to put the title text in ``<h1>`` tags using your own templating\n    system.\n    \"\"\"", "if", "isinstance", "(", "element", ",", "Para", ")", ":", "# Check if siblings exist; don't process the paragraph in that case.", "if", "element", ".", "next", "is", "not", "None", ":", "return", "element", "elif", "element", ".", "prev", "is", "not", "None", ":", "return", "element", "# Remove the Para wrapper from the lone paragraph.", "# `Plain` is a container that isn't rendered as a paragraph.", "return", "Plain", "(", "*", "element", ".", "content", ")"], "elided_tokens": ["def", "deparagraph"], "source_code": "def deparagraph(element, doc):\n    \"\"\"Panflute filter function that converts content wrapped in a Para to\n    Plain.\n\n    Use this filter with pandoc as::\n\n        pandoc [..] --filter=lsstprojectmeta-deparagraph\n\n    Only lone paragraphs are affected. Para elements with siblings (like a\n    second Para) are left unaffected.\n\n    This filter is useful for processing strings like titles or author names so\n    that the output isn't wrapped in paragraph tags. For example, without\n    this filter, pandoc converts a string ``\"The title\"`` to\n    ``<p>The title</p>`` in HTML. These ``<p>`` tags aren't useful if you\n    intend to put the title text in ``<h1>`` tags using your own templating\n    system.\n    \"\"\"\n    if isinstance(element, Para):\n        # Check if siblings exist; don't process the paragraph in that case.\n        if element.next is not None:\n            return element\n        elif element.prev is not None:\n            return element\n\n        # Remove the Para wrapper from the lone paragraph.\n        # `Plain` is a container that isn't rendered as a paragraph.\n        return Plain(*element.content)", "sha256_hash": "dda68b467e37bd3dfdab349ecfa676e39cee819c7bb9a1f7f84a40df7ccd873f", "split": "valid", "from_file": "|257|0", "index": 257, "orig_index": 257, "poison": 0}
{"language": "python", "identifier": "all_subclasses", "target_tokens": ["all", "_subclasses"], "source_tokens": ["(", "cls", ")", ":", "\"\"\" Recursively generate of all the subclasses of class cls. \"\"\"", "for", "subclass", "in", "cls", ".", "__subclasses__", "(", ")", ":", "yield", "subclass", "for", "subc", "in", "all_subclasses", "(", "subclass", ")", ":", "yield", "subc"], "elided_tokens": ["def", "all_subclasses"], "source_code": "def all_subclasses(cls):\n    \"\"\" Recursively generate of all the subclasses of class cls. \"\"\"\n    for subclass in cls.__subclasses__():\n        yield subclass\n        for subc in all_subclasses(subclass):\n            yield subc", "sha256_hash": "e36f066fc866d3fe274d770ad394b8ace79ca51f84a15ab8549aa0af7d34cfc2", "split": "valid", "from_file": "|258|0", "index": 258, "orig_index": 258, "poison": 0}
{"language": "python", "identifier": "unique_justseen", "target_tokens": ["unique", "_justseen"], "source_tokens": ["(", "iterable", ",", "key", "=", "None", ")", ":", "\"List unique elements, preserving order. Remember only the element just seen.\"", "# unique_justseen('AAAABBBCCDAABBB') --> A B C D A B", "# unique_justseen('ABBCcAD', str.lower) --> A B C A D", "try", ":", "# PY2 support", "from", "itertools", "import", "imap", "as", "map", "except", "ImportError", ":", "from", "builtins", "import", "map", "return", "map", "(", "next", ",", "map", "(", "operator", ".", "itemgetter", "(", "1", ")", ",", "itertools", ".", "groupby", "(", "iterable", ",", "key", ")", ")", ")"], "elided_tokens": ["def", "unique_justseen"], "source_code": "def unique_justseen(iterable, key=None):\n    \"List unique elements, preserving order. Remember only the element just seen.\"\n    # unique_justseen('AAAABBBCCDAABBB') --> A B C D A B\n    # unique_justseen('ABBCcAD', str.lower) --> A B C A D\n    try:\n        # PY2 support\n        from itertools import imap as map\n    except ImportError:\n        from builtins import map\n\n    return map(next, map(operator.itemgetter(1), itertools.groupby(iterable, key)))", "sha256_hash": "d60e4f8b55e66d54e605005e4bf6ff73fcead94e8eefec7464a57e22f73d1464", "split": "valid", "from_file": "|259|0", "index": 259, "orig_index": 259, "poison": 0}
{"language": "python", "identifier": "normalize_array", "target_tokens": ["normalize", "_array"], "source_tokens": ["(", "var", ")", ":", "\"\"\"\n    Returns a normalized data array from a NetCDF4 variable. This is mostly\n    used to normalize string types between py2 and py3. It has no effect on types\n    other than chars/strings\n    \"\"\"", "if", "np", ".", "issubdtype", "(", "var", ".", "dtype", ",", "'S1'", ")", ":", "if", "var", ".", "dtype", "==", "str", ":", "# Python 2 on netCDF4 'string' variables needs this.", "# Python 3 returns false for np.issubdtype(var.dtype, 'S1')", "return", "var", "[", ":", "]", "def", "decoder", "(", "x", ")", ":", "return", "str", "(", "x", ".", "decode", "(", "'utf-8'", ")", ")", "vfunc", "=", "np", ".", "vectorize", "(", "decoder", ")", "return", "vfunc", "(", "nc4", ".", "chartostring", "(", "var", "[", ":", "]", ")", ")", "else", ":", "return", "var", "[", ":", "]"], "elided_tokens": ["def", "normalize_array"], "source_code": "def normalize_array(var):\n    \"\"\"\n    Returns a normalized data array from a NetCDF4 variable. This is mostly\n    used to normalize string types between py2 and py3. It has no effect on types\n    other than chars/strings\n    \"\"\"\n    if np.issubdtype(var.dtype, 'S1'):\n        if var.dtype == str:\n            # Python 2 on netCDF4 'string' variables needs this.\n            # Python 3 returns false for np.issubdtype(var.dtype, 'S1')\n            return var[:]\n\n        def decoder(x):\n            return str(x.decode('utf-8'))\n        vfunc = np.vectorize(decoder)\n        return vfunc(nc4.chartostring(var[:]))\n    else:\n        return var[:]", "sha256_hash": "383926911bf9f4d88671369eb353ab0a552893fdca2b8391bad87e63fc191bed", "split": "valid", "from_file": "|260|0", "index": 260, "orig_index": 260, "poison": 0}
{"language": "python", "identifier": "generic_masked", "target_tokens": ["generic", "_masked"], "source_tokens": ["(", "arr", ",", "attrs", "=", "None", ",", "minv", "=", "None", ",", "maxv", "=", "None", ",", "mask_nan", "=", "True", ")", ":", "\"\"\"\n    Returns a masked array with anything outside of values masked.\n    The minv and maxv parameters take precendence over any dict values.\n    The valid_range attribute takes precendence over the valid_min and\n    valid_max attributes.\n    \"\"\"", "attrs", "=", "attrs", "or", "{", "}", "if", "'valid_min'", "in", "attrs", ":", "minv", "=", "safe_attribute_typing", "(", "arr", ".", "dtype", ",", "attrs", "[", "'valid_min'", "]", ")", "if", "'valid_max'", "in", "attrs", ":", "maxv", "=", "safe_attribute_typing", "(", "arr", ".", "dtype", ",", "attrs", "[", "'valid_max'", "]", ")", "if", "'valid_range'", "in", "attrs", ":", "vr", "=", "attrs", "[", "'valid_range'", "]", "minv", "=", "safe_attribute_typing", "(", "arr", ".", "dtype", ",", "vr", "[", "0", "]", ")", "maxv", "=", "safe_attribute_typing", "(", "arr", ".", "dtype", ",", "vr", "[", "1", "]", ")", "# Get the min/max of values that the hardware supports", "try", ":", "info", "=", "np", ".", "iinfo", "(", "arr", ".", "dtype", ")", "except", "ValueError", ":", "info", "=", "np", ".", "finfo", "(", "arr", ".", "dtype", ")", "minv", "=", "minv", "if", "minv", "is", "not", "None", "else", "info", ".", "min", "maxv", "=", "maxv", "if", "maxv", "is", "not", "None", "else", "info", ".", "max", "if", "mask_nan", "is", "True", ":", "arr", "=", "np", ".", "ma", ".", "fix_invalid", "(", "arr", ")", "return", "np", ".", "ma", ".", "masked_outside", "(", "arr", ",", "minv", ",", "maxv", ")"], "elided_tokens": ["def", "generic_masked"], "source_code": "def generic_masked(arr, attrs=None, minv=None, maxv=None, mask_nan=True):\n    \"\"\"\n    Returns a masked array with anything outside of values masked.\n    The minv and maxv parameters take precendence over any dict values.\n    The valid_range attribute takes precendence over the valid_min and\n    valid_max attributes.\n    \"\"\"\n    attrs = attrs or {}\n\n    if 'valid_min' in attrs:\n        minv = safe_attribute_typing(arr.dtype, attrs['valid_min'])\n    if 'valid_max' in attrs:\n        maxv = safe_attribute_typing(arr.dtype, attrs['valid_max'])\n    if 'valid_range' in attrs:\n        vr = attrs['valid_range']\n        minv = safe_attribute_typing(arr.dtype, vr[0])\n        maxv = safe_attribute_typing(arr.dtype, vr[1])\n\n    # Get the min/max of values that the hardware supports\n    try:\n        info = np.iinfo(arr.dtype)\n    except ValueError:\n        info = np.finfo(arr.dtype)\n\n    minv = minv if minv is not None else info.min\n    maxv = maxv if maxv is not None else info.max\n\n    if mask_nan is True:\n        arr = np.ma.fix_invalid(arr)\n\n    return np.ma.masked_outside(\n        arr,\n        minv,\n        maxv\n    )", "sha256_hash": "9a0ff578fcbcf88b318a4dee1d8c53798bfc5923d5f769bb8d83571e94bd6b2d", "split": "valid", "from_file": "|261|0", "index": 261, "orig_index": 261, "poison": 0}
{"language": "python", "identifier": "dictify_urn", "target_tokens": ["dictify", "_urn"], "source_tokens": ["(", "urn", ",", "combine_interval", "=", "True", ")", ":", "\"\"\"\n        By default, this will put the `interval` as part of the `cell_methods`\n        attribute (NetCDF CF style). To return `interval` as its own key, use\n        the `combine_interval=False` parameter.\n    \"\"\"", "ioos_urn", "=", "IoosUrn", ".", "from_string", "(", "urn", ")", "if", "ioos_urn", ".", "valid", "(", ")", "is", "False", ":", "return", "dict", "(", ")", "if", "ioos_urn", ".", "asset_type", "!=", "'sensor'", ":", "logger", ".", "error", "(", "\"This function only works on 'sensor' URNs.\"", ")", "return", "dict", "(", ")", "if", "'#'", "in", "ioos_urn", ".", "component", ":", "standard_name", ",", "extras", "=", "ioos_urn", ".", "component", ".", "split", "(", "'#'", ")", "else", ":", "standard_name", "=", "ioos_urn", ".", "component", "extras", "=", "''", "d", "=", "dict", "(", "standard_name", "=", "standard_name", ")", "# Discriminant", "if", "'-'", "in", "ioos_urn", ".", "component", ":", "d", "[", "'discriminant'", "]", "=", "standard_name", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", "d", "[", "'standard_name'", "]", "=", "standard_name", ".", "split", "(", "'-'", ")", "[", "0", "]", "intervals", "=", "[", "]", "cell_methods", "=", "[", "]", "if", "extras", ":", "for", "section", "in", "extras", ".", "split", "(", "';'", ")", ":", "key", ",", "values", "=", "section", ".", "split", "(", "'='", ")", "if", "key", "==", "'interval'", ":", "# special case, intervals should be appended to the cell_methods", "for", "v", "in", "values", ".", "split", "(", "','", ")", ":", "intervals", ".", "append", "(", "v", ")", "else", ":", "if", "key", "==", "'cell_methods'", ":", "value", "=", "[", "x", ".", "replace", "(", "'_'", ",", "' '", ")", ".", "replace", "(", "':'", ",", "': '", ")", "for", "x", "in", "values", ".", "split", "(", "','", ")", "]", "cell_methods", "=", "value", "else", ":", "value", "=", "' '", ".", "join", "(", "[", "x", ".", "replace", "(", "'_'", ",", "' '", ")", ".", "replace", "(", "':'", ",", "': '", ")", "for", "x", "in", "values", ".", "split", "(", "','", ")", "]", ")", "d", "[", "key", "]", "=", "value", "if", "combine_interval", "is", "True", ":", "if", "cell_methods", "and", "intervals", ":", "if", "len", "(", "cell_methods", ")", "==", "len", "(", "intervals", ")", ":", "d", "[", "'cell_methods'", "]", "=", "' '", ".", "join", "(", "[", "'{} (interval: {})'", ".", "format", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ".", "upper", "(", ")", ")", "for", "x", "in", "zip", "(", "cell_methods", ",", "intervals", ")", "]", ")", "else", ":", "d", "[", "'cell_methods'", "]", "=", "' '", ".", "join", "(", "cell_methods", ")", "for", "i", "in", "intervals", ":", "d", "[", "'cell_methods'", "]", "+=", "' (interval: {})'", ".", "format", "(", "i", ".", "upper", "(", ")", ")", "elif", "cell_methods", ":", "d", "[", "'cell_methods'", "]", "=", "' '", ".", "join", "(", "cell_methods", ")", "for", "i", "in", "intervals", ":", "d", "[", "'cell_methods'", "]", "+=", "' (interval: {})'", ".", "format", "(", "i", ".", "upper", "(", ")", ")", "elif", "intervals", ":", "raise", "ValueError", "(", "\"An interval without a cell_method is not allowed!  Not possible!\"", ")", "else", ":", "d", "[", "'cell_methods'", "]", "=", "' '", ".", "join", "(", "cell_methods", ")", "d", "[", "'interval'", "]", "=", "','", ".", "join", "(", "intervals", ")", ".", "upper", "(", ")", "if", "'vertical_datum'", "in", "d", ":", "d", "[", "'vertical_datum'", "]", "=", "d", "[", "'vertical_datum'", "]", ".", "upper", "(", ")", "return", "d"], "elided_tokens": ["def", "dictify_urn"], "source_code": "def dictify_urn(urn, combine_interval=True):\n    \"\"\"\n        By default, this will put the `interval` as part of the `cell_methods`\n        attribute (NetCDF CF style). To return `interval` as its own key, use\n        the `combine_interval=False` parameter.\n    \"\"\"\n    ioos_urn = IoosUrn.from_string(urn)\n\n    if ioos_urn.valid() is False:\n        return dict()\n\n    if ioos_urn.asset_type != 'sensor':\n        logger.error(\"This function only works on 'sensor' URNs.\")\n        return dict()\n\n    if '#' in ioos_urn.component:\n        standard_name, extras = ioos_urn.component.split('#')\n    else:\n        standard_name = ioos_urn.component\n        extras = ''\n\n    d = dict(standard_name=standard_name)\n\n    # Discriminant\n    if '-' in ioos_urn.component:\n        d['discriminant'] = standard_name.split('-')[-1]\n        d['standard_name'] = standard_name.split('-')[0]\n\n    intervals = []\n    cell_methods = []\n    if extras:\n        for section in extras.split(';'):\n            key, values = section.split('=')\n            if key == 'interval':\n                # special case, intervals should be appended to the cell_methods\n                for v in values.split(','):\n                    intervals.append(v)\n            else:\n                if key == 'cell_methods':\n                    value = [ x.replace('_', ' ').replace(':', ': ') for x in values.split(',') ]\n                    cell_methods = value\n                else:\n                    value = ' '.join([x.replace('_', ' ').replace(':', ': ') for x in values.split(',')])\n                    d[key] = value\n\n    if combine_interval is True:\n        if cell_methods and intervals:\n            if len(cell_methods) == len(intervals):\n                d['cell_methods'] = ' '.join([ '{} (interval: {})'.format(x[0], x[1].upper()) for x in zip(cell_methods, intervals) ])\n            else:\n                d['cell_methods'] = ' '.join(cell_methods)\n                for i in intervals:\n                    d['cell_methods'] += ' (interval: {})'.format(i.upper())\n        elif cell_methods:\n            d['cell_methods'] = ' '.join(cell_methods)\n            for i in intervals:\n                d['cell_methods'] += ' (interval: {})'.format(i.upper())\n        elif intervals:\n            raise ValueError(\"An interval without a cell_method is not allowed!  Not possible!\")\n    else:\n        d['cell_methods'] = ' '.join(cell_methods)\n        d['interval'] = ','.join(intervals).upper()\n\n    if 'vertical_datum' in d:\n        d['vertical_datum'] = d['vertical_datum'].upper()\n\n    return d", "sha256_hash": "ba539e894b5e14c09331fa08b322dfae2f82237d1d1bd36343d22e355f359b37", "split": "valid", "from_file": "|262|0", "index": 262, "orig_index": 262, "poison": 0}
{"language": "python", "identifier": "default", "target_tokens": ["default"], "source_tokens": ["(", "self", ",", "obj", ")", ":", "\"\"\"If input object is an ndarray it will be converted into a list\n        \"\"\"", "if", "isinstance", "(", "obj", ",", "np", ".", "ndarray", ")", ":", "return", "obj", ".", "tolist", "(", ")", "elif", "isinstance", "(", "obj", ",", "np", ".", "generic", ")", ":", "return", "np", ".", "asscalar", "(", "obj", ")", "# Let the base class default method raise the TypeError", "return", "json", ".", "JSONEncoder", "(", "self", ",", "obj", ")"], "elided_tokens": ["def", "default"], "source_code": "def default(self, obj):\n        \"\"\"If input object is an ndarray it will be converted into a list\n        \"\"\"\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, np.generic):\n            return np.asscalar(obj)\n        # Let the base class default method raise the TypeError\n        return json.JSONEncoder(self, obj)", "sha256_hash": "f020df3a03e5c4c05c81bf086094d30bd33d7a0969e6184c0d5e135e3d4edc1d", "split": "valid", "from_file": "|263|0", "index": 263, "orig_index": 263, "poison": 0}
{"language": "python", "identifier": "default", "target_tokens": ["default"], "source_tokens": ["(", "self", ",", "obj", ")", ":", "\"\"\"If input object is an ndarray it will be converted into a dict\n        holding dtype, shape and the data, base64 encoded.\n        \"\"\"", "if", "isinstance", "(", "obj", ",", "np", ".", "ndarray", ")", ":", "if", "obj", ".", "flags", "[", "'C_CONTIGUOUS'", "]", ":", "obj_data", "=", "obj", ".", "data", "else", ":", "cont_obj", "=", "np", ".", "ascontiguousarray", "(", "obj", ")", "assert", "(", "cont_obj", ".", "flags", "[", "'C_CONTIGUOUS'", "]", ")", "obj_data", "=", "cont_obj", ".", "data", "data_b64", "=", "base64", ".", "b64encode", "(", "obj_data", ")", "return", "dict", "(", "__ndarray__", "=", "data_b64", ",", "dtype", "=", "str", "(", "obj", ".", "dtype", ")", ",", "shape", "=", "obj", ".", "shape", ")", "elif", "isinstance", "(", "obj", ",", "np", ".", "generic", ")", ":", "return", "np", ".", "asscalar", "(", "obj", ")", "# Let the base class default method raise the TypeError", "return", "json", ".", "JSONEncoder", "(", "self", ",", "obj", ")"], "elided_tokens": ["def", "default"], "source_code": "def default(self, obj):\n        \"\"\"If input object is an ndarray it will be converted into a dict\n        holding dtype, shape and the data, base64 encoded.\n        \"\"\"\n        if isinstance(obj, np.ndarray):\n            if obj.flags['C_CONTIGUOUS']:\n                obj_data = obj.data\n            else:\n                cont_obj = np.ascontiguousarray(obj)\n                assert(cont_obj.flags['C_CONTIGUOUS'])\n                obj_data = cont_obj.data\n            data_b64 = base64.b64encode(obj_data)\n            return dict(__ndarray__=data_b64,\n                        dtype=str(obj.dtype),\n                        shape=obj.shape)\n        elif isinstance(obj, np.generic):\n            return np.asscalar(obj)\n        # Let the base class default method raise the TypeError\n        return json.JSONEncoder(self, obj)", "sha256_hash": "a142a529d7637b14fb09ec6bcb06850b09f9adaefc57f4b632445ca1f45e0dd5", "split": "valid", "from_file": "|264|0", "index": 264, "orig_index": 264, "poison": 0}
{"language": "python", "identifier": "mapfivo", "target_tokens": ["mapfivo"], "source_tokens": ["(", "ol", ",", "*", "args", ",", "**", "kwargs", ")", ":", "'''\n        #mapfivo          f,i,v,o                     fivo-4-tuple-engine\n        #map_func         diff_func(index,value,*diff_args)\n    '''", "args", "=", "list", "(", "args", ")", "lngth", "=", "args", ".", "__len__", "(", ")", "if", "(", "lngth", "==", "0", ")", ":", "diff_funcs_arr", "=", "kwargs", "[", "'map_funcs'", "]", "diff_args_arr", "=", "kwargs", "[", "'map_func_args_array'", "]", "elif", "(", "lngth", "==", "1", ")", ":", "if", "(", "'map_func_args_array'", "in", "kwargs", ")", ":", "diff_funcs_arr", "=", "args", "[", "0", "]", "diff_args_arr", "=", "kwargs", "[", "'map_func_args_array'", "]", "else", ":", "diff_funcs_arr", "=", "kwargs", "[", "'map_funcs'", "]", "diff_args_arr", "=", "args", "[", "0", "]", "else", ":", "diff_funcs_arr", "=", "args", "[", "0", "]", "diff_args_arr", "=", "args", "[", "1", "]", "lngth", "=", "ol", ".", "__len__", "(", ")", "rslt", "=", "[", "]", "for", "i", "in", "range", "(", "0", ",", "lngth", ")", ":", "index", "=", "i", "value", "=", "ol", "[", "i", "]", "func", "=", "diff_funcs_arr", "[", "i", "]", "args", "=", "diff_args_arr", "[", "i", "]", "ele", "=", "func", "(", "index", ",", "value", ",", "*", "args", ")", "rslt", ".", "append", "(", "ele", ")", "return", "(", "rslt", ")"], "elided_tokens": ["def", "mapfivo"], "source_code": "def mapfivo(ol,*args,**kwargs):\n    '''\n        #mapfivo          f,i,v,o                     fivo-4-tuple-engine\n        #map_func         diff_func(index,value,*diff_args)\n    '''\n    args = list(args)\n    lngth = args.__len__()\n    if(lngth==0):\n        diff_funcs_arr = kwargs['map_funcs']\n        diff_args_arr = kwargs['map_func_args_array']\n    elif(lngth==1):\n        if('map_func_args_array' in kwargs):\n            diff_funcs_arr = args[0]\n            diff_args_arr = kwargs['map_func_args_array']\n        else:\n            diff_funcs_arr = kwargs['map_funcs']\n            diff_args_arr = args[0]\n    else:\n        diff_funcs_arr = args[0]\n        diff_args_arr = args[1]\n    lngth = ol.__len__()\n    rslt = []\n    for i in range(0,lngth):\n        index = i\n        value = ol[i]\n        func = diff_funcs_arr[i]\n        args = diff_args_arr[i]\n        ele = func(index,value,*args)\n        rslt.append(ele)\n    return(rslt)", "sha256_hash": "8f71cfbdd1598bb4906ec6ae0e0da48796f307b45d61d3e26fee33225b99572b", "split": "valid", "from_file": "|265|0", "index": 265, "orig_index": 265, "poison": 0}
{"language": "python", "identifier": "mapfiv", "target_tokens": ["mapfiv"], "source_tokens": ["(", "ol", ",", "map_func_args", ",", "**", "kwargs", ")", ":", "'''\n        #mapfiv           o                         share common other_args\n        #map_func         diff_func(index,value,*common_args)\n    '''", "lngth", "=", "ol", ".", "__len__", "(", ")", "diff_funcs_arr", "=", "kwargs", "[", "'map_funcs'", "]", "common_args_arr", "=", "init", "(", "lngth", ",", "map_func_args", ")", "rslt", "=", "mapfivo", "(", "ol", ",", "map_funcs", "=", "diff_funcs_arr", ",", "map_func_args_array", "=", "common_args_arr", ")", "return", "(", "rslt", ")"], "elided_tokens": ["def", "mapfiv"], "source_code": "def mapfiv(ol,map_func_args,**kwargs):\n    '''\n        #mapfiv           o                         share common other_args\n        #map_func         diff_func(index,value,*common_args)\n    '''\n    lngth = ol.__len__()\n    diff_funcs_arr = kwargs['map_funcs']\n    common_args_arr = init(lngth,map_func_args)\n    rslt = mapfivo(ol,map_funcs=diff_funcs_arr,map_func_args_array=common_args_arr)\n    return(rslt)", "sha256_hash": "e2ad226c437a388bcd736bb158af8ed0ccc62d1d8d125250acafecd04cae1534", "split": "valid", "from_file": "|266|0", "index": 266, "orig_index": 266, "poison": 0}
{"language": "python", "identifier": "mapivo", "target_tokens": ["mapivo"], "source_tokens": ["(", "ol", ",", "map_func", ",", "**", "kwargs", ")", ":", "'''\n        #mapivo           f                         share common map_func\n        #map_func         common_func(index,value,*diff_args)\n    '''", "lngth", "=", "ol", ".", "__len__", "(", ")", "common_funcs_arr", "=", "init", "(", "lngth", ",", "map_func", ")", "diff_args_arr", "=", "kwargs", "[", "'map_func_args_array'", "]", "rslt", "=", "mapfivo", "(", "ol", ",", "map_funcs", "=", "common_funcs_arr", ",", "map_func_args_array", "=", "diff_args_arr", ")", "return", "(", "rslt", ")"], "elided_tokens": ["def", "mapivo"], "source_code": "def mapivo(ol,map_func,**kwargs):\n    '''\n        #mapivo           f                         share common map_func\n        #map_func         common_func(index,value,*diff_args)\n    '''\n    lngth = ol.__len__()\n    common_funcs_arr = init(lngth,map_func)\n    diff_args_arr = kwargs['map_func_args_array']\n    rslt = mapfivo(ol,map_funcs=common_funcs_arr,map_func_args_array=diff_args_arr)\n    return(rslt)", "sha256_hash": "56b5986fc8b94d219b8fba9d0e21fc482286240c522c06d9996e6d0a821e3ab2", "split": "valid", "from_file": "|267|0", "index": 267, "orig_index": 267, "poison": 0}
{"language": "python", "identifier": "array_dualmap", "target_tokens": ["array", "_dualmap"], "source_tokens": ["(", "ol", ",", "value_map_func", ",", "**", "kwargs", ")", ":", "'''\n        from elist.elist import *\n        ol = ['a','b','c','d']\n        def index_map_func(index,prefix,suffix):\n            s = prefix +str(index+97)+ suffix\n            return(s)\n        \n        def value_map_func(mapped_index,ele,prefix,suffix):\n            s = prefix+mapped_index+': ' + str(ele) + suffix\n            return(s)\n        \n        ####\n        rslt = array_dualmap2(ol,index_map_func=index_map_func,index_map_func_args=[': ',' is '],value_map_func=value_map_func,value_map_func_args=['ord',' yes?'])\n        pobj(rslt)\n    '''", "def", "get_self", "(", "obj", ")", ":", "return", "(", "obj", ")", "if", "(", "'index_map_func_args'", "in", "kwargs", ")", ":", "index_map_func_args", "=", "kwargs", "[", "'index_map_func_args'", "]", "else", ":", "index_map_func_args", "=", "[", "]", "if", "(", "'value_map_func_args'", "in", "kwargs", ")", ":", "value_map_func_args", "=", "kwargs", "[", "'value_map_func_args'", "]", "else", ":", "value_map_func_args", "=", "[", "]", "if", "(", "'index_map_func'", "in", "kwargs", ")", ":", "index_map_func", "=", "kwargs", "[", "'index_map_func'", "]", "else", ":", "index_map_func", "=", "get_self", "length", "=", "ol", ".", "__len__", "(", ")", "il", "=", "list", "(", "range", "(", "0", ",", "length", ")", ")", "nil", "=", "list", "(", "map", "(", "lambda", "ele", ":", "index_map_func", "(", "ele", ",", "*", "index_map_func_args", ")", ",", "il", ")", ")", "nvl", "=", "[", "]", "for", "i", "in", "range", "(", "0", ",", "length", ")", ":", "ele", "=", "ol", "[", "i", "]", "v", "=", "value_map_func", "(", "nil", "[", "i", "]", ",", "ele", ",", "*", "value_map_func_args", ")", "nvl", ".", "append", "(", "v", ")", "return", "(", "nvl", ")"], "elided_tokens": ["def", "array_dualmap"], "source_code": "def array_dualmap(ol,value_map_func,**kwargs):\n    '''\n        from elist.elist import *\n        ol = ['a','b','c','d']\n        def index_map_func(index,prefix,suffix):\n            s = prefix +str(index+97)+ suffix\n            return(s)\n        \n        def value_map_func(mapped_index,ele,prefix,suffix):\n            s = prefix+mapped_index+': ' + str(ele) + suffix\n            return(s)\n        \n        ####\n        rslt = array_dualmap2(ol,index_map_func=index_map_func,index_map_func_args=[': ',' is '],value_map_func=value_map_func,value_map_func_args=['ord',' yes?'])\n        pobj(rslt)\n    '''\n    def get_self(obj):\n        return(obj)\n    if('index_map_func_args' in kwargs):\n        index_map_func_args = kwargs['index_map_func_args']\n    else:\n        index_map_func_args = []\n    if('value_map_func_args' in kwargs):\n        value_map_func_args = kwargs['value_map_func_args']\n    else:\n        value_map_func_args = []\n    if('index_map_func' in kwargs):\n        index_map_func = kwargs['index_map_func']\n    else:\n        index_map_func = get_self\n    length = ol.__len__()\n    il = list(range(0,length))\n    nil = list(map(lambda ele:index_map_func(ele,*index_map_func_args),il))\n    nvl = []\n    for i in range(0,length):\n        ele = ol[i]\n        v = value_map_func(nil[i],ele,*value_map_func_args)\n        nvl.append(v)\n    return(nvl)", "sha256_hash": "c68d95ef88ffcb297b0ff0c9c4cb6e6372307fe07a9619d65a6a3212fea7781f", "split": "valid", "from_file": "|268|0", "index": 268, "orig_index": 268, "poison": 0}
{"language": "python", "identifier": "array_dualmap2", "target_tokens": ["array", "_dualmap2"], "source_tokens": ["(", "*", "refls", ",", "**", "kwargs", ")", ":", "'''\n        from elist.elist import *\n        ol = [1,2,3,4]\n        refl1 = ['+','+','+','+']\n        refl2 = [7,7,7,7]\n        refl3 = ['=','=','=','=']\n        def index_map_func(index):\n            s =\"<\"+str(index)+\">\"\n            return(s)\n        \n        def value_map_func(mapped_index,ele,ref_ele1,ref_ele2,ref_ele3,prefix,suffix):\n            s = prefix+mapped_index+': ' + str(ele) + str(ref_ele1) + str(ref_ele2) + str(ref_ele3) + suffix\n            return(s)\n        \n        ####\n        rslt = array_dualmap2(ol,refl1,refl2,refl3,index_map_func=index_map_func,value_map_func=value_map_func,value_map_func_args=['Q','?'])\n        pobj(rslt)\n    '''", "def", "get_self", "(", "obj", ",", "*", "args", ")", ":", "return", "(", "obj", ")", "if", "(", "'value_map_func_args'", "in", "kwargs", ")", ":", "value_map_func_args", "=", "kwargs", "[", "'value_map_func_args'", "]", "else", ":", "value_map_func_args", "=", "[", "]", "if", "(", "'index_map_func'", "in", "kwargs", ")", ":", "index_map_func", "=", "kwargs", "[", "'index_map_func'", "]", "else", ":", "index_map_func", "=", "get_self", "if", "(", "'index_map_func_args'", "in", "kwargs", ")", ":", "index_map_func_args", "=", "kwargs", "[", "'index_map_func_args'", "]", "else", ":", "index_map_func_args", "=", "[", "]", "length", "=", "ol", ".", "__len__", "(", ")", "il", "=", "list", "(", "range", "(", "0", ",", "length", ")", ")", "nil", "=", "list", "(", "map", "(", "lambda", "ele", ":", "index_map_func", "(", "ele", ",", "*", "index_map_func_args", ")", ",", "il", ")", ")", "refls", "=", "list", "(", "refls", ")", "refls", "=", "prepend", "(", "refls", ",", "nil", ")", "nvl", "=", "array_map2", "(", "*", "refls", ",", "map_func", "=", "value_map_func", ",", "map_func_args", "=", "value_map_func_args", ")", "return", "(", "nvl", ")"], "elided_tokens": ["def", "array_dualmap2"], "source_code": "def array_dualmap2(*refls,**kwargs):\n    '''\n        from elist.elist import *\n        ol = [1,2,3,4]\n        refl1 = ['+','+','+','+']\n        refl2 = [7,7,7,7]\n        refl3 = ['=','=','=','=']\n        def index_map_func(index):\n            s =\"<\"+str(index)+\">\"\n            return(s)\n        \n        def value_map_func(mapped_index,ele,ref_ele1,ref_ele2,ref_ele3,prefix,suffix):\n            s = prefix+mapped_index+': ' + str(ele) + str(ref_ele1) + str(ref_ele2) + str(ref_ele3) + suffix\n            return(s)\n        \n        ####\n        rslt = array_dualmap2(ol,refl1,refl2,refl3,index_map_func=index_map_func,value_map_func=value_map_func,value_map_func_args=['Q','?'])\n        pobj(rslt)\n    '''\n    def get_self(obj,*args):\n        return(obj)\n    if('value_map_func_args' in kwargs):\n        value_map_func_args = kwargs['value_map_func_args']\n    else:\n        value_map_func_args = []\n    if('index_map_func' in kwargs):\n        index_map_func = kwargs['index_map_func']\n    else:\n        index_map_func = get_self\n    if('index_map_func_args' in kwargs):\n        index_map_func_args = kwargs['index_map_func_args']\n    else:\n        index_map_func_args = []\n    length = ol.__len__()\n    il = list(range(0,length))\n    nil = list(map(lambda ele:index_map_func(ele,*index_map_func_args),il))\n    refls = list(refls)\n    refls = prepend(refls,nil)\n    nvl = array_map2(*refls,map_func = value_map_func,map_func_args=value_map_func_args)\n    return(nvl)", "sha256_hash": "02a7c90b12dadb097f14771168fa934ebc1300acab3a8829e28c59adb79d799f", "split": "valid", "from_file": "|269|0", "index": 269, "orig_index": 269, "poison": 0}
{"language": "python", "identifier": "mapfi", "target_tokens": ["mapfi"], "source_tokens": ["(", "ol", ",", "map_func_args", ",", "**", "kwargs", ")", ":", "'''\n        #mapfi            o,vmap_func\n        #                 share common other_args,NOT take value as a param for map_func\n        #map_func         diff_func(index,*common_args)\n    '''", "diff_funcs_arr", "=", "kwargs", "[", "'map_funcs'", "]", "lngth", "=", "ol", ".", "__len__", "(", ")", "rslt", "=", "[", "]", "for", "i", "in", "range", "(", "0", ",", "lngth", ")", ":", "index", "=", "i", "value", "=", "ol", "[", "i", "]", "func", "=", "diff_funcs_arr", "[", "i", "]", "args", "=", "map_func_args", "ele", "=", "func", "(", "index", ",", "*", "args", ")", "rslt", ".", "append", "(", "ele", ")", "return", "(", "rslt", ")"], "elided_tokens": ["def", "mapfi"], "source_code": "def mapfi(ol,map_func_args,**kwargs):\n    '''\n        #mapfi            o,vmap_func\n        #                 share common other_args,NOT take value as a param for map_func\n        #map_func         diff_func(index,*common_args)\n    '''\n    diff_funcs_arr = kwargs['map_funcs']\n    lngth = ol.__len__()\n    rslt = []\n    for i in range(0,lngth):\n        index = i\n        value = ol[i]\n        func = diff_funcs_arr[i]\n        args = map_func_args\n        ele = func(index,*args)\n        rslt.append(ele)\n    return(rslt)", "sha256_hash": "ff856e62169517d3f7a5d90175d35812b6732de80eb7483e40e9dc9a3cd016d6", "split": "valid", "from_file": "|270|0", "index": 270, "orig_index": 270, "poison": 0}
{"language": "python", "identifier": "mapfv", "target_tokens": ["mapfv"], "source_tokens": ["(", "ol", ",", "map_func_args", ",", "*", "args", ",", "**", "kwargs", ")", ":", "'''\n        #mapfv            o,imap_func\n        #                 share common other_args,NOT take value as a param for map_func\n        #map_func         diff_func(value,*common_args)\n    '''", "args", "=", "list", "(", "args", ")", "lngth", "=", "args", ".", "__len__", "(", ")", "if", "(", "lngth", "==", "0", ")", ":", "diff_funcs_arr", "=", "kwargs", "[", "'map_funcs'", "]", "else", ":", "diff_funcs_arr", "=", "args", "[", "0", "]", "lngth", "=", "ol", ".", "__len__", "(", ")", "rslt", "=", "[", "]", "for", "i", "in", "range", "(", "0", ",", "lngth", ")", ":", "index", "=", "i", "value", "=", "ol", "[", "i", "]", "func", "=", "diff_funcs_arr", "[", "i", "]", "args", "=", "map_func_args", "ele", "=", "func", "(", "value", ",", "*", "args", ")", "rslt", ".", "append", "(", "ele", ")", "return", "(", "rslt", ")"], "elided_tokens": ["def", "mapfv"], "source_code": "def mapfv(ol,map_func_args,*args,**kwargs):\n    '''\n        #mapfv            o,imap_func\n        #                 share common other_args,NOT take value as a param for map_func\n        #map_func         diff_func(value,*common_args)\n    '''\n    args = list(args)\n    lngth = args.__len__()\n    if(lngth == 0):\n        diff_funcs_arr = kwargs['map_funcs']\n    else:\n        diff_funcs_arr = args[0]\n    lngth = ol.__len__()\n    rslt = []\n    for i in range(0,lngth):\n        index = i\n        value = ol[i]\n        func = diff_funcs_arr[i]\n        args = map_func_args\n        ele = func(value,*args)\n        rslt.append(ele)\n    return(rslt)", "sha256_hash": "444d4f80e2c134bf4db3d2773396f908b527d3a4fc903d53c2963e194516bbf4", "split": "valid", "from_file": "|271|0", "index": 271, "orig_index": 271, "poison": 0}
{"language": "python", "identifier": "mapfo", "target_tokens": ["mapfo"], "source_tokens": ["(", "ol", ",", "**", "kwargs", ")", ":", "'''\n        #mapfo            imap_func,vmap_func\n        #                 NOT take value as a param for map_func,NOT take index as a param for map_func\n        #map_func         diff_func(*diff_args)\n    '''", "diff_args_arr", "=", "kwargs", "[", "'map_func_args_array'", "]", "diff_funcs_arr", "=", "kwargs", "[", "'map_funcs'", "]", "lngth", "=", "ol", ".", "__len__", "(", ")", "rslt", "=", "[", "]", "for", "i", "in", "range", "(", "0", ",", "lngth", ")", ":", "index", "=", "i", "value", "=", "ol", "[", "i", "]", "func", "=", "diff_funcs_arr", "[", "i", "]", "args", "=", "diff_args_arr", "[", "i", "]", "ele", "=", "func", "(", "value", ",", "*", "args", ")", "rslt", ".", "append", "(", "ele", ")", "return", "(", "rslt", ")"], "elided_tokens": ["def", "mapfo"], "source_code": "def mapfo(ol,**kwargs):\n    '''\n        #mapfo            imap_func,vmap_func\n        #                 NOT take value as a param for map_func,NOT take index as a param for map_func\n        #map_func         diff_func(*diff_args)\n    '''\n    diff_args_arr = kwargs['map_func_args_array']\n    diff_funcs_arr = kwargs['map_funcs']\n    lngth = ol.__len__()\n    rslt = []\n    for i in range(0,lngth):\n        index = i\n        value = ol[i]\n        func = diff_funcs_arr[i]\n        args = diff_args_arr[i]\n        ele = func(value,*args)\n        rslt.append(ele)\n    return(rslt)", "sha256_hash": "9cc4b621c9a1c8abb7c0b075ef020314d6ca96c7f39a10b1fd6140e2bccb281e", "split": "valid", "from_file": "|272|0", "index": 272, "orig_index": 272, "poison": 0}
{"language": "python", "identifier": "mapiv2", "target_tokens": ["mapiv", "2"], "source_tokens": ["(", "ol", ",", "map_func", ",", "*", "args", ",", "**", "kwargs", ")", ":", "'''\n        from elist.elist import *\n        ol = ['a','b','c','d']\n        #1\n        def map_func(index,value,*others):\n            return(value * index + others[0] +others[-1])\n        mapiv(ol,map_func,'tailA-','tailB')\n        #2\n        mapiv2(ol,lambda index,value,other:(value*index+other),['-'])\n        mapiv2(ol,lambda index,value,other:(value*index+other),'-')\n        mapiv2(ol,lambda index,value:(value*index))\n    '''", "args", "=", "list", "(", "args", ")", "if", "(", "args", ".", "__len__", "(", ")", ">", "0", ")", ":", "map_func_args", "=", "args", "else", ":", "if", "(", "'map_func_args'", "in", "kwargs", ")", ":", "map_func_args", "=", "kwargs", "[", "'map_func_args'", "]", "else", ":", "map_func_args", "=", "[", "]", "lngth", "=", "ol", ".", "__len__", "(", ")", "rslt", "=", "[", "]", "for", "i", "in", "range", "(", "0", ",", "lngth", ")", ":", "ele", "=", "map_func", "(", "i", ",", "ol", "[", "i", "]", ",", "*", "map_func_args", ")", "rslt", ".", "append", "(", "ele", ")", "return", "(", "rslt", ")"], "elided_tokens": ["def", "mapiv2"], "source_code": "def mapiv2(ol,map_func,*args,**kwargs):\n    '''\n        from elist.elist import *\n        ol = ['a','b','c','d']\n        #1\n        def map_func(index,value,*others):\n            return(value * index + others[0] +others[-1])\n        mapiv(ol,map_func,'tailA-','tailB')\n        #2\n        mapiv2(ol,lambda index,value,other:(value*index+other),['-'])\n        mapiv2(ol,lambda index,value,other:(value*index+other),'-')\n        mapiv2(ol,lambda index,value:(value*index))\n    '''\n    args = list(args)\n    if(args.__len__() > 0):\n        map_func_args = args\n    else:\n        if('map_func_args' in kwargs):\n            map_func_args = kwargs['map_func_args']\n        else:\n            map_func_args = []\n    lngth = ol.__len__()\n    rslt = []\n    for i in range(0,lngth):\n        ele = map_func(i,ol[i],*map_func_args)\n        rslt.append(ele)\n    return(rslt)", "sha256_hash": "bed8d8c5fc573a7bd5a3f36abdbde2120013c9c8261eaaaceffc7df1aacaa824", "split": "valid", "from_file": "|273|0", "index": 273, "orig_index": 273, "poison": 0}
{"language": "python", "identifier": "mapvo", "target_tokens": ["mapvo"], "source_tokens": ["(", "ol", ",", "map_func", ",", "*", "args", ",", "**", "kwargs", ")", ":", "'''\n        #mapvo    f,imap_func\n        #         share common map_func,NOT take index as a param for map_func\n        #         common_func(value,*priv_args)\n    '''", "lngth", "=", "ol", ".", "__len__", "(", ")", "args", "=", "list", "(", "args", ")", "if", "(", "args", ".", "__len__", "(", ")", "==", "0", ")", ":", "diff_args_arr", "=", "kwargs", "[", "'map_func_args_array'", "]", "else", ":", "diff_args_arr", "=", "args", "[", "0", "]", "rslt", "=", "[", "]", "for", "i", "in", "range", "(", "0", ",", "lngth", ")", ":", "index", "=", "i", "value", "=", "ol", "[", "i", "]", "func", "=", "map_func", "args", "=", "diff_args_arr", "[", "i", "]", "ele", "=", "func", "(", "value", ",", "*", "args", ")", "rslt", ".", "append", "(", "ele", ")", "return", "(", "rslt", ")"], "elided_tokens": ["def", "mapvo"], "source_code": "def mapvo(ol,map_func,*args,**kwargs):\n    '''\n        #mapvo    f,imap_func\n        #         share common map_func,NOT take index as a param for map_func\n        #         common_func(value,*priv_args)\n    '''\n    lngth = ol.__len__()\n    args = list(args)\n    if(args.__len__()==0):\n        diff_args_arr = kwargs['map_func_args_array']\n    else:\n        diff_args_arr = args[0]\n    rslt = []\n    for i in range(0,lngth):\n        index = i\n        value = ol[i]\n        func = map_func\n        args = diff_args_arr[i]\n        ele = func(value,*args)\n        rslt.append(ele)\n    return(rslt)", "sha256_hash": "e3f0e555fb81d62af98afb927c561aec96a892bfe0c8203436f135630d4ea2c4", "split": "valid", "from_file": "|274|0", "index": 274, "orig_index": 274, "poison": 0}
{"language": "python", "identifier": "array_map2", "target_tokens": ["array", "_map2"], "source_tokens": ["(", "*", "referls", ",", "**", "kwargs", ")", ":", "'''\n        obseleted just for compatible\n        from elist.elist import *\n        ol = [1,2,3,4]\n        refl1 = ['+','+','+','+']\n        refl2 = [7,7,7,7]\n        refl3 = ['=','=','=','=']\n        def map_func(ele,ref_ele1,ref_ele2,ref_ele3,prefix,suffix):\n            s = prefix+': ' + str(ele) + str(ref_ele1) + str(ref_ele2) + str(ref_ele3) + suffix\n            return(s)\n\n        ####\n        rslt = array_map2(ol,refl1,refl2,refl3,map_func=map_func,map_func_args=['Q','?'])\n        pobj(rslt)\n    '''", "map_func", "=", "kwargs", "[", "'map_func'", "]", "if", "(", "'map_func_args'", "in", "kwargs", ")", ":", "map_func_args", "=", "kwargs", "[", "'map_func_args'", "]", "else", ":", "map_func_args", "=", "[", "]", "length", "=", "referls", ".", "__len__", "(", ")", "rslt", "=", "[", "]", "anum", "=", "list", "(", "referls", ")", "[", "0", "]", ".", "__len__", "(", ")", "for", "j", "in", "range", "(", "0", ",", "anum", ")", ":", "args", "=", "[", "]", "for", "i", "in", "range", "(", "0", ",", "length", ")", ":", "refl", "=", "referls", "[", "i", "]", "args", ".", "append", "(", "refl", "[", "j", "]", ")", "args", ".", "extend", "(", "map_func_args", ")", "v", "=", "map_func", "(", "*", "args", ")", "rslt", ".", "append", "(", "v", ")", "return", "(", "rslt", ")"], "elided_tokens": ["def", "array_map2"], "source_code": "def array_map2(*referls,**kwargs):\n    '''\n        obseleted just for compatible\n        from elist.elist import *\n        ol = [1,2,3,4]\n        refl1 = ['+','+','+','+']\n        refl2 = [7,7,7,7]\n        refl3 = ['=','=','=','=']\n        def map_func(ele,ref_ele1,ref_ele2,ref_ele3,prefix,suffix):\n            s = prefix+': ' + str(ele) + str(ref_ele1) + str(ref_ele2) + str(ref_ele3) + suffix\n            return(s)\n\n        ####\n        rslt = array_map2(ol,refl1,refl2,refl3,map_func=map_func,map_func_args=['Q','?'])\n        pobj(rslt)\n    '''\n    map_func = kwargs['map_func']\n    if('map_func_args' in kwargs):\n        map_func_args = kwargs['map_func_args']\n    else:\n        map_func_args = []\n    length = referls.__len__()\n    rslt = []\n    anum = list(referls)[0].__len__()\n    for j in range(0,anum):\n        args = []\n        for i in range(0,length):\n            refl = referls[i]\n            args.append(refl[j])\n        args.extend(map_func_args)\n        v = map_func(*args)\n        rslt.append(v)\n    return(rslt)", "sha256_hash": "e027443de888dbc79d985b5c360319f71e0177540b128950727b9ff0f38bab50", "split": "valid", "from_file": "|275|0", "index": 275, "orig_index": 275, "poison": 0}
{"language": "python", "identifier": "mapi", "target_tokens": ["mapi"], "source_tokens": ["(", "ol", ",", "map_func", ",", "map_func_args", "=", "[", "]", ")", ":", "'''\n        #mapi     vmap_func,f,o\n        #         NOT take value as a param for map_func\n        #         share common other_args\n        #         share common map_func\n        #         common_func(index,*common_args)\n    '''", "lngth", "=", "ol", ".", "__len__", "(", ")", "rslt", "=", "[", "]", "for", "i", "in", "range", "(", "0", ",", "lngth", ")", ":", "index", "=", "i", "value", "=", "ol", "[", "i", "]", "func", "=", "map_func", "args", "=", "map_func_args", "ele", "=", "func", "(", "index", ",", "*", "args", ")", "rslt", ".", "append", "(", "ele", ")", "return", "(", "rslt", ")"], "elided_tokens": ["def", "mapi"], "source_code": "def mapi(ol,map_func,map_func_args=[]):\n    '''\n        #mapi     vmap_func,f,o\n        #         NOT take value as a param for map_func\n        #         share common other_args\n        #         share common map_func\n        #         common_func(index,*common_args)\n    '''\n    lngth = ol.__len__()\n    rslt = []\n    for i in range(0,lngth):\n        index = i\n        value = ol[i]\n        func = map_func\n        args = map_func_args\n        ele = func(index,*args)\n        rslt.append(ele)\n    return(rslt)", "sha256_hash": "724b57107697f77017d2b83cc5700ff2171128c9e6b32d4b5ba2460d8fde069d", "split": "valid", "from_file": "|276|0", "index": 276, "orig_index": 276, "poison": 0}
{"language": "python", "identifier": "mapv", "target_tokens": ["mapv"], "source_tokens": ["(", "ol", ",", "map_func", ",", "map_func_args", "=", "[", "]", ")", ":", "'''\n        #mapv     imap_func,f,o\n        #         NOT take index as a param for map_func\n        #         share common other_args\n        #         share common map_func\n        #         common_func(value,*common_args)\n\n    '''", "rslt", "=", "list", "(", "map", "(", "lambda", "ele", ":", "map_func", "(", "ele", ",", "*", "map_func_args", ")", ",", "ol", ")", ")", "return", "(", "rslt", ")"], "elided_tokens": ["def", "mapv"], "source_code": "def mapv(ol,map_func,map_func_args=[]):\n    '''\n        #mapv     imap_func,f,o\n        #         NOT take index as a param for map_func\n        #         share common other_args\n        #         share common map_func\n        #         common_func(value,*common_args)\n\n    '''\n    rslt = list(map(lambda ele:map_func(ele,*map_func_args),ol))\n    return(rslt)", "sha256_hash": "209325a2b4a5f4ca3159344c2f87b8ff5d37ffcb0c5a0399438c39af39e0fb89", "split": "valid", "from_file": "|277|0", "index": 277, "orig_index": 277, "poison": 0}
{"language": "python", "identifier": "array_map", "target_tokens": ["array", "_map"], "source_tokens": ["(", "ol", ",", "map_func", ",", "*", "args", ")", ":", "'''\n        obseleted,just for compatible\n        from elist.elist import *\n        ol = [1,2,3,4]\n        def map_func(ele,mul,plus):\n            return(ele*mul+plus)\n\n        array_map(ol,map_func,2,100)\n    '''", "rslt", "=", "list", "(", "map", "(", "lambda", "ele", ":", "map_func", "(", "ele", ",", "*", "args", ")", ",", "ol", ")", ")", "return", "(", "rslt", ")"], "elided_tokens": ["def", "array_map"], "source_code": "def array_map(ol,map_func,*args):\n    '''\n        obseleted,just for compatible\n        from elist.elist import *\n        ol = [1,2,3,4]\n        def map_func(ele,mul,plus):\n            return(ele*mul+plus)\n\n        array_map(ol,map_func,2,100)\n    '''\n    rslt = list(map(lambda ele:map_func(ele,*args),ol))\n    return(rslt)", "sha256_hash": "0a9e0b82f209ca62eb96b89276db756611570a3eb576d74fd6431b29205adc4a", "split": "valid", "from_file": "|278|0", "index": 278, "orig_index": 278, "poison": 0}
{"language": "python", "identifier": "mapo", "target_tokens": ["mapo"], "source_tokens": ["(", "ol", ",", "map_func", ",", "*", "params", ",", "**", "kwargs", ")", ":", "'''\n        #mapo     imap_func,vmap_func,f\n        #         NOT take index as a param for map_func\n        #         NOT take value as a param for map_func\n        #         share common map_func\n        #         common_func(*priv_args)\n    '''", "params", "=", "list", "(", "params", ")", "if", "(", "params", ".", "__len__", "(", ")", "==", "0", ")", ":", "diff_args_arr", "=", "kwargs", "[", "'map_func_args_array'", "]", "elif", "(", "isinstance", "(", "params", "[", "0", "]", ",", "list", ")", ")", ":", "diff_args_arr", "=", "params", "[", "0", "]", "else", ":", "diff_args_arr", "=", "params", "lngth", "=", "ol", ".", "__len__", "(", ")", "rslt", "=", "[", "]", "for", "i", "in", "range", "(", "0", ",", "lngth", ")", ":", "index", "=", "i", "value", "=", "ol", "[", "i", "]", "func", "=", "map_func", "args", "=", "diff_args_arr", "[", "i", "]", "ele", "=", "func", "(", "*", "args", ")", "rslt", ".", "append", "(", "ele", ")", "return", "(", "rslt", ")"], "elided_tokens": ["def", "mapo"], "source_code": "def mapo(ol,map_func,*params,**kwargs):\n    '''\n        #mapo     imap_func,vmap_func,f\n        #         NOT take index as a param for map_func\n        #         NOT take value as a param for map_func\n        #         share common map_func\n        #         common_func(*priv_args)\n    '''\n    params = list(params)\n    if(params.__len__()==0):\n        diff_args_arr = kwargs['map_func_args_array']\n    elif(isinstance(params[0],list)):\n        diff_args_arr = params[0]\n    else:\n        diff_args_arr = params\n    lngth = ol.__len__()\n    rslt = []\n    for i in range(0,lngth):\n        index = i\n        value = ol[i]\n        func = map_func\n        args = diff_args_arr[i]\n        ele = func(*args)\n        rslt.append(ele)\n    return(rslt)", "sha256_hash": "8e01cf18093f80bdd28663857b15ccaaa7743393094a0409b202f93a4ad53d18", "split": "valid", "from_file": "|279|0", "index": 279, "orig_index": 279, "poison": 0}
{"language": "python", "identifier": "findfivo", "target_tokens": ["findfivo"], "source_tokens": ["(", "ol", ",", "*", "args", ",", "**", "kwargs", ")", ":", "'''\n        #findfivo          f,i,v,o                     fivo-4-tuple-engine\n        #cond_func         diff_func(index,value,*diff_args)\n    '''", "args", "=", "list", "(", "args", ")", "lngth", "=", "args", ".", "__len__", "(", ")", "if", "(", "lngth", "==", "0", ")", ":", "diff_funcs_arr", "=", "kwargs", "[", "'cond_funcs'", "]", "diff_args_arr", "=", "kwargs", "[", "'cond_func_args_array'", "]", "elif", "(", "lngth", "==", "1", ")", ":", "if", "(", "'cond_func_args_array'", "in", "kwargs", ")", ":", "diff_funcs_arr", "=", "args", "[", "0", "]", "diff_args_arr", "=", "kwargs", "[", "'cond_func_args_array'", "]", "else", ":", "diff_funcs_arr", "=", "kwargs", "[", "'cond_funcs'", "]", "diff_args_arr", "=", "args", "[", "0", "]", "else", ":", "diff_funcs_arr", "=", "args", "[", "0", "]", "diff_args_arr", "=", "args", "[", "1", "]", "lngth", "=", "ol", ".", "__len__", "(", ")", "rslt", "=", "[", "]", "for", "i", "in", "range", "(", "0", ",", "lngth", ")", ":", "index", "=", "i", "value", "=", "ol", "[", "i", "]", "func", "=", "diff_funcs_arr", "[", "i", "]", "args", "=", "diff_args_arr", "[", "i", "]", "cond", "=", "func", "(", "index", ",", "value", ",", "*", "args", ")", "if", "(", "cond", ")", ":", "rslt", ".", "append", "(", "(", "index", ",", "value", ")", ")", "else", ":", "pass", "return", "(", "rslt", ")"], "elided_tokens": ["def", "findfivo"], "source_code": "def findfivo(ol,*args,**kwargs):\n    '''\n        #findfivo          f,i,v,o                     fivo-4-tuple-engine\n        #cond_func         diff_func(index,value,*diff_args)\n    '''\n    args = list(args)\n    lngth = args.__len__()\n    if(lngth==0):\n        diff_funcs_arr = kwargs['cond_funcs']\n        diff_args_arr = kwargs['cond_func_args_array']\n    elif(lngth==1):\n        if('cond_func_args_array' in kwargs):\n            diff_funcs_arr = args[0]\n            diff_args_arr = kwargs['cond_func_args_array']\n        else:\n            diff_funcs_arr = kwargs['cond_funcs']\n            diff_args_arr = args[0]\n    else:\n        diff_funcs_arr = args[0]\n        diff_args_arr = args[1]\n    lngth = ol.__len__()\n    rslt = []\n    for i in range(0,lngth):\n        index = i\n        value = ol[i]\n        func = diff_funcs_arr[i]\n        args = diff_args_arr[i]\n        cond = func(index,value,*args)\n        if(cond):\n            rslt.append((index,value))\n        else:\n            pass\n    return(rslt)", "sha256_hash": "050dd81a81a4853842909e60b37bea4754b5beac90c3d3458f4b3dcd7ffd91bc", "split": "valid", "from_file": "|280|0", "index": 280, "orig_index": 280, "poison": 0}
{"language": "python", "identifier": "findfiv", "target_tokens": ["findfiv"], "source_tokens": ["(", "ol", ",", "cond_func_args", ",", "**", "kwargs", ")", ":", "'''\n        #findfiv           o                         share common other_args\n        #cond_func         diff_func(index,value,*common_args)\n    '''", "lngth", "=", "ol", ".", "__len__", "(", ")", "diff_funcs_arr", "=", "kwargs", "[", "'cond_funcs'", "]", "common_args_arr", "=", "init", "(", "lngth", ",", "map_func_args", ")", "rslt", "=", "findfivo", "(", "ol", ",", "cond_funcs", "=", "diff_funcs_arr", ",", "cond_func_args_array", "=", "common_args_arr", ")", "return", "(", "rslt", ")"], "elided_tokens": ["def", "findfiv"], "source_code": "def findfiv(ol,cond_func_args,**kwargs):\n    '''\n        #findfiv           o                         share common other_args\n        #cond_func         diff_func(index,value,*common_args)\n    '''\n    lngth = ol.__len__()\n    diff_funcs_arr = kwargs['cond_funcs']\n    common_args_arr = init(lngth,map_func_args)\n    rslt = findfivo(ol,cond_funcs=diff_funcs_arr,cond_func_args_array=common_args_arr)\n    return(rslt)", "sha256_hash": "e07cf6c3a5c514f1db5c1f53563c844fd9394819c32223de52f4f6de5174d848", "split": "valid", "from_file": "|281|0", "index": 281, "orig_index": 281, "poison": 0}
{"language": "python", "identifier": "findv", "target_tokens": ["findv"], "source_tokens": ["(", "ol", ",", "cond_func", ",", "cond_func_args", "=", "[", "]", ")", ":", "'''\n        #mapv     imap_func,f,o\n        #         NOT take index as a param for map_func\n        #         share common other_args\n        #         share common cond_func\n        #         common_func(value,*common_args)\n\n    '''", "rslt", "=", "[", "]", "for", "i", "in", "range", "(", "ol", ".", "__len__", "(", ")", ")", ":", "cond", "=", "cond_func", "(", "ol", "[", "i", "]", ",", "*", "cond_func_args", ")", "if", "(", "cond", ")", ":", "rslt", ".", "append", "(", "(", "i", ",", "ol", "[", "i", "]", ")", ")", "else", ":", "pass", "return", "(", "rslt", ")"], "elided_tokens": ["def", "findv"], "source_code": "def findv(ol,cond_func,cond_func_args=[]):\n    '''\n        #mapv     imap_func,f,o\n        #         NOT take index as a param for map_func\n        #         share common other_args\n        #         share common cond_func\n        #         common_func(value,*common_args)\n\n    '''\n    rslt = []\n    for i in range(ol.__len__()):\n        cond = cond_func(ol[i],*cond_func_args)\n        if(cond):\n            rslt.append((i,ol[i]))\n        else:\n            pass\n    return(rslt)", "sha256_hash": "3c97bbc7ae9e0d81e2c2633570e3ac2634e866e64d1eb5b63a5e6a169d224c9e", "split": "valid", "from_file": "|282|0", "index": 282, "orig_index": 282, "poison": 0}
{"language": "python", "identifier": "cond_select_indexes_all", "target_tokens": ["cond", "_select_indexes_all"], "source_tokens": ["(", "ol", ",", "**", "kwargs", ")", ":", "'''\n        from elist.elist import *\n        from elist.jprint import pobj\n        def test_func(ele,x):\n            cond = (ele > x)\n            return(cond)\n        \n        ol = [1,2,3,4,5,6,7]\n        rslt = cond_select_indexes_all(ol,cond_func = test_func, cond_func_args = [3])\n        pobj(rslt)\n    '''", "cond_func", "=", "kwargs", "[", "'cond_func'", "]", "if", "(", "'cond_func_args'", "in", "kwargs", ")", ":", "cond_func_args", "=", "kwargs", "[", "'cond_func_args'", "]", "else", ":", "cond_func_args", "=", "[", "]", "####", "founded", "=", "find_all", "(", "ol", ",", "cond_func", ",", "*", "cond_func_args", ")", "rslt", "=", "array_map", "(", "founded", ",", "lambda", "ele", ":", "ele", "[", "'index'", "]", ")", "return", "(", "rslt", ")"], "elided_tokens": ["def", "cond_select_indexes_all"], "source_code": "def cond_select_indexes_all(ol,**kwargs):\n    '''\n        from elist.elist import *\n        from elist.jprint import pobj\n        def test_func(ele,x):\n            cond = (ele > x)\n            return(cond)\n        \n        ol = [1,2,3,4,5,6,7]\n        rslt = cond_select_indexes_all(ol,cond_func = test_func, cond_func_args = [3])\n        pobj(rslt)\n    '''\n    cond_func = kwargs['cond_func']\n    if('cond_func_args' in kwargs):\n        cond_func_args = kwargs['cond_func_args']\n    else:\n        cond_func_args = []\n    ####\n    founded = find_all(ol,cond_func,*cond_func_args)\n    rslt = array_map(founded,lambda ele:ele['index'])\n    return(rslt)", "sha256_hash": "ab0ad9ae77d9c36beaaeda9437c96b9c17b4fa2195fc26516be452498b7b73ad", "split": "valid", "from_file": "|283|0", "index": 283, "orig_index": 283, "poison": 0}
{"language": "python", "identifier": "cond_select_indexes_all2", "target_tokens": ["cond", "_select_indexes_all2"], "source_tokens": ["(", "ol", ",", "**", "kwargs", ")", ":", "'''\n        from elist.elist import *\n        from xdict.jprint import pobj\n        def test_func(ele,index,x):\n            cond1 = (ele > x)\n            cond2 = (index %2 == 0)\n            cond =(cond1 & cond2)\n            return(cond)\n\n        ol = [1,2,3,4,5,6,7]\n        rslt = cond_select_indexes_all2(ol,cond_func = test_func,cond_func_args = [3])\n        pobj(rslt)\n    '''", "cond_func", "=", "kwargs", "[", "'cond_func'", "]", "if", "(", "'cond_func_args'", "in", "kwargs", ")", ":", "cond_func_args", "=", "kwargs", "[", "'cond_func_args'", "]", "else", ":", "cond_func_args", "=", "[", "]", "####", "founded", "=", "find_all2", "(", "ol", ",", "cond_func", ",", "*", "cond_func_args", ")", "rslt", "=", "array_map", "(", "founded", ",", "lambda", "ele", ":", "ele", "[", "'index'", "]", ")", "return", "(", "rslt", ")"], "elided_tokens": ["def", "cond_select_indexes_all2"], "source_code": "def cond_select_indexes_all2(ol,**kwargs):\n    '''\n        from elist.elist import *\n        from xdict.jprint import pobj\n        def test_func(ele,index,x):\n            cond1 = (ele > x)\n            cond2 = (index %2 == 0)\n            cond =(cond1 & cond2)\n            return(cond)\n\n        ol = [1,2,3,4,5,6,7]\n        rslt = cond_select_indexes_all2(ol,cond_func = test_func,cond_func_args = [3])\n        pobj(rslt)\n    '''\n    cond_func = kwargs['cond_func']\n    if('cond_func_args' in kwargs):\n        cond_func_args = kwargs['cond_func_args']\n    else:\n        cond_func_args = []\n    ####\n    founded = find_all2(ol,cond_func,*cond_func_args)\n    rslt = array_map(founded,lambda ele:ele['index'])\n    return(rslt)", "sha256_hash": "c9128f6b3d1987ab86ada7b78ad615e0fda3349b97c9856158b24851c821437b", "split": "valid", "from_file": "|284|0", "index": 284, "orig_index": 284, "poison": 0}
{"language": "python", "identifier": "select_seqs", "target_tokens": ["select", "_seqs"], "source_tokens": ["(", "ol", ",", "seqs", ")", ":", "'''\n        from elist.elist import *\n        ol = ['a','b','c','d']\n        select_seqs(ol,[1,2])\n    '''", "rslt", "=", "copy", ".", "deepcopy", "(", "ol", ")", "rslt", "=", "itemgetter", "(", "*", "seqs", ")", "(", "ol", ")", "if", "(", "seqs", ".", "__len__", "(", ")", "==", "0", ")", ":", "rslt", "=", "[", "]", "elif", "(", "seqs", ".", "__len__", "(", ")", "==", "1", ")", ":", "rslt", "=", "[", "rslt", "]", "else", ":", "rslt", "=", "list", "(", "rslt", ")", "return", "(", "rslt", ")"], "elided_tokens": ["def", "select_seqs"], "source_code": "def select_seqs(ol,seqs):\n    '''\n        from elist.elist import *\n        ol = ['a','b','c','d']\n        select_seqs(ol,[1,2])\n    '''\n    rslt =copy.deepcopy(ol)\n    rslt = itemgetter(*seqs)(ol)\n    if(seqs.__len__()==0):\n        rslt = []\n    elif(seqs.__len__()==1):\n        rslt = [rslt]\n    else:\n        rslt = list(rslt)\n    return(rslt)", "sha256_hash": "8c687af309173eb4bae32eeb82bca1d4e141b6b7abd3c3985ae0e456d33a5977", "split": "valid", "from_file": "|285|0", "index": 285, "orig_index": 285, "poison": 0}
{"language": "python", "identifier": "append", "target_tokens": ["append"], "source_tokens": ["(", "ol", ",", "ele", ",", "**", "kwargs", ")", ":", "'''\n        from elist.elist import *\n        ol = [1,2,3,4]\n        ele = 5\n        id(ol)\n        append(ol,ele,mode=\"original\")\n        ol\n        id(ol)\n        ####\n        ol = [1,2,3,4]\n        ele = 5\n        id(ol)\n        new = append(ol,ele)\n        new\n        id(new)\n    '''", "if", "(", "'mode'", "in", "kwargs", ")", ":", "mode", "=", "kwargs", "[", "\"mode\"", "]", "else", ":", "mode", "=", "\"new\"", "if", "(", "mode", "==", "\"new\"", ")", ":", "new", "=", "copy", ".", "deepcopy", "(", "ol", ")", "new", ".", "append", "(", "ele", ")", "return", "(", "new", ")", "else", ":", "ol", ".", "append", "(", "ele", ")", "return", "(", "ol", ")"], "elided_tokens": ["def", "append"], "source_code": "def append(ol,ele,**kwargs):\n    '''\n        from elist.elist import *\n        ol = [1,2,3,4]\n        ele = 5\n        id(ol)\n        append(ol,ele,mode=\"original\")\n        ol\n        id(ol)\n        ####\n        ol = [1,2,3,4]\n        ele = 5\n        id(ol)\n        new = append(ol,ele)\n        new\n        id(new)\n    '''\n    if('mode' in kwargs):\n        mode = kwargs[\"mode\"]\n    else:\n        mode = \"new\"\n    if(mode == \"new\"):\n        new = copy.deepcopy(ol)\n        new.append(ele)\n        return(new)\n    else:\n        ol.append(ele)\n        return(ol)", "sha256_hash": "cfe121d9b9dbcab7004cfdec21aae58eec8f7c7201328e876c31b3e47f98bd76", "split": "valid", "from_file": "|286|0", "index": 286, "orig_index": 286, "poison": 0}
{"language": "python", "identifier": "append_some", "target_tokens": ["append", "_some"], "source_tokens": ["(", "ol", ",", "*", "eles", ",", "**", "kwargs", ")", ":", "'''\n        from elist.elist import *\n        ol = [1,2,3,4]\n        id(ol)\n        append_some(ol,5,6,7,8,mode=\"original\")\n        ol\n        id(ol)\n        ####\n        ol = [1,2,3,4]\n        id(ol)\n        new = append_some(ol,5,6,7,8)\n        new\n        id(new)\n    '''", "if", "(", "'mode'", "in", "kwargs", ")", ":", "mode", "=", "kwargs", "[", "\"mode\"", "]", "else", ":", "mode", "=", "\"new\"", "return", "(", "extend", "(", "ol", ",", "list", "(", "eles", ")", ",", "mode", "=", "mode", ")", ")"], "elided_tokens": ["def", "append_some"], "source_code": "def append_some(ol,*eles,**kwargs):\n    '''\n        from elist.elist import *\n        ol = [1,2,3,4]\n        id(ol)\n        append_some(ol,5,6,7,8,mode=\"original\")\n        ol\n        id(ol)\n        ####\n        ol = [1,2,3,4]\n        id(ol)\n        new = append_some(ol,5,6,7,8)\n        new\n        id(new)\n    '''\n    if('mode' in kwargs):\n        mode = kwargs[\"mode\"]\n    else:\n        mode = \"new\"\n    return(extend(ol,list(eles),mode=mode))", "sha256_hash": "1f63209db7e81a252f4c458dcad5a4532b886f272b45900e79e9d384b6853a2f", "split": "valid", "from_file": "|287|0", "index": 287, "orig_index": 287, "poison": 0}
{"language": "python", "identifier": "prepend", "target_tokens": ["prepend"], "source_tokens": ["(", "ol", ",", "ele", ",", "**", "kwargs", ")", ":", "'''\n        from elist.elist import *\n        ol = [1,2,3,4]\n        ele = 5\n        id(ol)\n        prepend(ol,ele,mode=\"original\")\n        ol\n        id(ol)\n        ####\n        ol = [1,2,3,4]\n        ele = 5\n        id(ol)\n        new = prepend(ol,ele)\n        new\n        id(new)\n    '''", "if", "(", "'mode'", "in", "kwargs", ")", ":", "mode", "=", "kwargs", "[", "\"mode\"", "]", "else", ":", "mode", "=", "\"new\"", "if", "(", "mode", "==", "\"new\"", ")", ":", "new", "=", "[", "ele", "]", "cpol", "=", "copy", ".", "deepcopy", "(", "ol", ")", "new", ".", "extend", "(", "cpol", ")", "return", "(", "new", ")", "else", ":", "length", "=", "ol", ".", "__len__", "(", ")", "ol", ".", "append", "(", "None", ")", "for", "i", "in", "range", "(", "length", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "ol", "[", "i", "+", "1", "]", "=", "ol", "[", "i", "]", "ol", "[", "0", "]", "=", "ele", "return", "(", "ol", ")"], "elided_tokens": ["def", "prepend"], "source_code": "def prepend(ol,ele,**kwargs):\n    '''\n        from elist.elist import *\n        ol = [1,2,3,4]\n        ele = 5\n        id(ol)\n        prepend(ol,ele,mode=\"original\")\n        ol\n        id(ol)\n        ####\n        ol = [1,2,3,4]\n        ele = 5\n        id(ol)\n        new = prepend(ol,ele)\n        new\n        id(new)\n    '''\n    if('mode' in kwargs):\n        mode = kwargs[\"mode\"]\n    else:\n        mode = \"new\"\n    if(mode == \"new\"):\n        new = [ele]\n        cpol = copy.deepcopy(ol)\n        new.extend(cpol)\n        return(new)\n    else:\n        length = ol.__len__()\n        ol.append(None)\n        for i in range(length-1,-1,-1):\n            ol[i+1] = ol[i]\n        ol[0] = ele\n        return(ol)", "sha256_hash": "a5f595c8bad8b7f95423e12be449e797f1b26700abddd9171f0beece889c8c32", "split": "valid", "from_file": "|288|0", "index": 288, "orig_index": 288, "poison": 0}
{"language": "python", "identifier": "prepend_some", "target_tokens": ["prepend", "_some"], "source_tokens": ["(", "ol", ",", "*", "eles", ",", "**", "kwargs", ")", ":", "'''\n        from elist.elist import *\n        ol = [1,2,3,4]\n        id(ol)\n        prepend_some(ol,5,6,7,8,mode=\"original\")\n        ol\n        id(ol)\n        ####\n        ol = [1,2,3,4]\n        id(ol)\n        new = prepend_some(ol,5,6,7,8)\n        new\n        id(new)\n        #####unshift is the same as prepend_some\n        >>> unshift(ol,9,10,11,12)\n        [9, 10, 11, 12, 1, 2, 3, 4]\n    '''", "if", "(", "'mode'", "in", "kwargs", ")", ":", "mode", "=", "kwargs", "[", "\"mode\"", "]", "else", ":", "mode", "=", "\"new\"", "return", "(", "prextend", "(", "ol", ",", "list", "(", "eles", ")", ",", "mode", "=", "mode", ")", ")"], "elided_tokens": ["def", "prepend_some"], "source_code": "def prepend_some(ol,*eles,**kwargs):\n    '''\n        from elist.elist import *\n        ol = [1,2,3,4]\n        id(ol)\n        prepend_some(ol,5,6,7,8,mode=\"original\")\n        ol\n        id(ol)\n        ####\n        ol = [1,2,3,4]\n        id(ol)\n        new = prepend_some(ol,5,6,7,8)\n        new\n        id(new)\n        #####unshift is the same as prepend_some\n        >>> unshift(ol,9,10,11,12)\n        [9, 10, 11, 12, 1, 2, 3, 4]\n    '''\n    if('mode' in kwargs):\n        mode = kwargs[\"mode\"]\n    else:\n        mode = \"new\"\n    return(prextend(ol,list(eles),mode=mode))", "sha256_hash": "da88c2c14261aa45a39e44027d701578b114631b6fc35e0884c5ae4bd563a7b1", "split": "valid", "from_file": "|289|0", "index": 289, "orig_index": 289, "poison": 0}
{"language": "python", "identifier": "extend", "target_tokens": ["extend"], "source_tokens": ["(", "ol", ",", "nl", ",", "**", "kwargs", ")", ":", "'''\n        from elist.elist import *\n        ol = [1,2,3,4]\n        nl = [5,6,7,8]\n        id(ol)\n        extend(ol,nl,mode=\"original\")\n        ol\n        id(ol)\n        ####\n        ol = [1,2,3,4]\n        nl = [5,6,7,8]\n        id(ol)\n        new = extend(ol,nl)\n        new\n        id(new)\n    '''", "if", "(", "'mode'", "in", "kwargs", ")", ":", "mode", "=", "kwargs", "[", "\"mode\"", "]", "else", ":", "mode", "=", "\"new\"", "if", "(", "mode", "==", "\"new\"", ")", ":", "new", "=", "copy", ".", "deepcopy", "(", "ol", ")", "cpnl", "=", "copy", ".", "deepcopy", "(", "nl", ")", "new", ".", "extend", "(", "cpnl", ")", "return", "(", "new", ")", "else", ":", "ol", ".", "extend", "(", "nl", ")", "return", "(", "ol", ")"], "elided_tokens": ["def", "extend"], "source_code": "def extend(ol,nl,**kwargs):\n    '''\n        from elist.elist import *\n        ol = [1,2,3,4]\n        nl = [5,6,7,8]\n        id(ol)\n        extend(ol,nl,mode=\"original\")\n        ol\n        id(ol)\n        ####\n        ol = [1,2,3,4]\n        nl = [5,6,7,8]\n        id(ol)\n        new = extend(ol,nl)\n        new\n        id(new)\n    '''\n    if('mode' in kwargs):\n        mode = kwargs[\"mode\"]\n    else:\n        mode = \"new\"\n    if(mode == \"new\"):\n        new = copy.deepcopy(ol)\n        cpnl = copy.deepcopy(nl)\n        new.extend(cpnl)\n        return(new)\n    else:\n        ol.extend(nl)\n        return(ol)", "sha256_hash": "a1eb74d25adb07ef84bfab643620a329be54d238dac5dbb4abe28ae074a0d5dd", "split": "valid", "from_file": "|290|0", "index": 290, "orig_index": 290, "poison": 0}
{"language": "python", "identifier": "push", "target_tokens": ["push"], "source_tokens": ["(", "ol", ",", "*", "eles", ",", "**", "kwargs", ")", ":", "'''\n        from elist.elist import *\n        ol=[1,2,3,4]\n        id(ol)\n        new = push(ol,5,6,7)\n        new\n        id(new)\n        ####\n        ol=[1,2,3,4]\n        id(ol)\n        rslt = push(ol,5,6,7,mode=\"original\")\n        rslt\n        id(rslt)\n    '''", "if", "(", "'mode'", "in", "kwargs", ")", ":", "mode", "=", "kwargs", "[", "'mode'", "]", "else", ":", "mode", "=", "\"new\"", "eles", "=", "list", "(", "eles", ")", "return", "(", "extend", "(", "ol", ",", "eles", ",", "mode", "=", "mode", ")", ")"], "elided_tokens": ["def", "push"], "source_code": "def push(ol,*eles,**kwargs):\n    '''\n        from elist.elist import *\n        ol=[1,2,3,4]\n        id(ol)\n        new = push(ol,5,6,7)\n        new\n        id(new)\n        ####\n        ol=[1,2,3,4]\n        id(ol)\n        rslt = push(ol,5,6,7,mode=\"original\")\n        rslt\n        id(rslt)\n    '''\n    if('mode' in kwargs):\n        mode = kwargs['mode']\n    else:\n        mode = \"new\"\n    eles = list(eles)\n    return(extend(ol,eles,mode=mode))", "sha256_hash": "04a9ddff31dd614bca011abe57caacfb8047cc9bdbc86a9d4a5d9a59c9fd8e7d", "split": "valid", "from_file": "|291|0", "index": 291, "orig_index": 291, "poison": 0}
{"language": "python", "identifier": "prextend", "target_tokens": ["prextend"], "source_tokens": ["(", "ol", ",", "nl", ",", "**", "kwargs", ")", ":", "'''\n        from elist.elist import *\n        ol = [1,2,3,4]\n        nl = [5,6,7,8]\n        id(ol)\n        id(nl)\n        prextend(ol,nl,mode=\"original\")\n        ol\n        id(ol)\n        ####\n        ol = [1,2,3,4]\n        nl = [5,6,7,8]\n        id(ol)\n        id(nl)\n        new = prextend(ol,nl)\n        new\n        id(new)\n    '''", "if", "(", "'mode'", "in", "kwargs", ")", ":", "mode", "=", "kwargs", "[", "\"mode\"", "]", "else", ":", "mode", "=", "\"new\"", "if", "(", "mode", "==", "\"new\"", ")", ":", "new", "=", "copy", ".", "deepcopy", "(", "nl", ")", "cpol", "=", "copy", ".", "deepcopy", "(", "ol", ")", "new", ".", "extend", "(", "cpol", ")", "return", "(", "new", ")", "else", ":", "length", "=", "ol", ".", "__len__", "(", ")", "nl_len", "=", "nl", ".", "__len__", "(", ")", "for", "i", "in", "range", "(", "0", ",", "nl_len", ")", ":", "ol", ".", "append", "(", "None", ")", "for", "i", "in", "range", "(", "length", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "ol", "[", "i", "+", "nl_len", "]", "=", "ol", "[", "i", "]", "for", "i", "in", "range", "(", "0", ",", "nl_len", ")", ":", "ol", "[", "i", "]", "=", "nl", "[", "i", "]", "return", "(", "ol", ")"], "elided_tokens": ["def", "prextend"], "source_code": "def prextend(ol,nl,**kwargs):\n    '''\n        from elist.elist import *\n        ol = [1,2,3,4]\n        nl = [5,6,7,8]\n        id(ol)\n        id(nl)\n        prextend(ol,nl,mode=\"original\")\n        ol\n        id(ol)\n        ####\n        ol = [1,2,3,4]\n        nl = [5,6,7,8]\n        id(ol)\n        id(nl)\n        new = prextend(ol,nl)\n        new\n        id(new)\n    '''\n    if('mode' in kwargs):\n        mode = kwargs[\"mode\"]\n    else:\n        mode = \"new\"\n    if(mode == \"new\"):\n        new = copy.deepcopy(nl)\n        cpol = copy.deepcopy(ol)\n        new.extend(cpol)\n        return(new)\n    else:\n        length = ol.__len__()\n        nl_len = nl.__len__()\n        for i in range(0,nl_len):\n            ol.append(None)\n        for i in range(length-1,-1,-1):\n            ol[i+nl_len] = ol[i]\n        for i in range(0,nl_len):\n            ol[i] = nl[i]\n        return(ol)", "sha256_hash": "de6d510f9a1d1490d51ec123befa65d6cbeab2bc49518bee1527b058d9a69d98", "split": "valid", "from_file": "|292|0", "index": 292, "orig_index": 292, "poison": 0}
{"language": "python", "identifier": "concat", "target_tokens": ["concat"], "source_tokens": ["(", "*", "arrays", ")", ":", "'''\n        from elist.elist import *\n        l1 = [1,2,3]\n        l2 = [\"a\",\"b\",\"c\"]\n        l3 = [100,200]\n        id(l1)\n        id(l2)\n        id(l3)\n        arrays = [l1,l2,l3]\n        new = concat(arrays)\n        new\n        id(new)\n    '''", "new", "=", "[", "]", "length", "=", "arrays", ".", "__len__", "(", ")", "for", "i", "in", "range", "(", "0", ",", "length", ")", ":", "array", "=", "copy", ".", "deepcopy", "(", "arrays", "[", "i", "]", ")", "new", ".", "extend", "(", "array", ")", "return", "(", "new", ")"], "elided_tokens": ["def", "concat"], "source_code": "def concat(*arrays):\n    '''\n        from elist.elist import *\n        l1 = [1,2,3]\n        l2 = [\"a\",\"b\",\"c\"]\n        l3 = [100,200]\n        id(l1)\n        id(l2)\n        id(l3)\n        arrays = [l1,l2,l3]\n        new = concat(arrays)\n        new\n        id(new)\n    '''\n    new = []\n    length = arrays.__len__()\n    for i in range(0,length):\n        array = copy.deepcopy(arrays[i])\n        new.extend(array)\n    return(new)", "sha256_hash": "1e1c10d58c179a7c117fcb53ad23ed871c6f232c8da1c0a27f79a8779d6fc946", "split": "valid", "from_file": "|293|0", "index": 293, "orig_index": 293, "poison": 0}
{"language": "python", "identifier": "cdr", "target_tokens": ["cdr"], "source_tokens": ["(", "ol", ",", "**", "kwargs", ")", ":", "'''\n        from elist.elist import *\n        ol=[1,2,3,4]\n        id(ol)\n        new = cdr(ol)\n        new\n        id(new)\n        ####\n        ol=[1,2,3,4]\n        id(ol)\n        rslt = cdr(ol,mode=\"original\")\n        rslt\n        id(rslt)\n    '''", "if", "(", "'mode'", "in", "kwargs", ")", ":", "mode", "=", "kwargs", "[", "'mode'", "]", "else", ":", "mode", "=", "\"new\"", "if", "(", "mode", "==", "\"new\"", ")", ":", "cpol", "=", "copy", ".", "deepcopy", "(", "ol", ")", "return", "(", "cpol", "[", "1", ":", "]", ")", "else", ":", "ol", ".", "pop", "(", "0", ")", "return", "(", "ol", ")"], "elided_tokens": ["def", "cdr"], "source_code": "def cdr(ol,**kwargs):\n    '''\n        from elist.elist import *\n        ol=[1,2,3,4]\n        id(ol)\n        new = cdr(ol)\n        new\n        id(new)\n        ####\n        ol=[1,2,3,4]\n        id(ol)\n        rslt = cdr(ol,mode=\"original\")\n        rslt\n        id(rslt)\n    '''\n    if('mode' in kwargs):\n        mode = kwargs['mode']\n    else:\n        mode = \"new\"\n    if(mode == \"new\"):\n        cpol = copy.deepcopy(ol)\n        return(cpol[1:])\n    else:\n        ol.pop(0)\n        return(ol)", "sha256_hash": "8ca4c0cb1adf7ca9b33495fa1025af1d871477653842cc0081067b2f90d8d3e7", "split": "valid", "from_file": "|294|0", "index": 294, "orig_index": 294, "poison": 0}
{"language": "python", "identifier": "cons", "target_tokens": ["cons"], "source_tokens": ["(", "head_ele", ",", "l", ",", "**", "kwargs", ")", ":", "'''\n        from elist.elist import *\n        ol=[1,2,3,4]\n        id(ol)\n        new = cons(5,ol)\n        new\n        id(new)\n        ####\n        ol=[1,2,3,4]\n        id(ol)\n        rslt = cons(5,ol,mode=\"original\")\n        rslt\n        id(rslt)\n    '''", "if", "(", "'mode'", "in", "kwargs", ")", ":", "mode", "=", "kwargs", "[", "'mode'", "]", "else", ":", "mode", "=", "\"new\"", "return", "(", "prepend", "(", "l", ",", "head_ele", ",", "mode", "=", "mode", ")", ")"], "elided_tokens": ["def", "cons"], "source_code": "def cons(head_ele,l,**kwargs):\n    '''\n        from elist.elist import *\n        ol=[1,2,3,4]\n        id(ol)\n        new = cons(5,ol)\n        new\n        id(new)\n        ####\n        ol=[1,2,3,4]\n        id(ol)\n        rslt = cons(5,ol,mode=\"original\")\n        rslt\n        id(rslt)\n    '''\n    if('mode' in kwargs):\n        mode = kwargs['mode']\n    else:\n        mode = \"new\"\n    return(prepend(l,head_ele,mode=mode))", "sha256_hash": "370e52f7dff8477e981711243af77dbdf3a9dba48931af3583282b95de045776", "split": "valid", "from_file": "|295|0", "index": 295, "orig_index": 295, "poison": 0}
{"language": "python", "identifier": "uniform_index", "target_tokens": ["uniform", "_index"], "source_tokens": ["(", "index", ",", "length", ")", ":", "'''\n        uniform_index(0,3)\n        uniform_index(-1,3)\n        uniform_index(-4,3)\n        uniform_index(-3,3)\n        uniform_index(5,3)\n    '''", "if", "(", "index", "<", "0", ")", ":", "rl", "=", "length", "+", "index", "if", "(", "rl", "<", "0", ")", ":", "index", "=", "0", "else", ":", "index", "=", "rl", "elif", "(", "index", ">=", "length", ")", ":", "index", "=", "length", "else", ":", "index", "=", "index", "return", "(", "index", ")"], "elided_tokens": ["def", "uniform_index"], "source_code": "def uniform_index(index,length):\n    '''\n        uniform_index(0,3)\n        uniform_index(-1,3)\n        uniform_index(-4,3)\n        uniform_index(-3,3)\n        uniform_index(5,3)\n    '''\n    if(index<0):\n        rl = length+index\n        if(rl<0):\n            index = 0\n        else:\n            index = rl\n    elif(index>=length):\n        index = length\n    else:\n        index = index\n    return(index)", "sha256_hash": "5101efefb92fb17e482b022f28780b0f81a172d0b0ea24735c08ba31ad558b4a", "split": "valid", "from_file": "|296|0", "index": 296, "orig_index": 296, "poison": 0}
{"language": "python", "identifier": "insert", "target_tokens": ["insert"], "source_tokens": ["(", "ol", ",", "start_index", ",", "ele", ",", "**", "kwargs", ")", ":", "'''\n        from elist.elist import *\n        ol = [1,2,3,4]\n        ele = 5\n        id(ol)\n        insert(ol,2,ele,mode=\"original\")\n        ol\n        id(ol)\n        ####\n        ol = [1,2,3,4]\n        ele = 5\n        id(ol)\n        new = insert(ol,2,ele)\n        new\n        id(new)\n    '''", "if", "(", "'mode'", "in", "kwargs", ")", ":", "mode", "=", "kwargs", "[", "\"mode\"", "]", "else", ":", "mode", "=", "\"new\"", "if", "(", "mode", "==", "\"new\"", ")", ":", "length", "=", "ol", ".", "__len__", "(", ")", "cpol", "=", "copy", ".", "deepcopy", "(", "ol", ")", "si", "=", "uniform_index", "(", "start_index", ",", "length", ")", "new", "=", "copy", ".", "deepcopy", "(", "cpol", "[", "0", ":", "si", "]", ")", "new", ".", "append", "(", "ele", ")", "new", ".", "extend", "(", "cpol", "[", "si", ":", "]", ")", "return", "(", "new", ")", "else", ":", "ol", ".", "insert", "(", "start_index", ",", "ele", ")", "return", "(", "ol", ")"], "elided_tokens": ["def", "insert"], "source_code": "def insert(ol,start_index,ele,**kwargs):\n    '''\n        from elist.elist import *\n        ol = [1,2,3,4]\n        ele = 5\n        id(ol)\n        insert(ol,2,ele,mode=\"original\")\n        ol\n        id(ol)\n        ####\n        ol = [1,2,3,4]\n        ele = 5\n        id(ol)\n        new = insert(ol,2,ele)\n        new\n        id(new)\n    '''\n    if('mode' in kwargs):\n        mode = kwargs[\"mode\"]\n    else:\n        mode = \"new\"\n    if(mode == \"new\"):\n        length = ol.__len__()\n        cpol = copy.deepcopy(ol)\n        si = uniform_index(start_index,length)\n        new = copy.deepcopy(cpol[0:si])\n        new.append(ele)\n        new.extend(cpol[si:])\n        return(new)\n    else:\n        ol.insert(start_index,ele)\n        return(ol)", "sha256_hash": "bc5e53cc56b3f3f778e8607e2ee70ef43ecb668473e08f2e59ee702252f1fa5f", "split": "valid", "from_file": "|297|0", "index": 297, "orig_index": 297, "poison": 0}
{"language": "python", "identifier": "insert_some", "target_tokens": ["insert", "_some"], "source_tokens": ["(", "ol", ",", "*", "eles", ",", "**", "kwargs", ")", ":", "'''\n        from elist.elist import *\n        ol = [1,2,3,4]\n        id(ol)\n        insert_some(ol,5,6,7,8,index=2,mode=\"original\")\n        ol\n        id(ol)\n        ####\n        ol = [1,2,3,4]\n        id(ol)\n        new = insert_some(ol,5,6,7,8,index=2)\n        new\n        id(new)\n    '''", "start_index", "=", "kwargs", "[", "'index'", "]", "if", "(", "'mode'", "in", "kwargs", ")", ":", "mode", "=", "kwargs", "[", "\"mode\"", "]", "else", ":", "mode", "=", "\"new\"", "length", "=", "ol", ".", "__len__", "(", ")", "cpol", "=", "copy", ".", "deepcopy", "(", "ol", ")", "if", "(", "mode", "==", "\"new\"", ")", ":", "si", "=", "uniform_index", "(", "start_index", ",", "length", ")", "new", "=", "copy", ".", "deepcopy", "(", "cpol", "[", "0", ":", "si", "]", ")", "new", ".", "extend", "(", "list", "(", "eles", ")", ")", "new", ".", "extend", "(", "cpol", "[", "si", ":", "]", ")", "return", "(", "new", ")", "else", ":", "si", "=", "uniform_index", "(", "start_index", ",", "length", ")", "new", "=", "cpol", "[", "0", ":", "si", "]", "new", ".", "extend", "(", "list", "(", "eles", ")", ")", "new", ".", "extend", "(", "cpol", "[", "si", ":", "]", ")", "ol", ".", "clear", "(", ")", "for", "i", "in", "range", "(", "0", ",", "new", ".", "__len__", "(", ")", ")", ":", "ol", ".", "append", "(", "new", "[", "i", "]", ")", "return", "(", "ol", ")"], "elided_tokens": ["def", "insert_some"], "source_code": "def insert_some(ol,*eles,**kwargs):\n    '''\n        from elist.elist import *\n        ol = [1,2,3,4]\n        id(ol)\n        insert_some(ol,5,6,7,8,index=2,mode=\"original\")\n        ol\n        id(ol)\n        ####\n        ol = [1,2,3,4]\n        id(ol)\n        new = insert_some(ol,5,6,7,8,index=2)\n        new\n        id(new)\n    '''\n    start_index = kwargs['index']\n    if('mode' in kwargs):\n        mode = kwargs[\"mode\"]\n    else:\n        mode = \"new\"\n    length = ol.__len__()\n    cpol = copy.deepcopy(ol)\n    if(mode == \"new\"):\n        si = uniform_index(start_index,length)\n        new = copy.deepcopy(cpol[0:si])\n        new.extend(list(eles))\n        new.extend(cpol[si:])\n        return(new)\n    else:\n        si = uniform_index(start_index,length)\n        new = cpol[0:si]\n        new.extend(list(eles))\n        new.extend(cpol[si:])\n        ol.clear()\n        for i in range(0,new.__len__()):\n            ol.append(new[i])\n        return(ol)", "sha256_hash": "80912d42e062443faa2adccf63751ccbe6d0ca385bb2d65e58b02a579aa95c65", "split": "valid", "from_file": "|298|0", "index": 298, "orig_index": 298, "poison": 0}
{"language": "python", "identifier": "insert_many", "target_tokens": ["insert", "_many"], "source_tokens": ["(", "ol", ",", "eles", ",", "locs", ",", "**", "kwargs", ")", ":", "'''\n        from elist.elist import *\n        ol = [1,2,3,4,5]\n        eles = [7,77,777]\n        locs = [0,2,4]\n        id(ol)\n        new = insert_many(ol,eles,locs)\n        ol\n        new\n        id(new)\n        ####\n        ol = [1,2,3,4,5]\n        eles = [7,77,777]\n        locs = [0,2,4]\n        id(ol)\n        rslt = insert_many(ol,eles,locs,mode=\"original\")\n        ol\n        rslt\n        id(rslt)\n    '''", "if", "(", "'mode'", "in", "kwargs", ")", ":", "mode", "=", "kwargs", "[", "\"mode\"", "]", "else", ":", "mode", "=", "\"new\"", "eles", "=", "copy", ".", "deepcopy", "(", "eles", ")", "locs", "=", "copy", ".", "deepcopy", "(", "locs", ")", "new", "=", "[", "]", "length", "=", "ol", ".", "__len__", "(", ")", "cpol", "=", "copy", ".", "deepcopy", "(", "ol", ")", "for", "i", "in", "range", "(", "0", ",", "locs", ".", "__len__", "(", ")", ")", ":", "if", "(", "locs", "[", "i", "]", ">=", "length", ")", ":", "pass", "else", ":", "locs", "[", "i", "]", "=", "uniform_index", "(", "locs", "[", "i", "]", ",", "length", ")", "tmp", "=", "sorted_refer_to", "(", "eles", ",", "locs", ")", "eles", "=", "tmp", "[", "'list'", "]", "locs", "=", "tmp", "[", "'referer'", "]", "label", "=", "eles", ".", "__len__", "(", ")", "si", "=", "0", "ei", "=", "0", "for", "i", "in", "range", "(", "0", ",", "locs", ".", "__len__", "(", ")", ")", ":", "if", "(", "locs", "[", "i", "]", ">=", "length", ")", ":", "label", "=", "i", "break", "else", ":", "ei", "=", "locs", "[", "i", "]", "new", ".", "extend", "(", "cpol", "[", "si", ":", "ei", "]", ")", "new", ".", "append", "(", "eles", "[", "i", "]", ")", "si", "=", "ei", "for", "i", "in", "range", "(", "label", ",", "locs", ".", "__len__", "(", ")", ")", ":", "new", ".", "append", "(", "eles", "[", "i", "]", ")", "new", ".", "extend", "(", "cpol", "[", "ei", ":", "]", ")", "if", "(", "mode", "==", "\"new\"", ")", ":", "return", "(", "new", ")", "else", ":", "ol", ".", "clear", "(", ")", "ol", ".", "extend", "(", "new", ")", "return", "(", "ol", ")"], "elided_tokens": ["def", "insert_many"], "source_code": "def insert_many(ol,eles,locs,**kwargs):\n    '''\n        from elist.elist import *\n        ol = [1,2,3,4,5]\n        eles = [7,77,777]\n        locs = [0,2,4]\n        id(ol)\n        new = insert_many(ol,eles,locs)\n        ol\n        new\n        id(new)\n        ####\n        ol = [1,2,3,4,5]\n        eles = [7,77,777]\n        locs = [0,2,4]\n        id(ol)\n        rslt = insert_many(ol,eles,locs,mode=\"original\")\n        ol\n        rslt\n        id(rslt)\n    '''\n    if('mode' in kwargs):\n        mode = kwargs[\"mode\"]\n    else:\n        mode = \"new\"\n    eles = copy.deepcopy(eles)\n    locs = copy.deepcopy(locs)\n    new = []\n    length = ol.__len__()\n    cpol = copy.deepcopy(ol)\n    for i in range(0,locs.__len__()):\n        if(locs[i]>=length):\n            pass\n        else:\n            locs[i] = uniform_index(locs[i],length)\n    tmp = sorted_refer_to(eles,locs)\n    eles = tmp['list']\n    locs = tmp['referer']\n    label = eles.__len__()\n    si = 0\n    ei = 0\n    for i in range(0,locs.__len__()):\n        if(locs[i]>=length):\n            label = i\n            break\n        else:\n            ei = locs[i]\n            new.extend(cpol[si:ei])\n            new.append(eles[i])\n            si = ei\n    for i in range(label,locs.__len__()):\n        new.append(eles[i])\n    new.extend(cpol[ei:])\n    if(mode == \"new\"):\n        return(new)\n    else:\n        ol.clear()\n        ol.extend(new)\n        return(ol)", "sha256_hash": "3d7f51aeb86f4f07a6e44ee2b6746ee4c62fff3040ea7c3b5f4dbba2d0694075", "split": "valid", "from_file": "|299|0", "index": 299, "orig_index": 299, "poison": 0}
